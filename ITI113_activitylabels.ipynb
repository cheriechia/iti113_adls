{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c51a123-9e42-468e-abf5-33df80907797",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's install the necessary libraries and configure our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f5cd559d-0cf2-4c5f-803c-1092c41bf6f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T04:51:01.945796Z",
     "iopub.status.busy": "2025-08-23T04:51:01.945518Z",
     "iopub.status.idle": "2025-08-23T04:51:03.403261Z",
     "shell.execute_reply": "2025-08-23T04:51:03.402601Z",
     "shell.execute_reply.started": "2025-08-23T04:51:01.945777Z"
    }
   },
   "outputs": [],
   "source": [
    "# The SageMaker Studio environment comes with most of these pre-installed.\n",
    "# This cell ensures all dependencies are present.\n",
    "!pip install -q boto3 sagemaker mlflow \"scikit-learn>=1.0\" \"pandas>=1.2\"\n",
    "# !pip install -q boto3 sagemaker mlflow \"scikit-learn>=1.7\" \"pandas>=2.3\" \"matplotlib>=3.10\" \"numpy>=2.0\" \"seaborn>=0.13\"\n",
    "# !pip install --upgrade sagemaker google-api-core grpcio\n",
    "# !pip install \"sagemaker==2.218.0\" \"protobuf==3.20.*\" --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a2ac10a-5312-42c5-a9c2-436550dceb41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:02.737028Z",
     "iopub.status.busy": "2025-08-25T07:47:02.736394Z",
     "iopub.status.idle": "2025-08-25T07:47:02.740176Z",
     "shell.execute_reply": "2025-08-25T07:47:02.739668Z",
     "shell.execute_reply.started": "2025-08-25T07:47:02.737007Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Ensure MLflow is installed\n",
    "try:\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow\n",
    "except ImportError:\n",
    "    print(\"Installing MLflow...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",  \"boto3==1.37.1\", \"botocore==1.37.1\", \"s3transfer\", \"mlflow==2.22.0\", \"sagemaker-mlflow==0.1.0\"])\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc425c68-2edf-4e6b-944e-96119e59564f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:03.098101Z",
     "iopub.status.busy": "2025-08-25T07:47:03.097689Z",
     "iopub.status.idle": "2025-08-25T07:47:03.101189Z",
     "shell.execute_reply": "2025-08-25T07:47:03.100716Z",
     "shell.execute_reply.started": "2025-08-25T07:47:03.098082Z"
    }
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "import os\n",
    "import io\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948dd23-fd2c-4ac8-b9a7-4356b6145816",
   "metadata": {},
   "source": [
    "### Set up Sagemaker session & S3 folder for assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c127ebd-613c-4d8e-88a1-69eafc776da8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:04.007308Z",
     "iopub.status.busy": "2025-08-25T07:47:04.006722Z",
     "iopub.status.idle": "2025-08-25T07:47:04.452142Z",
     "shell.execute_reply": "2025-08-25T07:47:04.451705Z",
     "shell.execute_reply.started": "2025-08-25T07:47:04.007288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# --- IMPORTANT: CONFIGURE THESE VARIABLES ---\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "# ----------------------\n",
    "# UPDATE THESE VARIABLES\n",
    "bucket_name = 'iti113-team12-bucket'  # e.g., 'my-company-sagemaker-bucket'\n",
    "base_folder = 'project'      # e.g., 'users/my-name'\n",
    "# ----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3cd97a",
   "metadata": {},
   "source": [
    "### Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6beefbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:04.736724Z",
     "iopub.status.busy": "2025-08-25T07:47:04.736186Z",
     "iopub.status.idle": "2025-08-25T07:47:04.740146Z",
     "shell.execute_reply": "2025-08-25T07:47:04.739651Z",
     "shell.execute_reply.started": "2025-08-25T07:47:04.736704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dataset at: s3://iti113-team12-bucket/project/datasets/c17/working\n"
     ]
    }
   ],
   "source": [
    "# # Load dataset\n",
    "# df = pd.read_csv('volunteer_details.csv')\n",
    "\n",
    "# Define the base path for our datasets\n",
    "data_path = f\"s3://{bucket_name}/{base_folder}/datasets/c17/working\"\n",
    "print(f\"Working dataset at: {data_path}\")\n",
    "\n",
    "# # Define S3 path for dataset\n",
    "# # os.makedirs(dataset_s3_path, exist_ok=True)\n",
    "# dataset_s3_path = f\"{data_path}/users/volunteer_details.csv\"\n",
    "# data_v1_s3_uri = os.path.dirname(dataset_s3_path)\n",
    "\n",
    "# # Upload to S3\n",
    "# print(f\"Uploading data to {dataset_s3_path}\")\n",
    "# csv_buffer = io.StringIO()\n",
    "# df.to_csv(csv_buffer, index=False)\n",
    "# dataset_s3_key = f\"{base_folder}/datasets/adl/users/volunteer_details.csv\"\n",
    "# s3_client.put_object(Bucket=bucket_name, Key=dataset_s3_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "# print(f\"Dataset uploaded to: {dataset_s3_path}\")\n",
    "# # s3://iti113-team12-bucket/project/datasets/adl/users/volunteer_details.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b01dbab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:05.248049Z",
     "iopub.status.busy": "2025-08-25T07:47:05.247531Z",
     "iopub.status.idle": "2025-08-25T07:47:05.250516Z",
     "shell.execute_reply": "2025-08-25T07:47:05.250041Z",
     "shell.execute_reply.started": "2025-08-25T07:47:05.248031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the base path for our datasets\n",
    "# Scidirect ADL: s3://iti113-team12-bucket/project/datasets/adl/adl/\n",
    "# Scidirect fall: s3://iti113-team12-bucket/project/datasets/adl/fall/ \n",
    "# Dryad C17: s3://iti113-team12-bucket/project/datasets/c17/\n",
    "# data_path = f\"s3://{bucket_name}/{base_folder}/datasets/adl\"\n",
    "# data_path_adl = \"s3://iti113-team12-bucket/project/datasets/adl/adl/\"\n",
    "# data_path_fall = \"s3://iti113-team12-bucket/project/datasets/adl/fall/\"\n",
    "# data_path_adlfalls_users = \"s3://iti113-team12-bucket/project/datasets/adl/users/\"\n",
    "\n",
    "# print(f\"S3 data paths: {data_path_adl}\\n {data_path_fall} \\n {data_path_adlfalls_users}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e4da0-391b-41f6-b172-3a68b699eb21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Check file exists in S3 (section not in use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "380c1051-a0f4-4acd-a759-bd0adcf8dff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T03:48:32.551003Z",
     "iopub.status.busy": "2025-08-23T03:48:32.550731Z",
     "iopub.status.idle": "2025-08-23T03:48:32.603048Z",
     "shell.execute_reply": "2025-08-23T03:48:32.602645Z",
     "shell.execute_reply.started": "2025-08-23T03:48:32.550986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Key': 'project/datasets/adl/users/volunteer_details.csv', 'LastModified': datetime.datetime(2025, 8, 19, 8, 30, 13, tzinfo=tzlocal()), 'ETag': '\"1b9bf950ed61e8618a4a5d37a85130ad\"', 'ChecksumAlgorithm': ['CRC32'], 'ChecksumType': 'FULL_OBJECT', 'Size': 2086, 'StorageClass': 'STANDARD'}]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "resp = s3.list_objects_v2(\n",
    "    Bucket=\"iti113-team12-bucket\",\n",
    "    Prefix=\"project/datasets/adl/users/volunteer_details.csv\"\n",
    ")\n",
    "print(resp.get(\"Contents\"))\n",
    "\n",
    "# s3://iti113-team12-bucket/project/datasets/adl/users/volunteer_details.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab46daf7-9d4b-4561-8825-c976e54eca97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T03:48:34.401299Z",
     "iopub.status.busy": "2025-08-23T03:48:34.401012Z",
     "iopub.status.idle": "2025-08-23T03:48:34.444618Z",
     "shell.execute_reply": "2025-08-23T03:48:34.444160Z",
     "shell.execute_reply.started": "2025-08-23T03:48:34.401280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File found: s3://iti113-team12-bucket/project/datasets/adl/users/volunteer_details.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_file_exists(bucket, key):\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket, Key=key)\n",
    "        print(f\"✅ File found: s3://{bucket}/{key}\")\n",
    "        return True\n",
    "    except s3_client.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            print(f\"❌ File not found: s3://{bucket}/{key}\")\n",
    "            return False\n",
    "        else:\n",
    "            raise  # Something else went wrong\n",
    "\n",
    "# s3://iti113-team12-bucket/project/datasets/adl/users/volunteer_details.csv\n",
    "check_file_exists(bucket_name, \"project/datasets/adl/users/volunteer_details.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f938d5-5fb0-4876-9d61-ac3a3c5b929f",
   "metadata": {},
   "source": [
    "### Set up tracking server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f364b43c-ed09-4dab-b4c7-3d74a742afdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:07.836027Z",
     "iopub.status.busy": "2025-08-25T07:47:07.835692Z",
     "iopub.status.idle": "2025-08-25T07:47:07.946977Z",
     "shell.execute_reply": "2025-08-25T07:47:07.946499Z",
     "shell.execute_reply.started": "2025-08-25T07:47:07.836007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found MLflow Tracking Server ARN: arn:aws:sagemaker:ap-southeast-1:837028399719:mlflow-tracking-server/mlflow-elderly\n",
      "SageMaker Role ARN: arn:aws:iam::837028399719:role/iti113-team12-sagemaker-iti113-team12-domain-iti113-team12-Role\n",
      "MLflow Tracking Server ARN: arn:aws:sagemaker:ap-southeast-1:837028399719:mlflow-tracking-server/mlflow-elderly\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have your boto3 client and server name\n",
    "tracking_server_name = \"mlflow-elderly\"\n",
    "\n",
    "try:\n",
    "    response = sagemaker_client.describe_mlflow_tracking_server(\n",
    "        TrackingServerName=tracking_server_name\n",
    "    )\n",
    "    mlflow_tracking_server_arn = response['TrackingServerArn']\n",
    "    print(f\"Found MLflow Tracking Server ARN: {mlflow_tracking_server_arn}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not find tracking server: {e}\")\n",
    "    mlflow_tracking_server_arn = None\n",
    "\n",
    "# ARN of your MLflow Tracking Server\n",
    "# Find this in the SageMaker console or by running `aws sagemaker list-mlflow-tracking-servers`\n",
    "# mlflow_tracking_server_arn = tracking_server_arn\n",
    "\n",
    "\n",
    "# IAM role for SageMaker execution\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"SageMaker Role ARN: {sagemaker_role}\")\n",
    "print(f\"MLflow Tracking Server ARN: {mlflow_tracking_server_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd155d9-ea39-4610-924a-2217a155eb8a",
   "metadata": {},
   "source": [
    "## Read and combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f530a69f-ca1e-4566-a4d8-1d5d561bd4a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T09:08:47.943889Z",
     "iopub.status.busy": "2025-08-24T09:08:47.943580Z",
     "iopub.status.idle": "2025-08-24T09:14:33.821808Z",
     "shell.execute_reply": "2025-08-24T09:14:33.821198Z",
     "shell.execute_reply.started": "2025-08-24T09:08:47.943868Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous combined file removed from S3.\n",
      "Processing c17.p001.csv | User=001\n",
      "Processing c17.p002.csv | User=002\n",
      "Processing c17.p003.csv | User=003\n",
      "Processing c17.p004.csv | User=004\n",
      "Processing c17.p005.csv | User=005\n",
      "Processing c17.p006.csv | User=006\n",
      "Processing c17.p007.csv | User=007\n",
      "Processing c17.p008.csv | User=008\n",
      "Processing c17.p009.csv | User=009\n",
      "Processing c17.p010.csv | User=010\n",
      "Processing c17.p011.csv | User=011\n",
      "Processing c17.p012.csv | User=012\n",
      "Processing c17.p013.csv | User=013\n",
      "Processing c17.p014.csv | User=014\n",
      "Processing c17.p015.csv | User=015\n",
      "Processing c17.p016.csv | User=016\n",
      "Processing c17.p017.csv | User=017\n",
      "Processing c17.p018.csv | User=018\n",
      "Processing c17.p019.csv | User=019\n",
      "Processing c17.p020.csv | User=020\n",
      "Processing c17.p021.csv | User=021\n",
      "Processing c17.p022.csv | User=022\n",
      "Processing c17.p023.csv | User=023\n",
      "Processing c17.p024.csv | User=024\n",
      "Processing c17.p025.csv | User=025\n",
      "Processing c17.p026.csv | User=026\n",
      "Processing c17.p027.csv | User=027\n",
      "Processing c17.p028.csv | User=028\n",
      "Processing c17.p029.csv | User=029\n",
      "Processing c17.p030.csv | User=030\n",
      "Processing c17.p031.csv | User=031\n",
      "Processing c17.p032.csv | User=032\n",
      "Processing c17.p033.csv | User=033\n",
      "Processing c17.p034.csv | User=034\n",
      "Processing c17.p035.csv | User=035\n",
      "Processing c17.p036.csv | User=036\n",
      "Processing c17.p037.csv | User=037\n",
      "Processing c17.p038.csv | User=038\n",
      "Processing c17.p039.csv | User=039\n",
      "Processing c17.p040.csv | User=040\n",
      "Processing c17.p041.csv | User=041\n",
      "Processing c17.p042.csv | User=042\n",
      "Processing c17.p043.csv | User=043\n",
      "Processing c17.p044.csv | User=044\n",
      "Processing c17.p045.csv | User=045\n",
      "Processing c17.p046.csv | User=046\n",
      "Processing c17.p047.csv | User=047\n",
      "Processing c17.p048.csv | User=048\n",
      "Processing c17.p049.csv | User=049\n",
      "Processing c17.p050.csv | User=050\n",
      "Processing c17.p051.csv | User=051\n",
      "Processing c17.p052.csv | User=052\n",
      "Processing c17.p053.csv | User=053\n",
      "Processing c17.p054.csv | User=054\n",
      "Processing c17.p055.csv | User=055\n",
      "Processing c17.p056.csv | User=056\n",
      "Processing c17.p057.csv | User=057\n",
      "Processing c17.p058.csv | User=058\n",
      "Processing c17.p059.csv | User=059\n",
      "Processing c17.p060.csv | User=060\n",
      "Processing c17.p061.csv | User=061\n",
      "Processing c17.p062.csv | User=062\n",
      "Processing c17.p063.csv | User=063\n",
      "Processing c17.p064.csv | User=064\n",
      "Processing c17.p065.csv | User=065\n",
      "Processing c17.p066.csv | User=066\n",
      "Processing c17.p067.csv | User=067\n",
      "Processing c17.p068.csv | User=068\n",
      "Processing c17.p069.csv | User=069\n",
      "Processing c17.p070.csv | User=070\n",
      "Processing c17.p071.csv | User=071\n",
      "Processing c17.p072.csv | User=072\n",
      "Processing c17.p073.csv | User=073\n",
      "Processing c17.p074.csv | User=074\n",
      "Processing c17.p075.csv | User=075\n",
      "Processing c17.p076.csv | User=076\n",
      "Processing c17.p077.csv | User=077\n",
      "Processing c17.p078.csv | User=078\n",
      "Processing c17.p079.csv | User=079\n",
      "Processing c17.p080.csv | User=080\n",
      "Processing c17.p081.csv | User=081\n",
      "Processing c17.p082.csv | User=082\n",
      "Processing c17.p083.csv | User=083\n",
      "Processing c17.p084.csv | User=084\n",
      "Processing c17.p085.csv | User=085\n",
      "Processing c17.p086.csv | User=086\n",
      "Processing c17.p087.csv | User=087\n",
      "Processing c17.p088.csv | User=088\n",
      "Processing c17.p089.csv | User=089\n",
      "Processing c17.p090.csv | User=090\n",
      "Processing c17.p091.csv | User=091\n",
      "Processing c17.p092.csv | User=092\n",
      "Processing c17.p093.csv | User=093\n",
      "Processing c17.p094.csv | User=094\n",
      "Processing c17.p095.csv | User=095\n",
      "Processing c17.p096.csv | User=096\n",
      "Processing c17.p097.csv | User=097\n",
      "Processing c17.p098.csv | User=098\n",
      "Processing c17.p099.csv | User=099\n",
      "Processing c17.p100.csv | User=100\n",
      "Processing c17.p101.csv | User=101\n",
      "Processing c17.p102.csv | User=102\n",
      "Processing c17.p103.csv | User=103\n",
      "Final combined CSV uploaded to: s3://iti113-team12-bucket/project/datasets/c17/working/combined_c17.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === S3 info ===\n",
    "bucket = bucket_name\n",
    "prefix = \"project/datasets/c17/\"\n",
    "output_key = \"project/datasets/c17/working/combined_c17.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Optional: remove previous combined file\n",
    "try:\n",
    "    s3.delete_object(Bucket=bucket, Key=output_key)\n",
    "    print(\"Previous combined file removed from S3.\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Define expected columns\n",
    "dtype_map = {\n",
    "    \"stamp_start\": \"string\",\n",
    "    \"stamp_end\": \"string\",\n",
    "    \"yaw_mean\": \"float64\",\n",
    "    \"pitch_mean\": \"float64\",\n",
    "    \"roll_mean\": \"float64\",\n",
    "    \"rotation_rate_x_mean\": \"float64\",\n",
    "    \"rotation_rate_y_mean\": \"float64\",\n",
    "    \"rotation_rate_z_mean\": \"float64\",\n",
    "    \"user_acceleration_x_mean\": \"float64\",\n",
    "    \"user_acceleration_y_mean\": \"float64\",\n",
    "    \"user_acceleration_z_mean\": \"float64\",\n",
    "    \"yaw_std\": \"float64\",\n",
    "    \"pitch_std\": \"float64\",\n",
    "    \"roll_std\": \"float64\",\n",
    "    \"rotation_rate_x_std\": \"float64\",\n",
    "    \"rotation_rate_y_std\": \"float64\",\n",
    "    \"rotation_rate_z_std\": \"float64\",\n",
    "    \"user_acceleration_x_std\": \"float64\",\n",
    "    \"user_acceleration_y_std\": \"float64\",\n",
    "    \"user_acceleration_z_std\": \"float64\",\n",
    "    \"rotation_magnitude_mean\": \"float64\",\n",
    "    \"acceleration_magnitude_mean\": \"float64\",\n",
    "    \"rotation_magnitude_std\": \"float64\",\n",
    "    \"acceleration_magnitude_std\": \"float64\",\n",
    "    \"speed_mean\": \"float64\",\n",
    "    \"speed_std\": \"float64\",\n",
    "    \"course_mode\": \"string\",\n",
    "    \"course_std\": \"float64\",\n",
    "    \"distance_from_home_mean\": \"float64\",\n",
    "    \"distance_from_home_std\": \"float64\",\n",
    "    \"distance_from_home_latitude_mean\": \"float64\",\n",
    "    \"distance_from_home_latitude_std\": \"float64\",\n",
    "    \"distance_from_home_longitude_mean\": \"float64\",\n",
    "    \"distance_from_home_longitude_std\": \"float64\",\n",
    "    \"bearing_from_home_mode\": \"string\",\n",
    "    \"bearing_from_home_std\": \"float64\",\n",
    "    \"time_of_day_radians\": \"float64\",\n",
    "    \"time_of_day_sin\": \"float64\",\n",
    "    \"time_of_day_cos\": \"float64\",\n",
    "    \"day_of_week\": \"int8\",\n",
    "    \"activity_label\": \"string\"\n",
    "}\n",
    "\n",
    "expected_cols = list(dtype_map.keys())\n",
    "\n",
    "local_tmp = \"/tmp/combined_c17.csv\"\n",
    "if os.path.exists(local_tmp):\n",
    "    os.remove(local_tmp)\n",
    "\n",
    "bad_rows_report = []\n",
    "\n",
    "# === List all CSV files ===\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "if \"Contents\" not in response:\n",
    "    print(\"No files found in S3 path.\")\n",
    "else:\n",
    "    for obj in response[\"Contents\"]:\n",
    "        key = obj[\"Key\"]\n",
    "        if not key.endswith(\".csv\") or any(x in key.lower() for x in [\"combined\", \"train\", \"test\"]):\n",
    "            continue\n",
    "\n",
    "        filename = os.path.basename(key)\n",
    "        user_id = filename.split(\".\")[1].strip(\"p\")  # e.g., \"1\"\n",
    "        print(f\"Processing {filename} | User={user_id}\")\n",
    "\n",
    "        try:\n",
    "            s3_obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "            chunk_size = 100_000\n",
    "\n",
    "            for chunk in pd.read_csv(s3_obj[\"Body\"], chunksize=chunk_size, low_memory=False, on_bad_lines=\"warn\"):\n",
    "                # Keep only expected columns\n",
    "                extra_cols = [c for c in chunk.columns if c not in expected_cols]\n",
    "                missing_cols = [c for c in expected_cols if c not in chunk.columns]\n",
    "\n",
    "                if extra_cols or missing_cols:\n",
    "                    bad_rows_report.append({\"file\": filename, \"extra\": extra_cols, \"missing\": missing_cols})\n",
    "\n",
    "                # Align columns\n",
    "                for c in missing_cols:\n",
    "                    chunk[c] = pd.NA\n",
    "                chunk = chunk[expected_cols]\n",
    "\n",
    "                # Filter rows with missing activity_label\n",
    "                filtered_chunk = chunk[chunk[\"activity_label\"].notna()].copy()\n",
    "                filtered_chunk[\"user_id\"] = user_id\n",
    "\n",
    "                # Append to local temp CSV\n",
    "                filtered_chunk.to_csv(local_tmp, mode=\"a\", header=not os.path.exists(local_tmp), index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {key}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Upload final combined CSV to S3\n",
    "    s3.upload_file(local_tmp, bucket, output_key)\n",
    "    print(\"Final combined CSV uploaded to:\", f\"s3://{bucket}/{output_key}\")\n",
    "\n",
    "    if bad_rows_report:\n",
    "        print(\"Warning: Some files had unexpected columns. Review them:\")\n",
    "        for r in bad_rows_report:\n",
    "            print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95dc5836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:13.518397Z",
     "iopub.status.busy": "2025-08-25T07:47:13.518075Z",
     "iopub.status.idle": "2025-08-25T07:47:20.536062Z",
     "shell.execute_reply": "2025-08-25T07:47:20.535555Z",
     "shell.execute_reply.started": "2025-08-25T07:47:13.518378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded combined dataset:\n",
      "               stamp_start                stamp_end  yaw_mean  pitch_mean  \\\n",
      "0  2023-01-28 10:57:21.821  2023-01-28 10:58:21.784 -0.690362   -0.608920   \n",
      "1  2023-01-28 10:57:22.821  2023-01-28 10:58:22.783 -0.659091   -0.607907   \n",
      "2  2023-01-28 10:57:23.820  2023-01-28 10:58:23.783 -0.616051   -0.607774   \n",
      "3  2023-01-28 10:57:24.819  2023-01-28 10:58:24.782 -0.579319   -0.609040   \n",
      "4  2023-01-28 10:57:25.819  2023-01-28 10:58:25.781 -0.542710   -0.610304   \n",
      "\n",
      "   roll_mean  rotation_rate_x_mean  rotation_rate_y_mean  \\\n",
      "0  -0.608920              0.014896             -0.021061   \n",
      "1  -0.607907              0.003650             -0.020898   \n",
      "2  -0.607774              0.003679             -0.023021   \n",
      "3  -0.609040              0.003764             -0.022758   \n",
      "4  -0.610304              0.003763             -0.022828   \n",
      "\n",
      "   rotation_rate_z_mean  user_acceleration_x_mean  user_acceleration_y_mean  \\\n",
      "0              0.018753                  0.000663                  0.000663   \n",
      "1              0.025638                  0.000259                  0.000259   \n",
      "2              0.024974                  0.000496                  0.000496   \n",
      "3              0.024867                  0.000511                  0.000511   \n",
      "4              0.024683                  0.000551                  0.000551   \n",
      "\n",
      "   ...  distance_from_home_longitude_mean  distance_from_home_longitude_std  \\\n",
      "0  ...                           0.000023                          0.000003   \n",
      "1  ...                           0.000023                          0.000003   \n",
      "2  ...                           0.000023                          0.000003   \n",
      "3  ...                           0.000023                          0.000003   \n",
      "4  ...                           0.000023                          0.000003   \n",
      "\n",
      "   bearing_from_home_mode  bearing_from_home_std  time_of_day_radians  \\\n",
      "0                   166.0               1.846750             2.871067   \n",
      "1                   166.0               1.740563             2.871067   \n",
      "2                   166.0               1.636671             2.871067   \n",
      "3                   166.0               1.563089             2.871067   \n",
      "4                   166.0               1.533636             2.871067   \n",
      "\n",
      "   time_of_day_sin  time_of_day_cos  day_of_week  activity_label  user_id  \n",
      "0         0.267238         -0.96363          5.0             job        1  \n",
      "1         0.267238         -0.96363          5.0             job        1  \n",
      "2         0.267238         -0.96363          5.0             job        1  \n",
      "3         0.267238         -0.96363          5.0             job        1  \n",
      "4         0.267238         -0.96363          5.0             job        1  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    full_data_output_path = \"s3://iti113-team12-bucket/project/datasets/c17/working/combined_c17.csv\"\n",
    "    df = pd.read_csv(full_data_output_path)#, on_bad_lines=\"skip\")\n",
    "    print(\"Successfully loaded combined dataset:\")\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"\\nError loading combined csv file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4860abaf-5b5c-458d-81c2-ccf7bec38894",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T09:21:46.208719Z",
     "iopub.status.busy": "2025-08-24T09:21:46.208400Z",
     "iopub.status.idle": "2025-08-24T09:21:46.224043Z",
     "shell.execute_reply": "2025-08-24T09:21:46.223439Z",
     "shell.execute_reply.started": "2025-08-24T09:21:46.208699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 <span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"font-weight: bold; text-decoration: underline\">bad_rows</span>)                                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008700; text-decoration-color: #008700\">'bad_rows'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1 \u001b[96mprint\u001b[0m(\u001b[1;4mbad_rows\u001b[0m)                                                                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[38;2;0;135;0m'bad_rows'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(bad_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0b632-8c9f-4a7d-b29c-c8ff7033976f",
   "metadata": {},
   "source": [
    "## Create scripts to be used in SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278363b1",
   "metadata": {},
   "source": [
    "#### 0. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f591000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:20.537135Z",
     "iopub.status.busy": "2025-08-25T07:47:20.536917Z",
     "iopub.status.idle": "2025-08-25T07:47:20.543086Z",
     "shell.execute_reply": "2025-08-25T07:47:20.542670Z",
     "shell.execute_reply.started": "2025-08-25T07:47:20.537120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting health_featureEngr.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile health_featureEngr.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input-path', type=str, required=True, help=\"Directory containing main dataset\")\n",
    "    parser.add_argument(\"--output-path\", type=str, required=True, help=\"Output directory for dataset with engineered features\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Use provided paths or fall back to SageMaker defaults\n",
    "    input_path = args.input_path\n",
    "    output_path = args.output_path\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Load data\n",
    "    input_file = os.path.join(input_path, 'combined_c17.csv')\n",
    "    print(f\"Reading input file from {input_file}...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # ==============================================\n",
    "\n",
    "    print(\"Engineering features...\")\n",
    "\n",
    "    # --------------------------------\n",
    "    # Cleaning speed & course features\n",
    "    # --------------------------------\n",
    "    # Change speed, course -1 to NaN\n",
    "    df[[\"speed_mean\", \"speed_std\", \"course_mode\", \"course_std\"]] = df[[\"speed_mean\", \"speed_std\", \"course_mode\", \"course_std\"]].replace(-1, np.nan)\n",
    "    # drop rows with empty speed, course\n",
    "    df2 = df[\n",
    "        df[\"course_mode\"].notna() &\n",
    "        df[\"speed_mean\"].notna() &\n",
    "        df[\"speed_std\"].notna() &\n",
    "        df[\"course_std\"].notna()\n",
    "    ].copy().reset_index(drop=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Timestamp feature engineering\n",
    "    # -----------------------------\n",
    "    # Convert 'stamp_start' and 'stamp_end' into 'timeframe'\n",
    "    df2[\"start_time\"] = pd.to_datetime(df2[\"stamp_start\"]) # Convert to datetime\n",
    "    df2[\"end_time\"] = pd.to_datetime(df2[\"stamp_end\"])\n",
    "    df2[\"timeframe\"] = df2[\"end_time\"] - df2[\"start_time\"] # Row-wise timeframe (end - start)\n",
    "    df2[\"timeframe_seconds\"] = df2[\"timeframe\"].dt.total_seconds()\n",
    "\n",
    "    # Get total duration of each activity for each user, each instance\n",
    "    df2[\"stamp_start\"] = pd.to_datetime(df2[\"stamp_start\"], errors=\"coerce\")\n",
    "    df2[\"end_time\"] = pd.to_datetime(df2[\"end_time\"], errors=\"coerce\")\n",
    "\n",
    "    df2 = df2.sort_values([     # Sort by user, activity, and start time\n",
    "        \"user_id\", \n",
    "        \"activity_label\", \n",
    "        \"stamp_start\"]).reset_index(drop=True)\n",
    "    df2[\"activity_instance\"] = (\n",
    "        (df2[\"stamp_start\"] > df2.groupby([\"user_id\", \"activity_label\"])[\"end_time\"].shift())\n",
    "        .groupby([df2[\"user_id\"], df2[\"activity_label\"]])\n",
    "        .cumsum()\n",
    "    )   # Define new instance when gap occurs between end time and new start time\n",
    "    total_time = (\n",
    "        df2.groupby([\"user_id\", \"activity_label\", \"activity_instance\"])\n",
    "        .agg(duration=(\"end_time\", lambda x: x.max() - x.min()))\n",
    "        .reset_index()\n",
    "    )   # Compute duration per instance\n",
    "    total_time[\"duration_minutes\"] = total_time[\"duration\"].dt.total_seconds() / 60 # convert to mins\n",
    "    df2 = df2.merge(total_time, on=[\"user_id\", \"activity_label\", \"activity_instance\"], how=\"left\")\n",
    "\n",
    "    # frequency of activity per user per day & per user per week\n",
    "    df2[\"date\"] = df2[\"start_time\"].dt.date\n",
    "    df2[\"week\"] = df2[\"start_time\"].dt.to_period(\"W\").apply(lambda r: r.start_time.date())\n",
    "    daily_freq = (\n",
    "        df2.groupby([\"user_id\", \"activity_label\", \"date\"])[\"activity_instance\"]\n",
    "        .nunique() # count unique activity instances\n",
    "        .reset_index(name=\"daily_freq\")\n",
    "    )\n",
    "    weekly_freq = (\n",
    "        df2.groupby([\"user_id\", \"activity_label\", \"week\"])[\"activity_instance\"]\n",
    "        .nunique()\n",
    "        .reset_index(name=\"weekly_freq\")\n",
    "    )\n",
    "    df2 = df2.merge(daily_freq, on=[\"user_id\", \"activity_label\", \"date\"], how=\"left\")\n",
    "    df2 = df2.merge(weekly_freq, on=[\"user_id\", \"activity_label\", \"week\"], how=\"left\")\n",
    "\n",
    "    # drop unused\n",
    "    df2 = df2.drop(columns=['stamp_start','stamp_end',\n",
    "                        'start_time','end_time',\n",
    "                        'date','week',\n",
    "                        'duration','timeframe'])\n",
    "\n",
    "    # -------------------------------\n",
    "    # Cyclic day-of-week features\n",
    "    # ---------------------------\n",
    "    df2[\"day_of_week_sin\"] = np.sin(2*np.pi*df2[\"day_of_week\"]/7)\n",
    "    df2[\"day_of_week_cos\"] = np.cos(2*np.pi*df2[\"day_of_week\"]/7)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Activity duration normalized per user\n",
    "    # (compares each activity instance against that user’s typical duration across all activities)\n",
    "    # -------------------------------------\n",
    "    # calculate duration per activity instance \n",
    "    df_instance = (\n",
    "        df2.groupby(['user_id', 'activity_label', 'activity_instance'])\n",
    "        .agg(duration_minutes=('duration_minutes', 'first'))  # or recalc if needed\n",
    "        .reset_index()\n",
    "    )\n",
    "    # Compute mean and std per user\n",
    "    user_stats = (\n",
    "        df_instance.groupby('user_id')['duration_minutes']\n",
    "                .agg(user_mean='mean', user_std='std')\n",
    "                .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge back\n",
    "    df_instance = df_instance.merge(user_stats, on='user_id', how='left')\n",
    "\n",
    "    # Compute z-score\n",
    "    df_instance['duration_zscore_user_all_acts'] = (\n",
    "        (df_instance['duration_minutes'] - df_instance['user_mean']) / df_instance['user_std']\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Activity duration normalized per user per activity\n",
    "    # (compares each instance against that user's pattern for that specific activity)\n",
    "    # -----------------------------\n",
    "    # Compute mean and std per user & activity\n",
    "    user_act_stats = (\n",
    "        df_instance.groupby(['user_id', 'activity_label'])['duration_minutes']\n",
    "                .agg(user_act_mean='mean', user_act_std='std')\n",
    "                .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge back\n",
    "    df_instance = df_instance.merge(user_act_stats, on=['user_id', 'activity_label'], how='left')\n",
    "\n",
    "    # Compute z-score\n",
    "    df_instance['duration_zscore_user_each_act'] = (\n",
    "        (df_instance['duration_minutes'] - df_instance['user_act_mean']) / df_instance['user_act_std']\n",
    "    )\n",
    "    # z-score = 0 means this activity is the user's typical duration\n",
    "    df_instance['duration_zscore_user_each_act'].fillna(0, inplace=True)\n",
    "    # Merge z-scores back to df2\n",
    "    df2 = df2.merge(\n",
    "            df_instance[['user_id', 'activity_label', 'activity_instance',\n",
    "                        'duration_zscore_user_all_acts', 'duration_zscore_user_each_act']],\n",
    "            on=['user_id', 'activity_label', 'activity_instance'],\n",
    "            how='left'\n",
    "    )\n",
    "\n",
    "    # ----------------\n",
    "    # Combinations of motion\n",
    "    # -----------------\n",
    "    df2[\"total_orientation_variability\"] = df2[\"yaw_std\"] + df2[\"pitch_std\"] + df2[\"roll_std\"]\n",
    "    df2[\"total_orientation\"] = df2[\"yaw_mean\"] + df2[\"pitch_mean\"] + df2[\"roll_mean\"]\n",
    "    df2[\"total_motion\"] = df2[\"rotation_magnitude_mean\"] + df2[\"acceleration_magnitude_mean\"]\n",
    "    df2[\"total_motion_variability\"] = df2[\"rotation_magnitude_std\"] + df2[\"acceleration_magnitude_std\"]\n",
    "    df2[\"speed_n_orientation_stability\"] = df2[\"speed_mean\"] / (df2[\"rotation_magnitude_std\"] + 1e-6) # 1e-6 to avoid division by 0\n",
    "\n",
    "    # ----------------\n",
    "    # Combinations of time-related features\n",
    "    # ----------------\n",
    "    df2[\"duration_n_daily_freq\"] = df2[\"duration_minutes\"] * df2[\"daily_freq\"]\n",
    "    df2[\"duration_n_weekly_freq\"] = df2[\"duration_minutes\"] * df2[\"weekly_freq\"]\n",
    "\n",
    "    # ------------\n",
    "    # Binning distance from home mean\n",
    "    # --------------\n",
    "    # fixed bins (e.g. in meters)\n",
    "    bins = [0, 50, 500, 2000, 5000, float('inf')]\n",
    "    labels = ['At home (0-50m)', 'Within walking-distance (50-500m)', '30-mins-walk (500-2km)', 'Far (2km-5km)', 'Very far (>5km)']\n",
    "\n",
    "    df2['distance_from_home_mean_bin_fixed'] = pd.cut(\n",
    "        df2['distance_from_home_mean'],\n",
    "        bins=bins,\n",
    "        labels=labels,\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    df2 = df2.drop(columns='distance_from_home_mean')\n",
    "\n",
    "    # ========================================\n",
    "    \n",
    "\n",
    "    # Save data with engineered features\n",
    "    data_output = os.path.join(output_path, \"combined_c17_engineered.csv\")\n",
    "\n",
    "    print(f\"Saving train to {data_output}\")\n",
    "    df2.to_csv(data_output, index=False)\n",
    "\n",
    "    print(\"Feature engineering complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227f1c8-50fa-450f-84ef-a8de46986feb",
   "metadata": {},
   "source": [
    "#### 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c615a45-8a7e-4165-8eb7-ccaf4215699d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:20.543776Z",
     "iopub.status.busy": "2025-08-25T07:47:20.543626Z",
     "iopub.status.idle": "2025-08-25T07:47:20.549667Z",
     "shell.execute_reply": "2025-08-25T07:47:20.549236Z",
     "shell.execute_reply.started": "2025-08-25T07:47:20.543762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting health_datacleaning.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile health_datacleaning.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input-path', type=str, help=\"Directory containing main dataset\")\n",
    "    parser.add_argument(\"--output-train-path\", type=str, help=\"Output directory for train.csv\")\n",
    "    parser.add_argument(\"--output-test-path\", type=str, help=\"Output directory for test.csv\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Use provided paths or fall back to SageMaker defaults\n",
    "    input_path = args.input_path\n",
    "    output_train_path = args.output_train_path\n",
    "    output_test_path = args.output_test_path\n",
    "\n",
    "    os.makedirs(output_train_path, exist_ok=True)\n",
    "    os.makedirs(output_test_path, exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    input_file = os.path.join(input_path, 'combined_c17_engineered.csv')\n",
    "\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"Required input file not found: {input_file}\")\n",
    "    else:\n",
    "        print(f\"Reading input file from {input_file}...\")\n",
    "    df2 = pd.read_csv(input_file)\n",
    "\n",
    "    # ===========================\n",
    "    print('Cleaning data...')\n",
    "    # ---------\n",
    "    # drop duplicates based on all columns except 'user_id', keep first row\n",
    "    # ---------\n",
    "    print('Removing duplicates...')\n",
    "    df2 = df2.drop_duplicates(subset=df2.columns.difference(['user_id']), keep='first')\n",
    "\n",
    "    # ------------------------\n",
    "    # Standardize activities\n",
    "    # (not necessary for labelling activities!)\n",
    "    # ------------------------\n",
    "    # # Convert to lowercase for easier matching\n",
    "    # df2['activity_label'] = df2['activity_label'].astype(str).str.lower()\n",
    "\n",
    "    # # 4. Activity categorization by health relevance\n",
    "    # health_categories = {\n",
    "    #     'Essential ADLs': ['chores', 'eat', 'hygiene'],\n",
    "    #     'Health Indicators': ['exercise', 'sleep'],  \n",
    "    #     'Independence/Social': ['errands', 'travel'],\n",
    "    #     'Leisure/Behavioral': ['relax', 'entertainment', 'hobby'],\n",
    "    #     'Cognitive': ['job','school']\n",
    "    # }\n",
    "\n",
    "    # # Invert the mapping: activity -> category\n",
    "    # activity_to_category = {activity: category \n",
    "    #                         for category, activities in health_categories.items() \n",
    "    #                         for activity in activities}\n",
    "\n",
    "    # # Map to a new column\n",
    "    # df2[\"activity_category\"] = df2[\"activity_label\"].map(activity_to_category)\n",
    "\n",
    "    # # Categorizing into ADLs\n",
    "    # adl_mapping = {\n",
    "    #     \"ADL\": [\"eat\", \"hygiene\", \"chores\", \"exercise\", \"sleep\"],\n",
    "    #     \"IADL\": [\"errands\", \"travel\", \"relax\", \"entertainment\", \"hobby\", \"job\", \"school\"]\n",
    "    # }\n",
    "\n",
    "    # # Map to new column\n",
    "    # df2[\"ADL_category\"] = df2[\"activity_label\"].map(\n",
    "    #     {a: k for k, v in adl_mapping.items() for a in v}\n",
    "    # )\n",
    "\n",
    "    # Drop some less relevant activities\n",
    "    print('Dropping some activities...')\n",
    "    activities_to_drop = [\"job\", \"school\", \"travel\"]\n",
    "    df2 = df2[~df2[\"activity_label\"].isin(activities_to_drop)].reset_index(drop=True)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    print('Dropping some features...')\n",
    "    # --------------\n",
    "    # Drop features\n",
    "    # --------------\n",
    "\n",
    "    # Drop correlated features & weak predictors\n",
    "    features_to_drop = [\n",
    "        \"user_acceleration_y_mean\",\n",
    "        \"user_acceleration_y_std\",\n",
    "        \"total_motion\",\n",
    "        \"total_motion_variability\",\n",
    "        \"weekly_freq\",\n",
    "        \"rotation_magnitude_std\",\n",
    "        \"time_of_day_radians\",\n",
    "        \"rotation_rate_x_std\",\n",
    "        \"acceleration_magnitude_mean\",\n",
    "        \"user_acceleration_z_std\",\n",
    "        \"distance_from_home_std\",\n",
    "        \"rotation_magnitude_mean\",\n",
    "        \"user_acceleration_x_std\",\n",
    "        \"user_acceleration_y_std\",\n",
    "        \"daily_freq\",\n",
    "        \"rotation_rate_y_std\",\n",
    "        \"rotation_rate_x_std\",\n",
    "        \"rotation_rate_z_std\",\n",
    "        \"rotation_rate_y_std\",\n",
    "        \"rotation_rate_y_std\",\n",
    "        \"user_acceleration_x_std\",\n",
    "        \"user_acceleration_y_std\",\n",
    "        \"rotation_rate_x_std\",\n",
    "        \"rotation_rate_y_std\",\n",
    "        \"user_acceleration_x_std\",\n",
    "        \"user_acceleration_y_std\",\n",
    "        \"rotation_rate_z_std\",\n",
    "        \"rotation_rate_z_std\",\n",
    "        \"rotation_rate_x_std\",\n",
    "        \"yaw_std\",\n",
    "        \"acceleration_magnitude_std\",\n",
    "        \"user_acceleration_z_std\",\n",
    "        \"rotation_rate_y_std\",\n",
    "        \"rotation_rate_y_std\",\n",
    "        \"rotation_rate_z_std\",\n",
    "        \"rotation_rate_z_std\",\n",
    "        \"time_of_day_radians\",\n",
    "        \"user_acceleration_z_std\",\n",
    "        \"total_motion_variability\",\n",
    "        \"rotation_magnitude_std\",\n",
    "        \"rotation_rate_y_std\",\n",
    "        \"rotation_magnitude_std\",\n",
    "        \"rotation_rate_y_std\",\n",
    "        \"rotation_magnitude_std\",\n",
    "        \"rotation_rate_x_std\",\n",
    "        \"user_acceleration_x_std\",\n",
    "        \"user_acceleration_y_std\",\n",
    "        \"rotation_rate_y_std\",\n",
    "        \"roll_std\",\n",
    "        \"user_acceleration_z_std\",\n",
    "        \"rotation_rate_z_std\",\n",
    "        \"rotation_rate_x_std\",\n",
    "        \"rotation_rate_x_std\",\n",
    "        \"rotation_rate_x_std\",\n",
    "        \"rotation_rate_x_std\",\n",
    "        \"rotation_rate_z_std\",\n",
    "        \"acceleration_magnitude_std\",\n",
    "        \"acceleration_magnitude_std\",\n",
    "        \"user_acceleration_x_std\",\n",
    "        \"user_acceleration_y_std\",\n",
    "        \"rotation_magnitude_std\",\n",
    "        \"rotation_rate_x_std\",\n",
    "        \"user_acceleration_z_std\",\n",
    "        \"distance_from_home_latitude_std\",\n",
    "        \"distance_from_home_longitude_std\"\n",
    "    ]\n",
    "\n",
    "    # Dropping additional features\n",
    "    print('Dropping additional features...')\n",
    "    addn_features_to_drop = [\n",
    "        'time_of_day_cos',\n",
    "        'time_of_day_sin',\n",
    "        'day_of_week',\n",
    "        'day_of_week_sin',\n",
    "        'day_of_week_cos',\n",
    "        'distance_from_home_latitude_mean',\n",
    "        'distance_from_home_longitude_mean',\n",
    "        'bearing_from_home_mode',\n",
    "        'bearing_from_home_std',\n",
    "        'distance_from_home_mean_bin_fixed',\n",
    "        'speed_mean',\n",
    "        'speed_std',\n",
    "        'course_mode',\n",
    "        'course_std'\n",
    "    ]\n",
    "\n",
    "    df3 = df2.drop(columns=features_to_drop+['user_id']+addn_features_to_drop)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # =================\n",
    "    # Splitting data\n",
    "    # =================\n",
    "    print(\"Splitting into train/test...\")\n",
    "    # Stratified train-test split (stratify by health_status)\n",
    "    df3 = df3.dropna(subset=['activity_label'])  # can't stratify on NaNs in target (if any)\n",
    "    train_df, test_df = train_test_split(\n",
    "        df3,\n",
    "        test_size=0.2,\n",
    "        stratify=df3['activity_label'],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Save splits\n",
    "    train_output = os.path.join(output_train_path, \"train.csv\")\n",
    "    test_output = os.path.join(output_test_path, \"test.csv\")\n",
    "\n",
    "    print(f\"Saving train to {train_output}\")\n",
    "    train_df.to_csv(train_output, index=False)\n",
    "\n",
    "    print(f\"Saving test to {test_output}\")\n",
    "    test_df.to_csv(test_output, index=False)\n",
    "\n",
    "    print(\"Data cleaning complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e62d56b-3fc8-4151-99cd-da1d50b4d05b",
   "metadata": {},
   "source": [
    "#### 2. Preprocessor (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c99d8c87-5218-4c4c-a172-555bfa0168d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:20.550817Z",
     "iopub.status.busy": "2025-08-25T07:47:20.550585Z",
     "iopub.status.idle": "2025-08-25T07:47:20.556214Z",
     "shell.execute_reply": "2025-08-25T07:47:20.555789Z",
     "shell.execute_reply.started": "2025-08-25T07:47:20.550803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting health_preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile health_preprocessor.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# ---------------------\n",
    "# Include GroupImputer\n",
    "# ---------------------\n",
    "# -----------\n",
    "# Main script\n",
    "# ------------\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input-train-path', type=str, help=\"Previous output directory for train.csv\")\n",
    "    parser.add_argument('--output-train-path', type=str, help=\"Output directory for processed_train.csv\")\n",
    "    parser.add_argument('--input-test-path', type=str, help=\"Previous output directory for test.csv\")\n",
    "    parser.add_argument('--output-test-path', type=str, help=\"Output directory for processed_test.csv\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    input_train_path = os.path.join(args.input_train_path, \"train.csv\")\n",
    "    output_train_path = args.output_train_path\n",
    "    input_test_path = os.path.join(args.input_test_path, \"test.csv\")\n",
    "    output_test_path = args.output_test_path\n",
    "\n",
    "    # Make output directory\n",
    "    os.makedirs(output_train_path, exist_ok=True)\n",
    "    os.makedirs(output_test_path, exist_ok=True)\n",
    "\n",
    "    # Load train and test data\n",
    "  \n",
    "    if not os.path.exists(input_train_path):\n",
    "        raise FileNotFoundError(f\"Train data not found: {input_train_path}\")\n",
    "    print(f\"Reading input file from {input_train_path}...\")\n",
    "    train_df = pd.read_csv(input_train_path)\n",
    "\n",
    "    if not os.path.exists(input_test_path):\n",
    "        raise FileNotFoundError(f\"Test data not found: {input_test_path}\")\n",
    "    print(f\"Reading input file from {input_test_path}...\")\n",
    "    test_df = pd.read_csv(input_test_path)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Balancing imbalanced data (unused)\n",
    "    # ---------------------------\n",
    "    print('Balancing imbalanced data...')\n",
    "    X_train = train_df.drop(columns=['activity_label'])\n",
    "    y_train = train_df['activity_label']\n",
    "\n",
    "    # # Combine training features and target for resampling\n",
    "    # train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    # # Separate majority and minority classes in the training set only\n",
    "    # train_majority = train_df[train_df.health_status == 0]\n",
    "    # train_minority = train_df[train_df.health_status == 1]\n",
    "\n",
    "    # len_maj = len(train_majority)\n",
    "    # len_min = len(train_minority)\n",
    "    # n_samples_each = round((len_maj+len_min)/2)\n",
    "    # print(n_samples_each)\n",
    "\n",
    "    # # Upsample minority class in the training set\n",
    "    # train_minority_upsampled = resample(train_minority, \n",
    "    #                                     replace=True,    # sample with replacement\n",
    "    #                                     n_samples=n_samples_each,  # number of samples in upsampled minority class (reduced due to resource constraints)\n",
    "    #                                     random_state=42) # reproducible results\n",
    "\n",
    "    # # Downsample majority class in the training set\n",
    "    # train_majority_downsampled = resample(train_majority, \n",
    "    #                                     replace=False,    # sample without replacement\n",
    "    #                                     n_samples=n_samples_each,   # number of samples in downsampled majority class (reduced due to resource constraints)\n",
    "    #                                     random_state=42)  # reproducible results\n",
    "\n",
    "    # # Combine upsampled minority class with downsampled majority class\n",
    "    # train_combined = pd.concat([train_majority_downsampled, train_minority_upsampled])\n",
    "\n",
    "    # # Shuffle and reset index\n",
    "    # train_combined = train_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # # Re-separate the features and target in the combined training set\n",
    "    # X_train = train_combined.drop(columns='activity_label')\n",
    "    # y_train = train_combined['activity_label']\n",
    "\n",
    "    # Only separate the features and target in the combined test set (without resampling)\n",
    "    X_test = test_df.drop(columns='activity_label')\n",
    "    y_test = test_df['activity_label']\n",
    "    \n",
    "    # ---------------------\n",
    "    # Define pipeline columns\n",
    "    # ---------------------\n",
    "    # nominal_cols_encode = ['distance_from_home_mean_bin_fixed']\n",
    "    numerical_cols_scale = X_train.select_dtypes(include='number').columns.tolist()\n",
    "    \n",
    "    # ---------------------\n",
    "    # Build pipelines\n",
    "    # ---------------------\n",
    "        \n",
    "    # nominal_onehot_pipe = Pipeline([\n",
    "    #     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    # ])\n",
    "\n",
    "    numerical_scale_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        # ('nominal_encode', nominal_onehot_pipe, nominal_cols_encode),\n",
    "        ('num_scale', numerical_scale_pipe, numerical_cols_scale)\n",
    "    ], remainder='passthrough', verbose_feature_names_out=False)\n",
    "\n",
    "    # ---------------------\n",
    "    # Fit on train, transform train/test\n",
    "    # ---------------------\n",
    "    print(\"Train columns:\", X_train.columns.tolist())\n",
    "    print(\"Test columns:\", X_test.columns.tolist())\n",
    "    X_train_trf = preprocessor.fit_transform(X_train)\n",
    "    X_test_trf = preprocessor.transform(X_test)\n",
    "\n",
    "    # Convert back to DataFrame with feature names\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "    X_train_transformed = pd.DataFrame(X_train_trf, columns=feature_names)\n",
    "    X_test_transformed = pd.DataFrame(X_test_trf, columns=feature_names)\n",
    "\n",
    "    # Add back Y\n",
    "    train_transformed = pd.concat([X_train_transformed, y_train], axis=1)\n",
    "    test_transformed = pd.concat([X_test_transformed, y_test], axis=1)\n",
    "    \n",
    "    # ---------------------\n",
    "    # Save transformed datasets\n",
    "    # ---------------------\n",
    "    train_output_file = os.path.join(output_train_path, \"train_processed.csv\")\n",
    "    test_output_file = os.path.join(output_test_path, \"test_processed.csv\")\n",
    "\n",
    "    train_transformed.to_csv(train_output_file, index=False)\n",
    "    test_transformed.to_csv(test_output_file, index=False)\n",
    "\n",
    "    print(f\"Processed train saved to {train_output_file}\")\n",
    "    print(f\"Processed test saved to {test_output_file}\")\n",
    "\n",
    "    print(\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4babc3-1ed9-4644-9478-e7ccae9f99ee",
   "metadata": {},
   "source": [
    "#### 3. Training (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75bc2699-db86-4f29-8789-c9ec3632054b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:20.556807Z",
     "iopub.status.busy": "2025-08-25T07:47:20.556666Z",
     "iopub.status.idle": "2025-08-25T07:47:20.561590Z",
     "shell.execute_reply": "2025-08-25T07:47:20.561174Z",
     "shell.execute_reply.started": "2025-08-25T07:47:20.556794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "# boto3>=1.34\n",
    "# botocore>=1.34\n",
    "mlflow\n",
    "sagemaker-mlflow\n",
    "scikit-learn\n",
    "pandas\n",
    "joblib\n",
    "numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d48e270-46cd-4f7e-bd40-43990e0a513b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:20.562283Z",
     "iopub.status.busy": "2025-08-25T07:47:20.562141Z",
     "iopub.status.idle": "2025-08-25T07:47:20.566973Z",
     "shell.execute_reply": "2025-08-25T07:47:20.566569Z",
     "shell.execute_reply.started": "2025-08-25T07:47:20.562270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting health_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile health_train.py\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Ensure MLflow is installed\n",
    "try:\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow\n",
    "except ImportError:\n",
    "    print(\"Installing MLflow...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"mlflow\", \"sagemaker-mlflow\"])\n",
    "    # subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",  \"boto3==1.28.57\", \"botocore==1.31.85\", \"s3transfer\", \"mlflow\", \"sagemaker-mlflow\"])\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow\n",
    "\n",
    "# import mlflow\n",
    "# import sagemaker_mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import joblib\n",
    "import glob\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "def evaluate_classification(y_target, y_prediction, y_prediction_proba) -> list:\n",
    "    scores = {}\n",
    "    scores['accuracy']  = accuracy_score(y_target, y_prediction)\n",
    "    scores['precision'] = precision_score(y_target, y_prediction, average='macro')\n",
    "    scores['recall']    = recall_score(y_target, y_prediction, average='macro')\n",
    "    scores['f1']        = f1_score(y_target, y_prediction, average='macro')\n",
    "    \n",
    "    scores['roc_auc'] = None\n",
    "    if y_prediction_proba is not None:\n",
    "        try:\n",
    "            scores['roc_auc'] = roc_auc_score(\n",
    "                y_target, y_prediction_proba, multi_class='ovr', average='macro'\n",
    "            )\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    return scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--tracking-server-arn\", type=str, required=True)\n",
    "    parser.add_argument(\"--experiment-name\", type=str, default=\"Default\")\n",
    "    parser.add_argument(\"--model-output-path\", type=str, default=\"/opt/ml/model\", help=\"Output directory for model.joblib\")\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\"--n_estimators\", type=int, default=100)\n",
    "    parser.add_argument(\"--max_depth\", type=int, default=10)\n",
    "    parser.add_argument(\"--min_samples_leaf\", type=int, default=5)\n",
    "    parser.add_argument(\"--min_samples_split\", type=int, default=10)\n",
    "    parser.add_argument(\"--max_features\", type=str, default=\"sqrt\")\n",
    "    parser.add_argument('--bootstrap', action='store_true')\n",
    "    parser.add_argument('--no-bootstrap', dest='bootstrap', action='store_false')\n",
    "    parser.set_defaults(bootstrap=True)\n",
    "\n",
    "    # parser.add_argument(\"--n-estimators\", type=int, default=100)\n",
    "    # parser.add_argument(\"--max-depth\", type=int, default=10)\n",
    "    # parser.add_argument(\"--min-samples-leaf\", type=int, default=5)\n",
    "    # parser.add_argument(\"--max-features\", type=str, default='sqrt')\n",
    "    # parser.add_argument(\"--min-samples-split\", type=int, default=10)\n",
    "    # # parser.add_argument(\"--bootstrap\", type=bool, default=True)\n",
    "    # parser.add_argument('--bootstrap', action='store_true', help=\"Enable bootstrap\")\n",
    "    # parser.add_argument('--no-bootstrap', dest='bootstrap', action='store_false', help=\"Disable bootstrap\")\n",
    "    parser.set_defaults(bootstrap=True)\n",
    "\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Make output directory\n",
    "    model_output_path = args.model_output_path or \"/opt/ml/model\"\n",
    "    os.makedirs(model_output_path, exist_ok=True)\n",
    "    \n",
    "    # Load training data\n",
    "    files = glob.glob(\"/opt/ml/input/data/train/train_processed.csv\")\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Processed train data not found: {input_train}\")\n",
    "    input_train = files[0]\n",
    "\n",
    "    # input_train = glob.glob(\"/opt/ml/input/data/train/train_processed.csv\")[0] # first train_processed.csv\n",
    "    # if not os.path.exists(input_train):\n",
    "    #     raise FileNotFoundError(f\"Processed train data not found: {input_train}\")\n",
    "    df = pd.read_csv(input_train)\n",
    "    X = df.drop(columns='activity_label')\n",
    "    y = df['activity_label']\n",
    "\n",
    "    # Set up MLflow\n",
    "    mlflow.set_tracking_uri(args.tracking_server_arn)\n",
    "    print(\"MLflow tracking URI set successfully.\")\n",
    "    mlflow.set_experiment(args.experiment_name)\n",
    "    print(f\"MLflow tracking URI set to: {mlflow.get_tracking_uri()}\")\n",
    "    print(f\"MLflow experiment set to: '{args.experiment_name}'\")\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        # Log parameters\n",
    "\n",
    "        mlflow.log_param(\"bootstrap\", bool(args.bootstrap)) \n",
    "        mlflow.log_param(\"max_depth\", int(args.max_depth))\n",
    "        mlflow.log_param(\"max_features\", args.max_features)  # str\n",
    "        mlflow.log_param(\"min_samples_leaf\", int(args.min_samples_leaf))\n",
    "        mlflow.log_param(\"min_samples_split\", int(args.min_samples_split))\n",
    "        mlflow.log_param(\"n_estimators\", int(args.n_estimators))\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "            bootstrap = bool(args.bootstrap),\n",
    "            max_depth = int(args.max_depth),\n",
    "            max_features = args.max_features,\n",
    "            min_samples_leaf = int(args.min_samples_leaf),\n",
    "            min_samples_split = int(args.min_samples_split),\n",
    "            n_estimators = int(args.n_estimators),\n",
    "            random_state = 42\n",
    "        )\n",
    "        print(\"Fitting to model...\")\n",
    "        model.fit(X, y)\n",
    "        print(\"Predicting on training set...\")\n",
    "        y_pred = model.predict(X)\n",
    "        y_pred_proba = model.predict_proba(X)\n",
    "\n",
    "        scores = evaluate_classification(y, y_pred, y_pred_proba)\n",
    "\n",
    "        for label, value in scores.items():\n",
    "            mlflow.log_metric(label, value)\n",
    "    \n",
    "        mlflow.sklearn.log_model(sk_model=model, artifact_path=\"model\")\n",
    "\n",
    "    \n",
    "        joblib.dump(model, os.path.join(args.model_output_path, \"model.joblib\"))\n",
    "        with open(os.path.join(args.model_output_path, \"run_id.txt\"), \"w\") as f:\n",
    "            f.write(run.info.run_id)\n",
    "    \n",
    "        print(f\"Training complete. Scores: \\n{scores}\")\n",
    "        print(f\"MLflow Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265de90-4f9b-4c63-bb0f-c0ee7229fe1b",
   "metadata": {},
   "source": [
    "#### 4. Evaluation (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01279a12-9585-47c7-ba68-c623b38adfb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:20.567699Z",
     "iopub.status.busy": "2025-08-25T07:47:20.567557Z",
     "iopub.status.idle": "2025-08-25T07:47:20.573745Z",
     "shell.execute_reply": "2025-08-25T07:47:20.573342Z",
     "shell.execute_reply.started": "2025-08-25T07:47:20.567687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting health_evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile health_evaluate.py\n",
    "\n",
    "# Ensure MLflow is installed\n",
    "import sys\n",
    "import subprocess\n",
    "try:\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow\n",
    "except ImportError:\n",
    "    print(\"Installing MLflow...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"mlflow\", \"sagemaker-mlflow\"])\n",
    "    # subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",  \"boto3==1.28.57\", \"botocore==1.31.85\", \"s3transfer\", \"mlflow\", \"sagemaker-mlflow\"])\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import tarfile\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def evaluate_classification(y_target, y_prediction, y_prediction_proba) -> list:\n",
    "    scores = {}\n",
    "    scores['accuracy']  = accuracy_score(y_target, y_prediction)\n",
    "    scores['precision'] = precision_score(y_target, y_prediction, average='macro')\n",
    "    scores['recall']    = recall_score(y_target, y_prediction, average='macro')\n",
    "    scores['f1']        = f1_score(y_target, y_prediction, average='macro')\n",
    "    \n",
    "    scores['roc_auc'] = None\n",
    "    if y_prediction_proba is not None:\n",
    "        try:\n",
    "            scores['roc_auc'] = roc_auc_score(\n",
    "                y_target, y_prediction_proba, multi_class='ovr', average='macro'\n",
    "            )\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    return scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Parse Arguments ---\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model-path\", type=str, required=True, help=\"Previous output directory for model.joblib.\")\n",
    "    parser.add_argument(\"--input-test-path\", type=str, required=True, help=\"Previous output directory for test_processed.csv\")\n",
    "    parser.add_argument(\"--output-path\", type=str, required=True, help=\"Path to save the evaluation.json report.\")\n",
    "    parser.add_argument(\"--model-package-group-name\", type=str, required=True, help=\"Name of the SageMaker Model Package Group.\")\n",
    "    parser.add_argument(\"--region\", type=str, required=True, help=\"The AWS region for creating the boto3 client.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # --- Extract and Load Model ---\n",
    "    # SageMaker packages models in a .tar.gz file. We need to extract it first.\n",
    "    model_archive_path = os.path.join(args.model_path, 'model.tar.gz')\n",
    "    print(f\"Extracting model from archive: {model_archive_path}\")\n",
    "    with tarfile.open(model_archive_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=args.model_path)\n",
    "\n",
    "    # Load the model using joblib\n",
    "    model_file_path = os.path.join(args.model_path, \"model.joblib\")\n",
    "    if not os.path.exists(model_file_path):\n",
    "        raise FileNotFoundError(f\"Model file 'model.joblib' not found after extraction in: {args.model_path}\")\n",
    "    \n",
    "    print(f\"Loading model from: {model_file_path}\")\n",
    "    model = joblib.load(model_file_path)\n",
    "\n",
    "    # Load test data\n",
    "    input_test_path = args.input_test_path\n",
    "    input_test = os.path.join(input_test_path, \"test_processed.csv\")\n",
    "    if not os.path.exists(input_test):\n",
    "        raise FileNotFoundError(f\"Processed test data not found: {input_test}\")\n",
    "    df = pd.read_csv(input_test)\n",
    "    X_test = df.drop(columns='activity_label')\n",
    "    y_test = df['activity_label']\n",
    "\n",
    "    print(\"Running predictions on the test dataset.\")\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "    scores = evaluate_classification(y_test, y_test_pred, y_test_pred_proba)\n",
    "\n",
    "    for label, value in scores.items():\n",
    "        mlflow.log_metric(label, value)\n",
    "\n",
    "    print(f\"Evaluation complete. Scores: \\n{scores}\")\n",
    "\n",
    "    # accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    # f1_macro = f1_score(y_test, y_test_pred, average=\"macro\")\n",
    "    # report = {\"accuracy\": accuracy,\n",
    "    #          \"f1\": f1_macro} # labelled as 'f1' instead of 'f1_macro' in case of change\n",
    "    # print(f\"Calculated accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"Calculated f1_macro: {f1_macro:.4f}\")\n",
    "\n",
    "    # report = {\"scores\": scores}\n",
    "    # Convert your scores dict into SageMaker evaluation schema\n",
    "    # report = {\n",
    "    #     \"binary_classification_metrics\": {\n",
    "    #         \"accuracy\":  {\"value\": float(scores[\"accuracy\"])},\n",
    "    #         \"precision\": {\"value\": float(scores[\"precision\"])},\n",
    "    #         \"recall\":    {\"value\": float(scores[\"recall\"])},\n",
    "    #         \"f1\":        {\"value\": float(scores[\"f1\"])},\n",
    "    #         \"roc_auc\":   {\"value\": float(scores[\"roc_auc\"])} if scores[\"roc_auc\"] is not None else None\n",
    "    #     },\n",
    "    #      \"baseline_exists\": False\n",
    "    # }\n",
    "    # report = {\n",
    "    #     \"binary_classification_metrics\": {\n",
    "    #         \"accuracy\":  {\"value\": float(scores[\"accuracy\"])},\n",
    "    #         \"precision\": {\"value\": float(scores[\"precision\"])},\n",
    "    #         \"recall\":    {\"value\": float(scores[\"recall\"])},\n",
    "    #         \"f1\":        {\"value\": float(scores[\"f1\"])},\n",
    "    #     }\n",
    "    # }\n",
    "    report = {\"accuracy\": scores[\"accuracy\"],\n",
    "              \"precision\": scores[\"precision\"],\n",
    "              \"recall\": scores[\"recall\"],\n",
    "             \"f1\": scores[\"f1\"],\n",
    "             \"roc_auc\": scores[\"roc_auc\"]}\n",
    "\n",
    "\n",
    "    # --- Check for Existing Baseline Model in SageMaker Model Registry ---\n",
    "    print(f\"Checking for baseline model in region: {args.region}\")\n",
    "    sagemaker_client = boto3.client(\"sagemaker\", region_name=args.region)\n",
    "    try:\n",
    "        response = sagemaker_client.list_model_packages(\n",
    "            ModelPackageGroupName=args.model_package_group_name, \n",
    "            ModelApprovalStatus=\"Approved\", # filter for approved models only\n",
    "            SortBy=\"CreationTime\",\n",
    "            SortOrder=\"Descending\",\n",
    "            MaxResults=1,\n",
    "        )\n",
    "        # If the list is not empty, an approved model already exists\n",
    "        report[\"baseline_exists\"] = len(response[\"ModelPackageSummaryList\"]) > 0 # should include all models regardless of approval status\n",
    "        if report[\"baseline_exists\"]:\n",
    "            print(f\"An approved baseline model was found in '{args.model_package_group_name}'.\")\n",
    "        else:\n",
    "             print(f\"No approved baseline model was found in '{args.model_package_group_name}'.\")\n",
    "\n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        # If the ModelPackageGroup doesn't exist, there is no baseline\n",
    "        if \"ResourceNotFound\" in str(e):\n",
    "            report[\"baseline_exists\"] = False\n",
    "            print(f\"Model Package Group '{args.model_package_group_name}' not found. Assuming no baseline exists.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # --- Write Final Report ---\n",
    "    os.makedirs(args.output_path, exist_ok=True)\n",
    "    report_path = os.path.join(args.output_path, \"evaluation.json\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "        \n",
    "    print(f\"✅ Evaluation complete. Report written to: {report_path}\")\n",
    "    print(\"Evaluation Report:\")\n",
    "    print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b086e-e1af-47b4-bca7-eff8cfaca6ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T06:32:40.642926Z",
     "iopub.status.busy": "2025-07-24T06:32:40.642324Z",
     "iopub.status.idle": "2025-07-24T06:32:41.318403Z",
     "shell.execute_reply": "2025-07-24T06:32:41.317486Z",
     "shell.execute_reply.started": "2025-07-24T06:32:40.642900Z"
    }
   },
   "source": [
    "## Define the SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd78c79-c25a-4cee-bcef-7efc35dffce0",
   "metadata": {},
   "source": [
    "#### Pipeline settings & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d755f65-e06c-4ad1-9f9f-8f04ca4ff38e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:20.574321Z",
     "iopub.status.busy": "2025-08-25T07:47:20.574182Z",
     "iopub.status.idle": "2025-08-25T07:47:20.596132Z",
     "shell.execute_reply": "2025-08-25T07:47:20.595726Z",
     "shell.execute_reply.started": "2025-08-25T07:47:20.574309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current SageMaker Execution Role ARN is:\n",
      "arn:aws:iam::837028399719:role/iti113-team12-sagemaker-iti113-team12-domain-iti113-team12-Role\n"
     ]
    }
   ],
   "source": [
    "# Get the role from the current session\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "print(\"Your current SageMaker Execution Role ARN is:\")\n",
    "print(sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1927104-6c34-47b3-919d-3e3b82581ecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:20.596735Z",
     "iopub.status.busy": "2025-08-25T07:47:20.596592Z",
     "iopub.status.idle": "2025-08-25T07:47:20.599443Z",
     "shell.execute_reply": "2025-08-25T07:47:20.599048Z",
     "shell.execute_reply.started": "2025-08-25T07:47:20.596722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying data path:\n",
      "s3://iti113-team12-bucket/project/datasets/c17/working\n"
     ]
    }
   ],
   "source": [
    "# Same as defined above\n",
    "# data_path = f\"s3://{bucket_name}/{base_folder}/datasets/c17/working\"\n",
    "print(\"Verifying data path:\")\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4df01e03-eb61-46c1-a529-87c9297f85b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:20.600774Z",
     "iopub.status.busy": "2025-08-25T07:47:20.600511Z",
     "iopub.status.idle": "2025-08-25T07:47:20.638118Z",
     "shell.execute_reply": "2025-08-25T07:47:20.637699Z",
     "shell.execute_reply.started": "2025-08-25T07:47:20.600760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current SageMaker Execution Role ARN is: arn:aws:iam::837028399719:role/iti113-team12-sagemaker-iti113-team12-domain-iti113-team12-Role\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.parameters import ParameterFloat, ParameterInteger, ParameterString, ParameterBoolean\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "import sagemaker\n",
    "\n",
    "# Get SageMaker role and session\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "print(f\"Your current SageMaker Execution Role ARN is: {sagemaker_role}\")\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Parameters\n",
    "model_package_group_name = \"activityLabelModels\"\n",
    "processing_instance_type = \"ml.m5.large\" #\"ml.t3.medium\"\n",
    "training_instance_type = \"ml.m5.large\"\n",
    "experiment_name_param = ParameterString(name=\"ExperimentName\", default_value=\"c-activityLabelClf\")\n",
    "accuracy_threshold_param = ParameterFloat(name=\"AccuracyThreshold\", default_value=0.8)\n",
    "f1_threshold_param = ParameterFloat(name=\"F1Threshold\", default_value = 0.8) # low for now\n",
    "\n",
    "\n",
    "# Hyperparameters for random forest\n",
    "# Integer hyperparameters\n",
    "n_estimators_param = ParameterInteger(name=\"n_estimators\", default_value=100)\n",
    "max_depth_param = ParameterInteger(name=\"max_depth\", default_value=10)\n",
    "min_samples_leaf_param = ParameterInteger(name=\"min_samples_leaf\", default_value=5)\n",
    "min_samples_split_param = ParameterInteger(name=\"min_samples_split\", default_value=10)\n",
    "# String hyperparameter\n",
    "max_features_param = ParameterString(name=\"max_features\", default_value=\"sqrt\")\n",
    "# Boolean hyperparameter\n",
    "bootstrap_param = ParameterBoolean(name=\"bootstrap\", default_value=True)\n",
    "\n",
    "\n",
    "# Define input/output S3 URIs as pipeline parameters\n",
    "# data_path = f\"s3://{bucket_name}/{base_folder}/datasets/c17/working\"\n",
    "# dataset_s3_path = f\"{data_path}/users/volunteer_details.csv\"\n",
    "input_data_uri = ParameterString(\n",
    "    name=\"InputDataURI\", \n",
    "    default_value=f\"{data_path}/combined_c17.csv\",\n",
    ")\n",
    "engr_data_uri = ParameterString(\n",
    "    name=\"EngrDataURI\", \n",
    "    default_value=f\"{data_path}/combined_c17_engineered.csv\",\n",
    ")\n",
    "train_data_uri = ParameterString(\n",
    "    name=\"TrainDataURI\", \n",
    "    default_value=f\"{data_path}/preprocessed-data/train\" # S3 location\n",
    ")\n",
    "test_data_uri = ParameterString(\n",
    "    name=\"TestDataURI\", \n",
    "    default_value=f\"{data_path}/preprocessed-data/test\" \n",
    ")\n",
    "processed_train_data_uri = ParameterString(\n",
    "    name=\"ProcessedTrainDataURI\", \n",
    "    default_value=f\"{data_path}/preprocessed-data/train\"\n",
    ")\n",
    "processed_test_data_uri = ParameterString(\n",
    "    name=\"ProcessedTestDataURI\", \n",
    "    default_value=f\"{data_path}/preprocessed-data/test\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af1543",
   "metadata": {},
   "source": [
    "#### Define the feature engineering step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c071341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:20.638759Z",
     "iopub.status.busy": "2025-08-25T07:47:20.638579Z",
     "iopub.status.idle": "2025-08-25T07:47:20.663180Z",
     "shell.execute_reply": "2025-08-25T07:47:20.662801Z",
     "shell.execute_reply.started": "2025-08-25T07:47:20.638746Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "# Define the SKLearnProcessor\n",
    "featureEngr = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",  \n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"feature-engr\",\n",
    "    role=sagemaker_role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Define the feature engineering step\n",
    "featureEng_step = ProcessingStep(\n",
    "    name=f\"featureEngrStep-{int(time.time())}\",\n",
    "    processor=featureEngr,\n",
    "    code=\"health_featureEngr.py\", # Script was created in previous step\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name=\"input-path\",\n",
    "            source=input_data_uri,\n",
    "            destination=\"/opt/ml/processing/input\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"output-path\",\n",
    "            source=\"/opt/ml/processing/output\", # local\n",
    "            destination=engr_data_uri # S3\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--input-path\", \"/opt/ml/processing/input\",\n",
    "        \"--output-path\", \"/opt/ml/processing/output\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9ebb7-a06e-4908-ad8d-82e8a7e27974",
   "metadata": {},
   "source": [
    "#### Define the data cleaning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d032ef26-3929-4d90-8e93-f7d2e3ea2253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:20.833749Z",
     "iopub.status.busy": "2025-08-25T07:47:20.833546Z",
     "iopub.status.idle": "2025-08-25T07:47:21.233335Z",
     "shell.execute_reply": "2025-08-25T07:47:21.232865Z",
     "shell.execute_reply.started": "2025-08-25T07:47:20.833735Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "# Define the SKLearnProcessor\n",
    "datacleaning = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",  \n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"cleaning-data\",\n",
    "    role=sagemaker_role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Define the data cleaning step\n",
    "datacleaning_step = ProcessingStep(\n",
    "    name=f\"CleaningStep-{int(time.time())}\",\n",
    "    processor=datacleaning,\n",
    "    code=\"health_datacleaning.py\", # Script was created in previous step\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name=\"input-path\",\n",
    "            source=featureEng_step.properties.ProcessingOutputConfig.Outputs[\"output-path\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/input\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"output-train-path\",\n",
    "            source=\"/opt/ml/processing/output/train\", # local\n",
    "            destination=train_data_uri # S3\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"output-test-path\",\n",
    "            source=\"/opt/ml/processing/output/test\", # local\n",
    "            destination=test_data_uri # S3\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--input-path\", \"/opt/ml/processing/input\",\n",
    "        \"--output-train-path\", \"/opt/ml/processing/output/train\",\n",
    "        \"--output-test-path\", \"/opt/ml/processing/output/test\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534bfd6-8d1c-49d4-9f4f-a1143378abbc",
   "metadata": {},
   "source": [
    "#### Define the preprocessing step (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42372c20-1182-4891-9971-22e656476ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:21.440718Z",
     "iopub.status.busy": "2025-08-25T07:47:21.440441Z",
     "iopub.status.idle": "2025-08-25T07:47:21.460847Z",
     "shell.execute_reply": "2025-08-25T07:47:21.460439Z",
     "shell.execute_reply.started": "2025-08-25T07:47:21.440700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the SKLearnProcessor\n",
    "preprocessing = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",  \n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"preprocessing\",\n",
    "    role=sagemaker_role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Define the processing step\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=f\"PreprocessingStep-{int(time.time())}\",\n",
    "    processor=preprocessing,\n",
    "    code=\"health_preprocessor.py\", # Script was created in previous step\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name=\"input-train-path\",\n",
    "            source=datacleaning_step.properties.ProcessingOutputConfig.Outputs[\"output-train-path\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/input/train\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            input_name=\"input-test-path\",\n",
    "            source=datacleaning_step.properties.ProcessingOutputConfig.Outputs[\"output-test-path\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/input/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"output-train-path\",\n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            destination=processed_train_data_uri\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"output-test-path\",\n",
    "            source=\"/opt/ml/processing/output/test\",\n",
    "            destination=processed_test_data_uri\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--input-train-path\", \"/opt/ml/processing/input/train\",\n",
    "        \"--input-test-path\", \"/opt/ml/processing/input/test\",\n",
    "        \"--output-train-path\", \"/opt/ml/processing/output/train\",\n",
    "        \"--output-test-path\", \"/opt/ml/processing/output/test\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f0359-a0da-4a65-b6e8-0d8d79961990",
   "metadata": {},
   "source": [
    "#### Define the training step (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f53ff74-5f4e-43f7-8773-b04cf9f6ac90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:22.060404Z",
     "iopub.status.busy": "2025-08-25T07:47:22.059855Z",
     "iopub.status.idle": "2025-08-25T07:47:22.103511Z",
     "shell.execute_reply": "2025-08-25T07:47:22.103079Z",
     "shell.execute_reply.started": "2025-08-25T07:47:22.060384Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.workflow.steps import TrainingStep, TrainingInput\n",
    "\n",
    "# Training Step\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"health_train.py\", \n",
    "    # source_dir=\".\",  # 👈 include the current folder\n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=training_instance_type,\n",
    "    role=sagemaker_role,\n",
    "    hyperparameters={\n",
    "        \"tracking-server-arn\": mlflow_tracking_server_arn,\n",
    "        \"experiment-name\": experiment_name_param,\n",
    "        \"model-output-path\": \"/opt/ml/model\",\n",
    "        \"n_estimators\": n_estimators_param,\n",
    "        \"max_depth\": max_depth_param,\n",
    "        \"min_samples_leaf\": min_samples_leaf_param,\n",
    "        \"min_samples_split\": min_samples_split_param,\n",
    "        \"max_features\": max_features_param,\n",
    "        \"bootstrap\": bootstrap_param\n",
    "    },\n",
    "    py_version=\"py3\",\n",
    "    # requirements=\"requirements.txt\" \n",
    ")\n",
    "\n",
    "train_step = TrainingStep(\n",
    "    name=f\"TrainModel-{int(time.time())}\",\n",
    "    estimator=sklearn_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"output-train-path\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "            input_mode=\"File\",  # ensure it's mounted as a file\n",
    "        )\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b888c7-a9a2-47ab-9565-0879e048639b",
   "metadata": {},
   "source": [
    "#### Define the Evalution step (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58178a7e-71d3-4c11-9e86-6c228a3ec87c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:22.751753Z",
     "iopub.status.busy": "2025-08-25T07:47:22.751396Z",
     "iopub.status.idle": "2025-08-25T07:47:22.790035Z",
     "shell.execute_reply": "2025-08-25T07:47:22.789602Z",
     "shell.execute_reply.started": "2025-08-25T07:47:22.751735Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.model_metrics import ModelMetrics, FileSource\n",
    "\n",
    "evaluation_processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"sklearn\", sagemaker_session.boto_region_name, \"1.2-1\"),\n",
    "    command=['python3'],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"evaluate-model\",\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name=f\"EvaluateModel-{int(time.time())}\",\n",
    "    processor=evaluation_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"output-test-path\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\")],\n",
    "    code=\"health_evaluate.py\",  # SageMaker will handle uploading and running this script\n",
    "    job_arguments=[  # Pass arguments here instead of in command\n",
    "        \"--model-path\", \"/opt/ml/processing/model\",\n",
    "        \"--input-test-path\", \"/opt/ml/processing/test\",\n",
    "        \"--output-path\", \"/opt/ml/processing/evaluation\",\n",
    "        \"--model-package-group-name\", model_package_group_name,\n",
    "        \"--region\", \"ap-southeast-1\",\n",
    "    ],\n",
    "    property_files=[evaluation_report],\n",
    "    depends_on=[train_step]\n",
    ")\n",
    "\n",
    "model_metrics_report = ModelMetrics(\n",
    "    model_statistics=FileSource(\n",
    "        s3_uri=evaluation_step.properties.ProcessingOutputConfig.Outputs[\"evaluation\"].S3Output.S3Uri,\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78335c47-b9ac-405a-a219-bc827779bb07",
   "metadata": {},
   "source": [
    "#### Define the Model registration step (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a3e9786-f42b-4de2-b463-c825365feae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:23.426127Z",
     "iopub.status.busy": "2025-08-25T07:47:23.425750Z",
     "iopub.status.idle": "2025-08-25T07:47:23.438328Z",
     "shell.execute_reply": "2025-08-25T07:47:23.437870Z",
     "shell.execute_reply.started": "2025-08-25T07:47:23.426109Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.conditions import ConditionNot\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionEquals\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "# RegisterModel step (always defined, but executed conditionally)\n",
    "step_register_new = RegisterModel(\n",
    "    name=\"RegisterNewModel\",\n",
    "    estimator=sklearn_estimator,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    model_metrics=model_metrics_report,\n",
    "    approval_status=\"Approved\" # changed from \"PendingManualApproval\" since passed threshold\n",
    ")\n",
    "\n",
    "step_register_better_model = RegisterModel(\n",
    "    name=\"RegisterBetterModel\",\n",
    "    estimator=sklearn_estimator,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    model_metrics=model_metrics_report,\n",
    "    approval_status=\"Approved\" # changed from \"PendingManualApproval\" since passed threshold\n",
    ")\n",
    "\n",
    "\n",
    "# Conditions: check accuracy & f1 > threshold OR no model exists\n",
    "cond_accuracy = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"accuracy\"\n",
    "    ),\n",
    "    right=accuracy_threshold_param\n",
    ")\n",
    "\n",
    "cond_f1 = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"f1\"\n",
    "    ),\n",
    "    right=f1_threshold_param\n",
    ")\n",
    "\n",
    "cond_no_registered = ConditionEquals(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"baseline_exists\" # Check the key added to the report\n",
    "    ),\n",
    "    right=False # Condition is TRUE if baseline_exists is False\n",
    ")\n",
    "\n",
    "# Outer step: Checks for existence of registered model first\n",
    "step_cond_metrics = ConditionStep(\n",
    "    name=\"CheckAccuracyAndF1\",\n",
    "    conditions=[\n",
    "        cond_accuracy,\n",
    "        cond_f1,\n",
    "    ],\n",
    "    if_steps=[step_register_better_model], # Register model if both metrics pass\n",
    "    else_steps=[],\n",
    ")\n",
    "\n",
    "step_cond_no_registered = ConditionStep(\n",
    "    name=\"CheckIfNoModelExists\",\n",
    "    conditions=[cond_no_registered],\n",
    "    if_steps=[step_register_new], # Register model if no baseline exists\n",
    "    else_steps=[step_cond_metrics], # If model exists, check accuracy and f1\n",
    "    depends_on=[evaluation_step]\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a563330f-82dd-4496-ba31-ecee78495707",
   "metadata": {},
   "source": [
    "#### Define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b5f0579-bda6-493d-8e83-6bab8e162a6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:24.211486Z",
     "iopub.status.busy": "2025-08-25T07:47:24.211184Z",
     "iopub.status.idle": "2025-08-25T07:47:26.512037Z",
     "shell.execute_reply": "2025-08-25T07:47:26.511589Z",
     "shell.execute_reply.started": "2025-08-25T07:47:24.211468Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 'activityLabelPipeline' is defined and ready to be executed.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Define the Pipeline\n",
    "# This assembles our parameters and steps into a single workflow.\n",
    "pipeline_name = \"activityLabelPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[input_data_uri, \n",
    "                engr_data_uri,\n",
    "                train_data_uri,\n",
    "                test_data_uri,\n",
    "                processed_train_data_uri,\n",
    "                processed_test_data_uri,\n",
    "                experiment_name_param,\n",
    "                accuracy_threshold_param,\n",
    "                f1_threshold_param,\n",
    "                # RandomForest hyperparameters\n",
    "                n_estimators_param,\n",
    "                max_depth_param,\n",
    "                min_samples_leaf_param,\n",
    "                min_samples_split_param,\n",
    "                max_features_param,\n",
    "                bootstrap_param],\n",
    "    steps=[featureEng_step,\n",
    "           datacleaning_step,\n",
    "           preprocessing_step,\n",
    "           train_step,\n",
    "           evaluation_step,\n",
    "           step_cond_no_registered] # Use the 'no registered model' check as the primary condition step\n",
    ")\n",
    "\n",
    "# Create or update the pipeline definition in your AWS account.\n",
    "pipeline.upsert(role_arn=sagemaker_role)\n",
    "\n",
    "print(f\"Pipeline '{pipeline_name}' is defined and ready to be executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ade4bb-f9d5-4042-b7fa-aace46705972",
   "metadata": {},
   "source": [
    "## Start Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f3237ec-2264-43db-977e-593be5de8971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:27.639519Z",
     "iopub.status.busy": "2025-08-25T07:47:27.639171Z",
     "iopub.status.idle": "2025-08-25T07:47:27.914810Z",
     "shell.execute_reply": "2025-08-25T07:47:27.914332Z",
     "shell.execute_reply.started": "2025-08-25T07:47:27.639498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting pipeline execution...\n",
      "--> Execution started! ARN: arn:aws:sagemaker:ap-southeast-1:837028399719:pipeline/activityLabelPipeline/execution/sg0lgt18m52l\n",
      "{'PipelineArn': 'arn:aws:sagemaker:ap-southeast-1:837028399719:pipeline/activityLabelPipeline', 'PipelineExecutionArn': 'arn:aws:sagemaker:ap-southeast-1:837028399719:pipeline/activityLabelPipeline/execution/sg0lgt18m52l', 'PipelineExecutionDisplayName': 'dataEngr-to-modelReg', 'PipelineExecutionStatus': 'Executing', 'CreationTime': datetime.datetime(2025, 8, 25, 7, 47, 27, 763000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2025, 8, 25, 7, 47, 27, 763000, tzinfo=tzlocal()), 'CreatedBy': {'IamIdentity': {'Arn': 'arn:aws:sts::837028399719:assumed-role/iti113-team12-sagemaker-iti113-team12-domain-iti113-team12-Role/SageMaker', 'PrincipalId': 'AROA4FYWHZJT3SRRDCPYT:SageMaker'}}, 'LastModifiedBy': {'IamIdentity': {'Arn': 'arn:aws:sts::837028399719:assumed-role/iti113-team12-sagemaker-iti113-team12-domain-iti113-team12-Role/SageMaker', 'PrincipalId': 'AROA4FYWHZJT3SRRDCPYT:SageMaker'}}, 'ResponseMetadata': {'RequestId': '1519a30c-671f-4bb7-a7ff-5988927e948e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1519a30c-671f-4bb7-a7ff-5988927e948e', 'content-type': 'application/x-amz-json-1.1', 'content-length': '805', 'date': 'Mon, 25 Aug 2025 07:47:27 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# -------------------------\n",
    "# Running existing pipeline\n",
    "# -------------------------\n",
    "pipeline = Pipeline(name=\"activityLabelPipeline\")\n",
    "# bucket_name = 'iti113-team12-bucket'  # e.g., 'my-company-sagemaker-bucket'\n",
    "# base_folder = 'project'\n",
    "# data_path = f\"s3://{bucket_name}/{base_folder}/datasets/c17/working\"\n",
    "\n",
    "# Start the pipeline and pass the S3 path for v1.0 to our parameter\n",
    "print(\"\\nStarting pipeline execution...\")\n",
    "\n",
    "# Experiment with data\n",
    "execution = pipeline.start(\n",
    "    parameters={\n",
    "        \"InputDataURI\": f\"{data_path}/combined_c17.csv\",\n",
    "        \"EngrDataURI\": f\"{data_path}/combined_c17_engineered.csv\",\n",
    "        \"TrainDataURI\": f\"{data_path}/preprocessed-data/train\",\n",
    "        \"TestDataURI\": f\"{data_path}/preprocessed-data/test\",\n",
    "        \"ProcessedTrainDataURI\": f\"{data_path}/preprocessed-data/train\",\n",
    "        \"ProcessedTestDataURI\": f\"{data_path}/preprocessed-data/test\",\n",
    "\n",
    "        # RandomForest hyperparameters: pass raw values, not ParameterInteger/Boolean\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 10,\n",
    "        \"min_samples_leaf\": 5,\n",
    "        \"min_samples_split\": 10,\n",
    "        \"max_features\": \"sqrt\",\n",
    "        \"bootstrap\": True,\n",
    "\n",
    "        # MLflow / experiment\n",
    "        # \"experiment-name\": \"c-activityLabelClf\",\n",
    "    },\n",
    "    execution_display_name=\"dataEngr-to-modelReg\"\n",
    ")\n",
    "\n",
    "print(f\"--> Execution started! ARN: {execution.arn}\")\n",
    "print(execution.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94519d76-fa52-476f-8a5a-19000e5a2080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:47:32.609892Z",
     "iopub.status.busy": "2025-08-25T07:47:32.609605Z",
     "iopub.status.idle": "2025-08-25T07:47:32.724940Z",
     "shell.execute_reply": "2025-08-25T07:47:32.724489Z",
     "shell.execute_reply.started": "2025-08-25T07:47:32.609876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Status: Executing\n",
      "Creation Time: 2025-08-25 07:47:27.763000+00:00\n",
      "Last Modified: 2025-08-25 07:47:27.763000+00:00\n"
     ]
    }
   ],
   "source": [
    "sagemaker_client = sagemaker.Session().sagemaker_client\n",
    "\n",
    "# Get the details of the specific pipeline execution\n",
    "response = sagemaker_client.describe_pipeline_execution(\n",
    "    PipelineExecutionArn=execution.arn\n",
    ")\n",
    "\n",
    "# Print the current status\n",
    "status = response['PipelineExecutionStatus']\n",
    "print(f\"Pipeline Status: {status}\")\n",
    "\n",
    "# You can also see the creation time and last modified time\n",
    "print(f\"Creation Time: {response['CreationTime']}\")\n",
    "if 'LastModifiedTime' in response:\n",
    "    print(f\"Last Modified: {response['LastModifiedTime']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec469f-36cd-418f-86e0-b5bf017ea16e",
   "metadata": {},
   "source": [
    "## Manual Deployment (unused)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d238e0-8a2a-4ca4-8e40-dd6b3c4acadd",
   "metadata": {},
   "source": [
    "#### 1. Identify best training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0f032ee-0a5f-4f54-b0a9-15ae5117853b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T08:12:20.170280Z",
     "iopub.status.busy": "2025-08-25T08:12:20.169977Z",
     "iopub.status.idle": "2025-08-25T08:12:21.310724Z",
     "shell.execute_reply": "2025-08-25T08:12:21.310155Z",
     "shell.execute_reply.started": "2025-08-25T08:12:20.170261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: <Experiment: artifact_location='s3://iti113-team12-bucket/39', creation_time=1755998829627, experiment_id='39', last_update_time=1755998829627, lifecycle_stage='active', name='c-activityLabelClf', tags={}>\n",
      "Runs type: <class 'pandas.core.frame.DataFrame'>\n",
      "Best run: run_id                                            4eed5971a77249f297a020b2ef14bbf2\n",
      "experiment_id                                                                   39\n",
      "status                                                                    FINISHED\n",
      "artifact_uri                     s3://iti113-team12-bucket/39/4eed5971a77249f29...\n",
      "start_time                                        2025-08-25 08:03:06.679000+00:00\n",
      "end_time                                          2025-08-25 08:03:44.223000+00:00\n",
      "metrics.roc_auc                                                           0.998979\n",
      "metrics.f1                                                                0.959404\n",
      "metrics.precision                                                         0.972538\n",
      "metrics.accuracy                                                          0.962965\n",
      "metrics.recall                                                            0.949113\n",
      "params.min_samples_leaf                                                          5\n",
      "params.bootstrap                                                              True\n",
      "params.max_features                                                           sqrt\n",
      "params.n_estimators                                                            100\n",
      "params.max_depth                                                                10\n",
      "params.min_samples_split                                                        10\n",
      "tags.mlflow.source.type                                                      LOCAL\n",
      "tags.mlflow.source.name                                            health_train.py\n",
      "tags.mlflow.user                                                              root\n",
      "tags.mlflow.runName                                                 salty-dove-527\n",
      "tags.mlflow.log-model.history                                                 None\n",
      "Name: 0, dtype: object\n",
      "Registering model from URI: runs:/4eed5971a77249f297a020b2ef14bbf2/model\n",
      "Model 'activity-clf-model'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'activity-clf-model' already exists. Creating a new version of this model...\n",
      "2025/08/25 08:12:21 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: activity-clf-model, version 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'activity-clf-model' registered with version: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '3' of model 'activity-clf-model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_tracking_server_arn)\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(\"c-activityLabelClf\")\n",
    "runs = mlflow.search_runs(experiment.experiment_id)\n",
    "\n",
    "print(\"Experiment:\", experiment)\n",
    "\n",
    "# Find the run with the best metric (e.g., highest f1)\n",
    "runs = mlflow.search_runs(experiment.experiment_id)\n",
    "print(\"Runs type:\", type(runs))\n",
    "# print(\"Runs columns:\", runs.columns)\n",
    "# print(\"First few rows:\\n\", runs.head())\n",
    "\n",
    "# Check if 'metrics.f' exists:\n",
    "if 'metrics.f1' not in runs.columns:\n",
    "    raise ValueError(\"No 'metrics.f1' column found. Available metrics:\", runs.columns)\n",
    "\n",
    "best_run = runs.loc[runs['metrics.f1'].idxmax()]\n",
    "print(\"Best run:\", best_run)\n",
    "\n",
    "# Construct the model URI from the run\n",
    "best_run_id = best_run.run_id\n",
    "\n",
    "model_uri = f\"runs:/{best_run_id}/model\"\n",
    "print(f\"Registering model from URI: {model_uri}\")\n",
    "print(f\"Model '{model_name}'\")\n",
    "# Register the model to the MLflow Model Registry\n",
    "model_name = \"activity-clf-model\"\n",
    "registered_model = mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=model_name\n",
    ")\n",
    "\n",
    "print(f\"Model '{model_name}' registered with version: {registered_model.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e26db-6f45-425d-9ff5-c98415381589",
   "metadata": {},
   "source": [
    "#### 2. Retrieve model artifact from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6161397-309f-4588-9fa3-d3bc0b055eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T08:12:24.697646Z",
     "iopub.status.busy": "2025-08-25T08:12:24.697342Z",
     "iopub.status.idle": "2025-08-25T08:12:24.779087Z",
     "shell.execute_reply": "2025-08-25T08:12:24.778612Z",
     "shell.execute_reply.started": "2025-08-25T08:12:24.697628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model S3 Artifact URI: s3://iti113-team12-bucket/39/4eed5971a77249f297a020b2ef14bbf2/artifacts/model\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "model_version = client.get_model_version(model_name, registered_model.version)\n",
    "\n",
    "model_artifact_s3 = model_version.source\n",
    "print(\"Model S3 Artifact URI:\", model_artifact_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c048c-43bc-49f3-8058-d1678ca40c3e",
   "metadata": {},
   "source": [
    "#### 3. Download, repackage model, upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "389e80bf-7efb-4440-b46a-78dbbf4bfa84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T08:17:18.695908Z",
     "iopub.status.busy": "2025-08-25T08:17:18.695608Z",
     "iopub.status.idle": "2025-08-25T08:17:21.087834Z",
     "shell.execute_reply": "2025-08-25T08:17:21.087388Z",
     "shell.execute_reply.started": "2025-08-25T08:17:18.695890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83427a34cc7d4105a7d73111de837bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Compressed model uploaded to: s3://sagemaker-ap-southeast-1-837028399719/project/models/activity-clf-model-v3/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import boto3\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_folder = \"/tmp/model\" # temp local dir\n",
    "code_folder = os.path.join(model_folder, \"code\") \n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "model_artifact_s3 = \"s3://iti113-team12-bucket/39/models/m-36e80339c62749bdabd019fcfbcdab11/artifacts\"\n",
    "# model_artifact_s3 = \"s3://iti113-team12-bucket/39/4eed5971a77249f297a020b2ef14bbf2/artifacts\"\n",
    "\n",
    "# Download mMLflow artifact to local\n",
    "mlflow.artifacts.download_artifacts(\n",
    "    artifact_uri=model_artifact_s3,\n",
    "    dst_path=model_folder\n",
    ")\n",
    "\n",
    "# Find the actual model file inside model_folder (model.pkl)\n",
    "model_file_path = None\n",
    "for root, dirs, files in os.walk(model_folder):\n",
    "    for f in files:\n",
    "        if f.endswith(\".pkl\"): \n",
    "            model_file_path = os.path.join(root, f)\n",
    "            break\n",
    "\n",
    "if not model_file_path:\n",
    "    raise FileNotFoundError(\"No model.pkl found in MLflow artifacts\")\n",
    "\n",
    "# Copy it to a clean directory so it's at root level\n",
    "root_dir = \"/tmp/model_root\"\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "shutil.copy(model_file_path, os.path.join(root_dir, \"model.pkl\"))\n",
    "\n",
    "# Compress model to model.tar.gz with model.pkl at root\n",
    "model_tar_path = \"/tmp/model.tar.gz\"\n",
    "with tarfile.open(model_tar_path, \"w:gz\") as tar:\n",
    "    tar.add(root_dir, arcname=\".\")\n",
    "\n",
    "# Upload model.tar.gz to S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "# parsed = boto3.session.Session().resource(\"s3\").Bucket(sagemaker_session.default_bucket())\n",
    "s3_key = f\"{base_folder}/models/{model_name}-v{registered_model.version}/model.tar.gz\"\n",
    "s3.upload_file(model_tar_path, bucket_name, s3_key)\n",
    "\n",
    "model_s3_uri = f\"s3://{bucket_name}/{s3_key}\"\n",
    "print(\"✅ Compressed model uploaded to:\", model_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c4a88-b41f-4f8b-b78a-ffc2e456de3b",
   "metadata": {},
   "source": [
    "#### 4. Write inference.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d9d16592-d900-4e1e-96a5-11cfb5139403",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T08:17:21.088762Z",
     "iopub.status.busy": "2025-08-25T08:17:21.088534Z",
     "iopub.status.idle": "2025-08-25T08:17:21.091939Z",
     "shell.execute_reply": "2025-08-25T08:17:21.091523Z",
     "shell.execute_reply.started": "2025-08-25T08:17:21.088746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "# inference.py\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    return joblib.load(os.path.join(model_dir, \"model.pkl\"))\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    if content_type == \"application/json\":\n",
    "        return pd.DataFrame.from_dict(eval(request_body))  # simple eval for test input\n",
    "    raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    return model.predict(input_data)\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    return str(prediction.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453fb121-867f-4104-b838-a397d23385c8",
   "metadata": {},
   "source": [
    "#### 5. Check model file structure\n",
    "Make sure model.tar.gz contains model.pkl at the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eabc0946-3f96-4675-bfc6-e88cfc7d391b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T08:17:22.015538Z",
     "iopub.status.busy": "2025-08-25T08:17:22.015236Z",
     "iopub.status.idle": "2025-08-25T08:17:22.092953Z",
     "shell.execute_reply": "2025-08-25T08:17:22.092482Z",
     "shell.execute_reply.started": "2025-08-25T08:17:22.015520Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3_uri = model_s3_uri #\"s3://your-bucket/path/to/model.tar.gz\"\n",
    "\n",
    "bucket = s3_uri.replace(\"s3://\", \"\").split(\"/\")[0]\n",
    "key = \"/\".join(s3_uri.replace(\"s3://\", \"\").split(\"/\")[1:])\n",
    "\n",
    "local_path = \"model.tar.gz\"\n",
    "s3.download_file(bucket, key, local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf24e9c3-d5d0-4abe-bb5c-b59a64d7ff80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T08:17:23.173588Z",
     "iopub.status.busy": "2025-08-25T08:17:23.173291Z",
     "iopub.status.idle": "2025-08-25T08:17:23.190850Z",
     "shell.execute_reply": "2025-08-25T08:17:23.190415Z",
     "shell.execute_reply.started": "2025-08-25T08:17:23.173572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?rwxr-xr-x sagemaker-user/users          0 2025-08-25 07:35:44 ./ \n",
      "?rw-r--r-- sagemaker-user/users    4893768 2025-08-25 08:17:18 ./model.pkl \n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# Make sure model.tar.gz contains model.pkl at the root\n",
    "# OR contains MLflow's default format with `MLmodel` + `model.pkl`\n",
    "\n",
    "with tarfile.open(local_path, \"r:gz\") as tar:\n",
    "    tar.list()   # shows file structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd7f49-2327-4228-8858-70f11829e815",
   "metadata": {},
   "source": [
    "#### 6. Create SageMaker Model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bae8c6a9-39b7-4889-a28d-18628e2edb77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T08:17:25.484897Z",
     "iopub.status.busy": "2025-08-25T08:17:25.484604Z",
     "iopub.status.idle": "2025-08-25T08:17:25.487702Z",
     "shell.execute_reply": "2025-08-25T08:17:25.487280Z",
     "shell.execute_reply.started": "2025-08-25T08:17:25.484878Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "# Create the model object\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_s3_uri,\n",
    "    role=sagemaker_role,\n",
    "    entry_point=\"inference.py\", \n",
    "    framework_version=\"1.2-1\",\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562fa24-5176-4c18-b4aa-fe8d9c4333b4",
   "metadata": {},
   "source": [
    "#### 7. Check and deploy endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "630216b4-91ab-4249-87de-372b549582a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T08:17:26.518695Z",
     "iopub.status.busy": "2025-08-25T08:17:26.518410Z",
     "iopub.status.idle": "2025-08-25T08:20:58.682754Z",
     "shell.execute_reply": "2025-08-25T08:20:58.682264Z",
     "shell.execute_reply.started": "2025-08-25T08:17:26.518678Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-scikit-learn-2025-08-25-08-17-26-762\n",
      "INFO:sagemaker:Creating endpoint-config with name activity-labels-clf\n",
      "INFO:sagemaker:Creating endpoint with name activity-labels-clf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!Model deployed at endpoint: activity-labels-clf\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Define the endpoint name\n",
    "endpoint_name = \"activity-labels-clf\" \n",
    "# deleted previous endpoint & endpoint config instead of using {registered_model.version}\"\n",
    "\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "def endpoint_exists(endpoint_name):\n",
    "    try:\n",
    "        sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if \"Could not find endpoint\" in str(e):\n",
    "            return False\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "if endpoint_exists(endpoint_name):\n",
    "    predictor = sklearn_model.deploy(\n",
    "        instance_type=\"ml.t2.medium\",\n",
    "        initial_instance_count=1,\n",
    "        endpoint_name=endpoint_name,\n",
    "        update_endpoint=True # Update endpoint if it already exists\n",
    "    )\n",
    "else:\n",
    "    # Create endpoint if it doesn't exist\n",
    "    predictor = sklearn_model.deploy(\n",
    "        instance_type=\"ml.t2.medium\",\n",
    "        initial_instance_count=1,\n",
    "        endpoint_name=endpoint_name\n",
    "    )\n",
    "\n",
    "print(f\"Model deployed at endpoint: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ea9aa-f390-4528-9520-1cb7a0ffc3ec",
   "metadata": {},
   "source": [
    "## Inference and Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db9128-861b-4d2a-af7f-88dd18866ca3",
   "metadata": {},
   "source": [
    "#### Test endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1c4506ff-8130-42ad-a961-ca6dbc64554f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T08:32:17.833845Z",
     "iopub.status.busy": "2025-08-25T08:32:17.833496Z",
     "iopub.status.idle": "2025-08-25T08:32:18.240684Z",
     "shell.execute_reply": "2025-08-25T08:32:18.240087Z",
     "shell.execute_reply.started": "2025-08-25T08:32:17.833824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   yaw_mean  pitch_mean  roll_mean  rotation_rate_x_mean  \\\n",
      "0  2.630174   -0.847349  -1.804189             -0.000237   \n",
      "1  0.848279   -0.812936  -0.001545             -0.082376   \n",
      "2  0.468502   -0.492932   1.572375              0.014805   \n",
      "3 -1.008782   -0.048545  -0.803060              0.001440   \n",
      "4 -1.240486    0.098275  -1.382349              0.051009   \n",
      "\n",
      "   rotation_rate_y_mean  rotation_rate_z_mean  user_acceleration_x_mean  \\\n",
      "0              0.001718              0.000271                  0.000880   \n",
      "1             -0.124521              0.009532                  0.029942   \n",
      "2              0.093693             -0.011113                  0.025707   \n",
      "3             -0.009675             -0.018328                  0.004322   \n",
      "4             -0.015989             -0.012779                  0.003385   \n",
      "\n",
      "   user_acceleration_z_mean  pitch_std activity_label  timeframe_seconds  \\\n",
      "0                  0.001749   0.027841          relax          59.963069   \n",
      "1                  0.021527   0.333347         chores          59.896833   \n",
      "2                  0.018373   0.503268            eat          59.908175   \n",
      "3                  0.002956   0.193942         chores          59.899401   \n",
      "4                  0.002508   1.040489       exercise          59.932537   \n",
      "\n",
      "   activity_instance  duration_minutes  duration_zscore_user_all_acts  \\\n",
      "0                  1          5.982985                       0.268010   \n",
      "1                  4          5.966448                       0.382552   \n",
      "2                  1          5.984084                       0.486258   \n",
      "3                  3          5.983299                       0.393280   \n",
      "4                  1          5.966582                      -1.441764   \n",
      "\n",
      "   duration_zscore_user_each_act  total_orientation_variability  \\\n",
      "0                      -0.707107                       0.189589   \n",
      "1                      -1.783258                       3.966036   \n",
      "2                       0.707107                       3.576755   \n",
      "3                       0.421137                       0.966214   \n",
      "4                      -0.707107                       2.629039   \n",
      "\n",
      "   total_orientation  speed_n_orientation_stability  duration_n_daily_freq  \\\n",
      "0          -0.021364                       0.103186               5.982985   \n",
      "1           0.033798                       0.187639               5.966448   \n",
      "2           1.547946                       0.000000               5.984084   \n",
      "3          -1.860387                       0.033240               5.983299   \n",
      "4          -2.524559                       0.253272               5.966582   \n",
      "\n",
      "   duration_n_weekly_freq  \n",
      "0               11.965969  \n",
      "1               11.932897  \n",
      "2               11.968168  \n",
      "3               11.966599  \n",
      "4               11.933165  \n",
      "Sending payload:\n",
      " {'yaw_mean': [2.630173816264381, 0.8482788287867755, 0.4685022615422755, -1.0087823471289057, -1.240485579121014], 'pitch_mean': [-0.8473489671409515, -0.8129361024755235, -0.4929317327440565, -0.0485451999080691, 0.0982754701540114], 'roll_mean': [-1.8041892750311657, -0.0015449190648952, 1.5723752988177342, -0.8030595457588551, -1.3823492632292111], 'rotation_rate_x_mean': [-0.0002368325948677, -0.0823762153636546, 0.0148052525937083, 0.0014400658196927, 0.0510086949596491], 'rotation_rate_y_mean': [0.0017177998690715, -0.1245212599235212, 0.093693114346809, -0.009675181606178, -0.0159890662801141], 'rotation_rate_z_mean': [0.0002709498226847, 0.0095324191354536, -0.0111127183879694, -0.0183279430068274, -0.0127792745498785], 'user_acceleration_x_mean': [0.0008802409973398, 0.0299418517697812, 0.0257066509726902, 0.0043219053604178, 0.0033848104191322], 'user_acceleration_z_mean': [0.0017489711203908, 0.0215268418737424, 0.0183729146495087, 0.0029556053223403, 0.0025077055955383], 'pitch_std': [0.0278410566940439, 0.3333472882868497, 0.5032681334101218, 0.1939420872583131, 1.0404889003012694], 'activity_label': ['relax', 'chores', 'eat', 'chores', 'exercise'], 'timeframe_seconds': [59.963069, 59.896833, 59.90817500000001, 59.89940100000001, 59.932537], 'activity_instance': [1, 4, 1, 3, 1], 'duration_minutes': [5.98298475, 5.9664483, 5.984084133333334, 5.9832993000000005, 5.966582366666668], 'duration_zscore_user_all_acts': [0.268010232360926, 0.382551892627068, 0.486258435084028, 0.3932795653660936, -1.4417635137663865], 'duration_zscore_user_each_act': [-0.7071067811903654, -1.7832582294238262, 0.7071067811865475, 0.4211370427787725, -0.7071067811865666], 'total_orientation_variability': [0.1895890899303605, 3.9660360508214754, 3.5767546109401143, 0.9662138865772578, 2.629039067730538], 'total_orientation': [-0.0213644259077356, 0.0337978072463568, 1.5479458276159532, -1.86038709279583, -2.524559372196214], 'speed_n_orientation_stability': [0.1031860991743137, 0.1876393798338044, 0.0, 0.0332404277449451, 0.2532719818888636], 'duration_n_daily_freq': [5.98298475, 5.9664483, 5.984084133333334, 5.9832993000000005, 5.966582366666668], 'duration_n_weekly_freq': [11.9659695, 11.9328966, 11.968168266666668, 11.9665986, 11.933164733333337]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">27</span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Sending payload:\\n\"</span>, payload)                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>27 response = <span style=\"font-weight: bold; text-decoration: underline\">predictor.predict(payload)</span>                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Prediction response:\"</span>, response)                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.12/site-packages/sagemaker/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base_predictor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">212</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict</span>               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">209 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> inference_component_name:                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">210 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>request_args[<span style=\"color: #808000; text-decoration-color: #808000\">\"InferenceComponentName\"</span>] = inference_component_name              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">211 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>212 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**req</span>   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">213 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._handle_response(response)                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">214 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">215 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">_handle_response</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, response):                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.12/site-packages/botocore/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">569</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_api_call</span>                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 566 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>py_operation_name<span style=\"color: #808000; text-decoration-color: #808000\">}() only accepts keyword arguments.\"</span>              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 567 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 568 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The \"self\" in this scope is referring to the BaseClient.</span>                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 569 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">._make_api_call(operation_name, kwargs)</span>                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 570 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 571 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>_api_call.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(py_operation_name)                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 572 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.12/site-packages/botocore/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1023</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_make_api_call</span>                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1020 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Code\"</span>                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1021 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1022 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>error_class = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.exceptions.from_code(error_code)                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1023 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">raise</span><span style=\"font-weight: bold; text-decoration: underline\"> error_class(parsed_response, operation_name)</span>                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> parsed_response                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1026 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ModelError: </span>An error occurred <span style=\"font-weight: bold\">(</span>ModelError<span style=\"font-weight: bold\">)</span> when calling the InvokeEndpoint operation: Received server error <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span><span style=\"font-weight: bold\">)</span> \n",
       "from primary with message <span style=\"color: #008700; text-decoration-color: #008700\">\"</span><span style=\"color: #008700; text-decoration-color: #008700; font-weight: bold\">&lt;</span><span style=\"color: #008700; text-decoration-color: #008700\">!DOCTYPE HTML PUBLIC \"</span><span style=\"color: #000000; text-decoration-color: #000000\">-</span><span style=\"color: #e100e1; text-decoration-color: #e100e1\">//W3C//DTD</span><span style=\"color: #000000; text-decoration-color: #000000\"> HTML </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span><span style=\"color: #000000; text-decoration-color: #000000\"> Final/</span><span style=\"color: #e100e1; text-decoration-color: #e100e1\">/EN</span><span style=\"color: #000000; text-decoration-color: #000000\">\"&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;title&gt;</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span><span style=\"color: #000000; text-decoration-color: #000000\"> Internal Server Error&lt;</span><span style=\"color: #e100e1; text-decoration-color: #e100e1\">/title</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;h1&gt;Internal Server Error&lt;</span><span style=\"color: #e100e1; text-decoration-color: #e100e1\">/h1</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;p&gt;The server encountered an internal error and was unable to complete your request. Either the server is </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">overloaded or there is an error in the application.&lt;</span><span style=\"color: #e100e1; text-decoration-color: #e100e1\">/p</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\". See \n",
       "<span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://ap-southeast-1.console.aws.amazon.com/cloudwatch/home?</span><span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">region</span><span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">=</span><span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">ap</span><span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">-southeast-1#logEventViewer:</span><span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">group</span><span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">=/aws/sagem</span>\n",
       "<span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">aker/Endpoints/activity-labels-clf</span> in account <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">837028399719</span> for more information.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m27\u001b[0m                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m24 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mSending payload:\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m, payload)                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m26 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m27 response = \u001b[1;4mpredictor.predict(payload)\u001b[0m                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m28 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mPrediction response:\u001b[0m\u001b[33m\"\u001b[0m, response)                                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m29 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m30 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.12/site-packages/sagemaker/\u001b[0m\u001b[1;33mbase_predictor.py\u001b[0m:\u001b[94m212\u001b[0m in \u001b[92mpredict\u001b[0m               \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m209 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m inference_component_name:                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m210 \u001b[0m\u001b[2m│   │   │   \u001b[0mrequest_args[\u001b[33m\"\u001b[0m\u001b[33mInferenceComponentName\u001b[0m\u001b[33m\"\u001b[0m] = inference_component_name              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m211 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m212 \u001b[2m│   │   \u001b[0mresponse = \u001b[1;4;96mself\u001b[0m\u001b[1;4m.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**req\u001b[0m   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m213 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._handle_response(response)                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m214 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m215 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_handle_response\u001b[0m(\u001b[96mself\u001b[0m, response):                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1;33mclient.py\u001b[0m:\u001b[94m569\u001b[0m in \u001b[92m_api_call\u001b[0m                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 566 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mpy_operation_name\u001b[33m}\u001b[0m\u001b[33m() only accepts keyword arguments.\u001b[0m\u001b[33m\"\u001b[0m              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 567 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 568 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m 569 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m._make_api_call(operation_name, kwargs)\u001b[0m                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 570 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 571 \u001b[0m\u001b[2m│   │   \u001b[0m_api_call.\u001b[91m__name__\u001b[0m = \u001b[96mstr\u001b[0m(py_operation_name)                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 572 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1;33mclient.py\u001b[0m:\u001b[94m1023\u001b[0m in \u001b[92m_make_api_call\u001b[0m                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1020 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCode\u001b[0m\u001b[33m\"\u001b[0m                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1021 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1022 \u001b[0m\u001b[2m│   │   │   \u001b[0merror_class = \u001b[96mself\u001b[0m.exceptions.from_code(error_code)                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1023 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m error_class(parsed_response, operation_name)\u001b[0m                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1024 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1025 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m parsed_response                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1026 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mModelError: \u001b[0mAn error occurred \u001b[1m(\u001b[0mModelError\u001b[1m)\u001b[0m when calling the InvokeEndpoint operation: Received server error \u001b[1m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1m)\u001b[0m \n",
       "from primary with message \u001b[38;2;0;135;0m\"\u001b[0m\u001b[1;38;2;0;135;0m<\u001b[0m\u001b[38;2;0;135;0m!DOCTYPE HTML PUBLIC \"\u001b[0m\u001b[39m-\u001b[0m\u001b[38;2;225;0;225m/\u001b[0m\u001b[38;2;225;0;225m/W3C/\u001b[0m\u001b[38;2;225;0;225m/\u001b[0m\u001b[38;2;225;0;225mDTD\u001b[0m\u001b[39m HTML \u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[39m Final/\u001b[0m\u001b[38;2;225;0;225m/\u001b[0m\u001b[38;2;225;0;225mEN\u001b[0m\u001b[39m\">\u001b[0m\n",
       "\u001b[39m<title>\u001b[0m\u001b[1;36m500\u001b[0m\u001b[39m Internal Server Error<\u001b[0m\u001b[38;2;225;0;225m/\u001b[0m\u001b[38;2;225;0;225mtitle\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<h1>Internal Server Error<\u001b[0m\u001b[38;2;225;0;225m/\u001b[0m\u001b[38;2;225;0;225mh1\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<p>The server encountered an internal error and was unable to complete your request. Either the server is \u001b[0m\n",
       "\u001b[39moverloaded or there is an error in the application.<\u001b[0m\u001b[38;2;225;0;225m/\u001b[0m\u001b[38;2;225;0;225mp\u001b[0m\u001b[1m>\u001b[0m\n",
       "\". See \n",
       "\u001b[4;38;2;0;105;255mhttps://ap-southeast-1.console.aws.amazon.com/cloudwatch/home?\u001b[0m\u001b[4;38;2;0;105;255mregion\u001b[0m\u001b[4;38;2;0;105;255m=\u001b[0m\u001b[4;38;2;0;105;255map\u001b[0m\u001b[4;38;2;0;105;255m-southeast-1#logEventViewer:\u001b[0m\u001b[4;38;2;0;105;255mgroup\u001b[0m\u001b[4;38;2;0;105;255m=/aws/sagem\u001b[0m\n",
       "\u001b[4;38;2;0;105;255maker/Endpoints/activity-labels-clf\u001b[0m in account \u001b[1;36m837028399719\u001b[0m for more information.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import StringDeserializer\n",
    "\n",
    "# Create the predictor with JSON handling\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=JSONSerializer(), # convert to JSON format\n",
    "    deserializer=StringDeserializer()\n",
    ")\n",
    "\n",
    "# Get X_test\n",
    "processed_test_uri = f\"{data_path}/preprocessed-data/test/test.csv\"\n",
    "\n",
    "X_test = pd.read_csv(processed_test_uri)\n",
    "\n",
    "test_sample = X_test.iloc[:5]\n",
    "print(test_sample)\n",
    "# Convert to a dict (records format: {col_name: [v1, v2, ...]})\n",
    "test_sample = test_sample#.drop(columns='activity_labels')\n",
    "payload = test_sample.to_dict(orient=\"list\")\n",
    "\n",
    "print(\"Sending payload:\\n\", payload)\n",
    "\n",
    "response = predictor.predict(payload)\n",
    "print(\"Prediction response:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c7f24e-127c-4623-8134-372c97a72f82",
   "metadata": {},
   "source": [
    "#### View endpoint logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c463b635-01a6-46ca-b1be-2a599d424e6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T08:33:04.640421Z",
     "iopub.status.busy": "2025-08-25T08:33:04.640067Z",
     "iopub.status.idle": "2025-08-25T08:33:04.792760Z",
     "shell.execute_reply": "2025-08-25T08:33:04.792288Z",
     "shell.execute_reply.started": "2025-08-25T08:33:04.640400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for logs in: /aws/sagemaker/Endpoints/activity-labels-clf\n",
      "\n",
      "--- Logs from stream: AllTraffic/i-02c187fafcf27be76 ---\n",
      "169.254.178.2 - - [25/Aug/2025:08:32:54 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:32:49 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:32:44 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:32:39 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:32:34 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:32:29 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:32:24 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:32:19 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:32:18 +0000] \"POST /invocations HTTP/1.1\" 500 290 \"-\" \"AHC/2.0\"\n",
      "- activity_label\n",
      "Feature names unseen at fit time:\n",
      "sagemaker_containers._errors.ClientError: The feature names should match those that were passed during fit.\n",
      "Traceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/flask/app.py\", line 2446, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/flask/app.py\", line 1951, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/flask/app.py\", line 1820, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/flask/_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/flask/app.py\", line 1949, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/flask/app.py\", line 1935, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_transformer.py\", line 199, in transform\n",
      "    result = self._transform_fn(\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_transformer.py\", line 231, in _default_transform_fn\n",
      "    prediction = self._predict_fn(data, model)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_functions.py\", line 95, in wrapper\n",
      "    six.reraise(error_class, error_class(e), sys.exc_info()[2])\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/six.py\", line 702, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_functions.py\", line 93, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/ml/code/inference.py\", line 15, in predict_fn\n",
      "    return model.predict(input_data)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/base.py\", line 529, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/base.py\", line 462, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "During handling of the above exception, another exception occurred:\n",
      "- activity_label\n",
      "Feature names unseen at fit time:\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Traceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_functions.py\", line 93, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/ml/code/inference.py\", line 15, in predict_fn\n",
      "    return model.predict(input_data)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/base.py\", line 529, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/base.py\", line 462, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "2025-08-25 08:32:18,093 ERROR - inference - Exception on /invocations [POST]\n",
      "169.254.178.2 - - [25/Aug/2025:08:32:14 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:32:09 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:32:04 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:31:59 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:31:54 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:31:49 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:31:44 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:31:39 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:31:34 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:31:29 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:31:24 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:31:19 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:31:19 +0000] \"POST /invocations HTTP/1.1\" 500 290 \"-\" \"AHC/2.0\"\n",
      "- activity_label\n",
      "Feature names unseen at fit time:\n",
      "sagemaker_containers._errors.ClientError: The feature names should match those that were passed during fit.\n",
      "Traceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/flask/app.py\", line 2446, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/flask/app.py\", line 1951, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/flask/app.py\", line 1820, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/flask/_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/flask/app.py\", line 1949, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/flask/app.py\", line 1935, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_transformer.py\", line 199, in transform\n",
      "    result = self._transform_fn(\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_transformer.py\", line 231, in _default_transform_fn\n",
      "    prediction = self._predict_fn(data, model)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_functions.py\", line 95, in wrapper\n",
      "    six.reraise(error_class, error_class(e), sys.exc_info()[2])\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/six.py\", line 702, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_functions.py\", line 93, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/ml/code/inference.py\", line 15, in predict_fn\n",
      "    return model.predict(input_data)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/base.py\", line 529, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/base.py\", line 462, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "During handling of the above exception, another exception occurred:\n",
      "- activity_label\n",
      "Feature names unseen at fit time:\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Traceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_functions.py\", line 93, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/ml/code/inference.py\", line 15, in predict_fn\n",
      "    return model.predict(input_data)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/base.py\", line 529, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/miniconda3/lib/python3.9/site-packages/sklearn/base.py\", line 462, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "2025-08-25 08:31:19,008 ERROR - inference - Exception on /invocations [POST]\n",
      "169.254.178.2 - - [25/Aug/2025:08:31:14 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:31:09 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:31:04 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:30:59 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:30:54 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:30:49 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:30:44 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:08:30:39 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "------------------------------------------------------ \n",
      "\n",
      "--- Logs from stream: AllTraffic/i-0501ae24ca06d2f4f ---\n",
      "169.254.178.2 - - [25/Aug/2025:07:49:53 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:49:48 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:49:43 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:49:38 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:49:33 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:49:28 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:49:23 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:49:18 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:49:13 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:49:08 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:49:03 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:48:58 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:48:53 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:48:48 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:48:43 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:48:38 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:48:33 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:48:28 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:48:23 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:48:18 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:48:13 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:48:08 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:48:03 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:47:58 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:47:53 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:47:48 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:47:43 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:47:38 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:47:33 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:47:28 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:47:23 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:47:18 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:47:13 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:47:08 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:47:03 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:46:58 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:46:53 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:46:48 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:46:43 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:46:38 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:46:33 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:46:28 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:46:23 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:46:18 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:46:13 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:46:08 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:46:03 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:45:58 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:45:53 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [25/Aug/2025:07:45:48 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Enter the name of your SageMaker endpoint\n",
    "endpoint_name = \"activity-labels-clf\"\n",
    "\n",
    "# The log group is created based on the endpoint name\n",
    "log_group_name = f\"/aws/sagemaker/Endpoints/{endpoint_name}\"\n",
    "\n",
    "# Create a CloudWatch Logs client\n",
    "logs_client = boto3.client(\"logs\")\n",
    "\n",
    "print(f\"Searching for logs in: {log_group_name}\\n\")\n",
    "\n",
    "try:\n",
    "    # Find all log streams in the log group, ordered by the most recent\n",
    "    response = logs_client.describe_log_streams(\n",
    "        logGroupName=log_group_name,\n",
    "        orderBy='LastEventTime',\n",
    "        descending=True\n",
    "    )\n",
    "\n",
    "    log_streams = response.get(\"logStreams\", [])\n",
    "\n",
    "    if not log_streams:\n",
    "        print(\"No log streams found. The endpoint might not have processed any requests yet.\")\n",
    "    \n",
    "    # Loop through each stream and print its recent log events\n",
    "    for stream in log_streams:\n",
    "        stream_name = stream['logStreamName']\n",
    "        print(f\"--- Logs from stream: {stream_name} ---\")\n",
    "\n",
    "        # Get log events from the stream\n",
    "        log_events = logs_client.get_log_events(\n",
    "            logGroupName=log_group_name,\n",
    "            logStreamName=stream_name,\n",
    "            startFromHead=False,  # False gets recent logs first\n",
    "            limit=50  # Get up to 50 recent log events\n",
    "        )\n",
    "        \n",
    "        # Print events in chronological order\n",
    "        for event in reversed(log_events.get(\"events\", [])):\n",
    "            print(event['message'].strip())\n",
    "        \n",
    "        print(\"-\" * (len(stream_name) + 24), \"\\n\")\n",
    "\n",
    "except logs_client.exceptions.ResourceNotFoundException:\n",
    "    print(f\"Error: Log group '{log_group_name}' was not found.\")\n",
    "    print(\"Please check the endpoint name and ensure it has been invoked.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49938d-658b-4a23-9a72-a6ac4a1a56f4",
   "metadata": {},
   "source": [
    "#### Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f28b28b-b413-49ff-a3f7-ad113ec2af8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T07:46:20.787955Z",
     "iopub.status.busy": "2025-08-25T07:46:20.787349Z",
     "iopub.status.idle": "2025-08-25T07:46:22.036863Z",
     "shell.execute_reply": "2025-08-25T07:46:22.036308Z",
     "shell.execute_reply.started": "2025-08-25T07:46:20.787932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting SageMaker endpoint: activity-labels-clf...\n",
      "Endpoint deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Clean Up Resources\n",
    "print(f\"Deleting SageMaker endpoint: {endpoint_name}...\")\n",
    "predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully.\")\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    print(f\"Deleted old endpoint config: {endpoint_name}\")\n",
    "except ClientError as e:\n",
    "    if \"Could not find\" not in str(e):\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e5e93-7cf4-4519-b836-658fce0e8739",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Automated deployment with second pipeline\n",
    "(Couldn't make the Eventbridge work despite watching for 'model created with approved status' on top of 'model state change to approved status' -  pipeline created but no executions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7229dd4a-a694-4c84-a993-b00b9bfda54a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Deployment script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b69eb8ac-a7be-4d2a-ae6b-dfc307875a34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T08:20:54.214977Z",
     "iopub.status.busy": "2025-07-28T08:20:54.214602Z",
     "iopub.status.idle": "2025-07-28T08:20:54.220425Z",
     "shell.execute_reply": "2025-07-28T08:20:54.219713Z",
     "shell.execute_reply.started": "2025-07-28T08:20:54.214951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deploy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy.py\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# --- Install required packages ---\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"boto3==1.28.57\", \"botocore==1.31.57\", \"numpy==1.24.1\", \"sagemaker\" ])\n",
    "\n",
    "# Ensure sagemaker SDK is installed before importing\n",
    "try:\n",
    "    import sagemaker\n",
    "except ImportError:\n",
    "    print(\"sagemaker SDK not found. Installing now...\")\n",
    "    install(\"sagemaker\")\n",
    "    import sagemaker\n",
    "\n",
    "import argparse\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.model import ModelPackage\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Accept the registered model's ARN instead of the S3 data path\n",
    "    parser.add_argument(\"--model-package-arn\", type=str, required=True)\n",
    "    parser.add_argument(\"--role\", type=str, required=True)\n",
    "    parser.add_argument(\"--endpoint-name\", type=str, required=True)\n",
    "    parser.add_argument(\"--region\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    boto_session = boto3.Session(region_name=args.region)\n",
    "    sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "\n",
    "    # Create a SageMaker Model object directly from the Model Package ARN\n",
    "    model = ModelPackage(\n",
    "        model_package_arn=args.model_package_arn,\n",
    "        role=args.role,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "\n",
    "    # Deploy the model to an endpoint\n",
    "    print(f\"Deploying registered model from ARN to endpoint: {args.endpoint_name}\")\n",
    "    model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.t2.medium\",\n",
    "        endpoint_name=args.endpoint_name,\n",
    "        # Update endpoint if it already exists\n",
    "        update_endpoint=True\n",
    "    )\n",
    "    print(\"Deployment complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfef03-d828-4de3-b732-e0acb636dc11",
   "metadata": {},
   "source": [
    "#### Define deployment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f5db2d2b-63e6-4ef2-a40c-fc2d47d01a20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T08:20:55.108237Z",
     "iopub.status.busy": "2025-07-28T08:20:55.107954Z",
     "iopub.status.idle": "2025-07-28T08:20:58.142948Z",
     "shell.execute_reply": "2025-07-28T08:20:58.142270Z",
     "shell.execute_reply.started": "2025-07-28T08:20:55.108217Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment pipeline ARN: arn:aws:sagemaker:ap-southeast-1:287730026636:pipeline/8815V-Deploy\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "import sagemaker\n",
    "\n",
    "# Define Parameters for the deployment pipeline\n",
    "# This will be provided by the EventBridge trigger\n",
    "model_package_arn_param = ParameterString(name=\"ModelPackageArn\", default_value=\"\")\n",
    "role_param = ParameterString(name=\"ExecutionRole\", default_value=sagemaker_role)\n",
    "endpoint_name_param = ParameterString(name=\"EndpointName\", default_value=\"8815v-penguin-clf-endpoint\")\n",
    "\n",
    "# Create a ScriptProcessor for deployment\n",
    "# Using a more recent scikit-learn version is generally a good idea\n",
    "deploy_processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"sklearn\", sagemaker_session.boto_region_name, version=\"1.2-1\"),\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    role=role_param,\n",
    "    base_job_name=\"deploy-registered-model\"\n",
    ")\n",
    "\n",
    "# Define the deployment step that takes the model ARN as an argument\n",
    "step_deploy = ProcessingStep(\n",
    "    name=\"DeployRegisteredModel\",\n",
    "    processor=deploy_processor,\n",
    "    code=\"deploy.py\",\n",
    "    job_arguments=[\n",
    "        \"--model-package-arn\", model_package_arn_param,\n",
    "        \"--role\", role_param,\n",
    "        \"--endpoint-name\", endpoint_name_param,\n",
    "        \"--region\", \"ap-southeast-1\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define the independent deployment pipeline\n",
    "deploy_pipeline = Pipeline(\n",
    "    name=\"8815V-Deploy\",\n",
    "    parameters=[model_package_arn_param, role_param, endpoint_name_param],\n",
    "    steps=[step_deploy]\n",
    ")\n",
    "\n",
    "# Create or update the pipeline definition\n",
    "# Capture the response which contains the ARN\n",
    "response = deploy_pipeline.upsert(role_arn=sagemaker_role)\n",
    "\n",
    "# Extract the ARN from the response dictionary\n",
    "pipeline_arn = response['PipelineArn']\n",
    "\n",
    "print(f\"Deployment pipeline ARN: {pipeline_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c1460-a499-40bd-8af9-5c85f057bbbc",
   "metadata": {},
   "source": [
    "#### Eventbridge to watch for event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d2008ef-6458-43ec-a161-00b8afaac22e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T08:20:58.144402Z",
     "iopub.status.busy": "2025-07-28T08:20:58.144051Z",
     "iopub.status.idle": "2025-07-28T08:20:58.229554Z",
     "shell.execute_reply": "2025-07-28T08:20:58.228832Z",
     "shell.execute_reply.started": "2025-07-28T08:20:58.144381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating or updating EventBridge rule: 8708815v-myaccount.nyp.edu.sg-TriggerModelDeploymentOnApproval\n",
      "EventBridge rule created successfully!\n",
      "Now, when a model is approved in the Model Registry, the deployment pipeline will trigger automatically.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize the EventBridge client\n",
    "events_client = boto3.client(\"events\")\n",
    "\n",
    "# Define the event pattern to listen for\n",
    "# This pattern triggers when a model package in your group has its status changed to \"Approved\"\n",
    "event_pattern = {\n",
    "    \"source\": [\"aws.sagemaker\"],\n",
    "    \"detail-type\": [\n",
    "        \"SageMaker Model Package State Change\",\n",
    "        \"SageMaker Model Package Created\"],\n",
    "    \"detail\": {\n",
    "        \"ModelPackageGroupName\": [model_package_group_name], \n",
    "        \"ModelApprovalStatus\": [\"Approved\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the target for the rule (our deployment pipeline)\n",
    "# We need to map the event's detail to the pipeline's parameters\n",
    "target = {\n",
    "    \"Id\": \"DeployPenguinPipelineTarget\",\n",
    "    \"Arn\": pipeline_arn, # The ARN of the pipeline we just created\n",
    "    \"RoleArn\": sagemaker_role, # The execution role for the pipeline\n",
    "    \"SageMakerPipelineParameters\": {\n",
    "        \"PipelineParameterList\": [\n",
    "            {\n",
    "                # Map the ARN from the event to the pipeline's \"ModelPackageArn\" parameter\n",
    "                \"Name\": \"ModelPackageArn\",\n",
    "                \"Value\": \"$.detail.ModelPackageArn\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create or update the EventBridge rule\n",
    "try:\n",
    "    username_lower = \"8708815v@myaccount.nyp.edu.sg\".lower().replace(\"@\", \"-\")\n",
    "    rule_name = f\"{username_lower}-TriggerModelDeploymentOnApproval\"\n",
    "    print(f\"Creating or updating EventBridge rule: {rule_name}\")\n",
    "    response = events_client.put_rule(\n",
    "        Name=rule_name,\n",
    "        EventPattern=json.dumps(event_pattern),\n",
    "        State=\"ENABLED\",\n",
    "        Description=\"Triggers the SageMaker pipeline to deploy a model upon approval.\"\n",
    "    )\n",
    "    \n",
    "    # Add the pipeline as a target for the rule\n",
    "    events_client.put_targets(Rule=rule_name, Targets=[target])\n",
    "    print(\"EventBridge rule created successfully!\")\n",
    "    print(\"Now, when a model is approved in the Model Registry, the deployment pipeline will trigger automatically.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating rule: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df1104-4ac3-47df-a5ef-3631cc729280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
