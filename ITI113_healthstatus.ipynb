{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c51a123-9e42-468e-abf5-33df80907797",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's install the necessary libraries and configure our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5cd559d-0cf2-4c5f-803c-1092c41bf6f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T05:23:58.506060Z",
     "iopub.status.busy": "2025-08-18T05:23:58.505789Z",
     "iopub.status.idle": "2025-08-18T05:24:00.060562Z",
     "shell.execute_reply": "2025-08-18T05:24:00.059987Z",
     "shell.execute_reply.started": "2025-08-18T05:23:58.506042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.3.1 requires nvidia-ml-py3<8.0,>=7.352.0, which is not installed.\n",
      "jupyter-ai 2.31.5 requires faiss-cpu!=1.8.0.post0,<2.0.0,>=1.8.0, which is not installed.\n",
      "autogluon-multimodal 1.3.1 requires jsonschema<4.24,>=4.18, but you have jsonschema 4.25.0 which is incompatible.\n",
      "autogluon-multimodal 1.3.1 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
      "autogluon-multimodal 1.3.1 requires pandas<2.3.0,>=2.0.0, but you have pandas 2.3.1 which is incompatible.\n",
      "autogluon-multimodal 1.3.1 requires scikit-learn<1.7.0,>=1.4.0, but you have scikit-learn 1.7.1 which is incompatible.\n",
      "autogluon-multimodal 1.3.1 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.53.1 which is incompatible.\n",
      "autogluon-timeseries 1.3.1 requires pandas<2.3.0,>=2.0.0, but you have pandas 2.3.1 which is incompatible.\n",
      "autogluon-timeseries 1.3.1 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.53.1 which is incompatible.\n",
      "dask 2025.5.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
      "distributed 2025.5.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
      "sagemaker-studio-analytics-extension 0.2.0 requires sparkmagic==0.22.0, but you have sparkmagic 0.21.0 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.3.1 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "weasel 0.4.1 requires smart-open<8.0.0,>=5.2.1, but you have smart-open 0.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# The SageMaker Studio environment comes with most of these pre-installed.\n",
    "# This cell ensures all dependencies are present.\n",
    "!pip install -q boto3 sagemaker mlflow \"scikit-learn>=1.0\" \"pandas>=1.2\"\n",
    "# !pip install -q boto3 sagemaker mlflow \"scikit-learn>=1.7\" \"pandas>=2.3\" \"matplotlib>=3.10\" \"numpy>=2.0\" \"seaborn>=0.13\"\n",
    "# !pip install --upgrade sagemaker google-api-core grpcio\n",
    "# !pip install \"sagemaker==2.218.0\" \"protobuf==3.20.*\" --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2ac10a-5312-42c5-a9c2-436550dceb41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:30:08.361527Z",
     "iopub.status.busy": "2025-08-19T08:30:08.361239Z",
     "iopub.status.idle": "2025-08-19T08:30:10.163671Z",
     "shell.execute_reply": "2025-08-19T08:30:10.163220Z",
     "shell.execute_reply.started": "2025-08-19T08:30:08.361508Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Ensure MLflow is installed\n",
    "try:\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow\n",
    "except ImportError:\n",
    "    print(\"Installing MLflow...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",  \"boto3==1.37.1\", \"botocore==1.37.1\", \"s3transfer\", \"mlflow==2.22.0\", \"sagemaker-mlflow==0.1.0\"])\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc425c68-2edf-4e6b-944e-96119e59564f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:30:10.164786Z",
     "iopub.status.busy": "2025-08-19T08:30:10.164432Z",
     "iopub.status.idle": "2025-08-19T08:30:11.849258Z",
     "shell.execute_reply": "2025-08-19T08:30:11.848804Z",
     "shell.execute_reply.started": "2025-08-19T08:30:10.164760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "import os\n",
    "import io\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948dd23-fd2c-4ac8-b9a7-4356b6145816",
   "metadata": {},
   "source": [
    "### Set up Sagemaker session & S3 folder for assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c127ebd-613c-4d8e-88a1-69eafc776da8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:30:11.850038Z",
     "iopub.status.busy": "2025-08-19T08:30:11.849747Z",
     "iopub.status.idle": "2025-08-19T08:30:12.403583Z",
     "shell.execute_reply": "2025-08-19T08:30:12.403144Z",
     "shell.execute_reply.started": "2025-08-19T08:30:11.850023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# --- IMPORTANT: CONFIGURE THESE VARIABLES ---\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "# ----------------------\n",
    "# UPDATE THESE VARIABLES\n",
    "bucket_name = 'iti113-team12-bucket'  # e.g., 'my-company-sagemaker-bucket'\n",
    "base_folder = 'project'      # e.g., 'users/my-name'\n",
    "# ----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3cd97a",
   "metadata": {},
   "source": [
    "### Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6beefbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:30:12.404726Z",
     "iopub.status.busy": "2025-08-19T08:30:12.404494Z",
     "iopub.status.idle": "2025-08-19T08:30:12.487713Z",
     "shell.execute_reply": "2025-08-19T08:30:12.487293Z",
     "shell.execute_reply.started": "2025-08-19T08:30:12.404710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra dataset at: s3://iti113-team12-bucket/project/datasets/adl\n",
      "Uploading data to s3://iti113-team12-bucket/project/datasets/adl/users/volunteer_details.csv\n",
      "Dataset uploaded to: s3://iti113-team12-bucket/project/datasets/adl/users/volunteer_details.csv\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('volunteer_details.csv')\n",
    "\n",
    "# Define the base path for our datasets\n",
    "data_path = f\"s3://{bucket_name}/{base_folder}/datasets/adl\"\n",
    "print(f\"Extra dataset at: {data_path}\")\n",
    "\n",
    "# Define S3 path for dataset\n",
    "# os.makedirs(dataset_s3_path, exist_ok=True)\n",
    "dataset_s3_path = f\"{data_path}/users/volunteer_details.csv\"\n",
    "data_v1_s3_uri = os.path.dirname(dataset_s3_path)\n",
    "\n",
    "# Upload to S3\n",
    "print(f\"Uploading data to {dataset_s3_path}\")\n",
    "csv_buffer = io.StringIO()\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "dataset_s3_key = f\"{base_folder}/datasets/adl/users/volunteer_details.csv\"\n",
    "s3_client.put_object(Bucket=bucket_name, Key=dataset_s3_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"Dataset uploaded to: {dataset_s3_path}\")\n",
    "# s3://iti113-team12-bucket/project/datasets/adl/users/volunteer_details.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b01dbab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:30:12.488384Z",
     "iopub.status.busy": "2025-08-19T08:30:12.488215Z",
     "iopub.status.idle": "2025-08-19T08:30:12.491210Z",
     "shell.execute_reply": "2025-08-19T08:30:12.490810Z",
     "shell.execute_reply.started": "2025-08-19T08:30:12.488369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 data paths: s3://iti113-team12-bucket/project/datasets/adl/adl/\n",
      " s3://iti113-team12-bucket/project/datasets/adl/fall/ \n",
      " s3://iti113-team12-bucket/project/datasets/adl/users/\n"
     ]
    }
   ],
   "source": [
    "# Define the base path for our datasets\n",
    "# Scidirect ADL: s3://iti113-team12-bucket/project/datasets/adl/adl/\n",
    "# Scidirect fall: s3://iti113-team12-bucket/project/datasets/adl/fall/ \n",
    "# Dryad C17: s3://iti113-team12-bucket/project/datasets/c17/\n",
    "# data_path = f\"s3://{bucket_name}/{base_folder}/datasets/adl\"\n",
    "data_path_adl = \"s3://iti113-team12-bucket/project/datasets/adl/adl/\"\n",
    "data_path_fall = \"s3://iti113-team12-bucket/project/datasets/adl/fall/\"\n",
    "data_path_adlfalls_users = \"s3://iti113-team12-bucket/project/datasets/adl/users/\"\n",
    "\n",
    "print(f\"S3 data paths: {data_path_adl}\\n {data_path_fall} \\n {data_path_adlfalls_users}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e4da0-391b-41f6-b172-3a68b699eb21",
   "metadata": {},
   "source": [
    "### Check file exists in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "380c1051-a0f4-4acd-a759-bd0adcf8dff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:30:12.491892Z",
     "iopub.status.busy": "2025-08-19T08:30:12.491637Z",
     "iopub.status.idle": "2025-08-19T08:30:12.545364Z",
     "shell.execute_reply": "2025-08-19T08:30:12.544973Z",
     "shell.execute_reply.started": "2025-08-19T08:30:12.491879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Key': 'project/datasets/adl/users/volunteer_details.csv', 'LastModified': datetime.datetime(2025, 8, 19, 8, 30, 13, tzinfo=tzlocal()), 'ETag': '\"1b9bf950ed61e8618a4a5d37a85130ad\"', 'ChecksumAlgorithm': ['CRC32'], 'ChecksumType': 'FULL_OBJECT', 'Size': 2086, 'StorageClass': 'STANDARD'}]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "resp = s3.list_objects_v2(\n",
    "    Bucket=\"iti113-team12-bucket\",\n",
    "    Prefix=\"project/datasets/adl/users/volunteer_details.csv\"\n",
    ")\n",
    "print(resp.get(\"Contents\"))\n",
    "\n",
    "# s3://iti113-team12-bucket/project/datasets/adl/users/volunteer_details.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab46daf7-9d4b-4561-8825-c976e54eca97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:30:12.546113Z",
     "iopub.status.busy": "2025-08-19T08:30:12.545952Z",
     "iopub.status.idle": "2025-08-19T08:30:12.564905Z",
     "shell.execute_reply": "2025-08-19T08:30:12.564528Z",
     "shell.execute_reply.started": "2025-08-19T08:30:12.546099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File found: s3://iti113-team12-bucket/project/datasets/adl/users/volunteer_details.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_file_exists(bucket, key):\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket, Key=key)\n",
    "        print(f\"✅ File found: s3://{bucket}/{key}\")\n",
    "        return True\n",
    "    except s3_client.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            print(f\"❌ File not found: s3://{bucket}/{key}\")\n",
    "            return False\n",
    "        else:\n",
    "            raise  # Something else went wrong\n",
    "\n",
    "# s3://iti113-team12-bucket/project/datasets/adl/users/volunteer_details.csv\n",
    "check_file_exists(bucket_name, \"project/datasets/adl/users/volunteer_details.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f938d5-5fb0-4876-9d61-ac3a3c5b929f",
   "metadata": {},
   "source": [
    "### Set up tracking server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f364b43c-ed09-4dab-b4c7-3d74a742afdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:30:12.565596Z",
     "iopub.status.busy": "2025-08-19T08:30:12.565444Z",
     "iopub.status.idle": "2025-08-19T08:30:12.718353Z",
     "shell.execute_reply": "2025-08-19T08:30:12.717944Z",
     "shell.execute_reply.started": "2025-08-19T08:30:12.565583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found MLflow Tracking Server ARN: arn:aws:sagemaker:ap-southeast-1:837028399719:mlflow-tracking-server/mlflow-elderly\n",
      "SageMaker Role ARN: arn:aws:iam::837028399719:role/iti113-team12-sagemaker-iti113-team12-domain-iti113-team12-Role\n",
      "MLflow Tracking Server ARN: arn:aws:sagemaker:ap-southeast-1:837028399719:mlflow-tracking-server/mlflow-elderly\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have your boto3 client and server name\n",
    "tracking_server_name = \"mlflow-elderly\"\n",
    "\n",
    "try:\n",
    "    response = sagemaker_client.describe_mlflow_tracking_server(\n",
    "        TrackingServerName=tracking_server_name\n",
    "    )\n",
    "    mlflow_tracking_server_arn = response['TrackingServerArn']\n",
    "    print(f\"Found MLflow Tracking Server ARN: {mlflow_tracking_server_arn}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not find tracking server: {e}\")\n",
    "    mlflow_tracking_server_arn = None\n",
    "\n",
    "# ARN of your MLflow Tracking Server\n",
    "# Find this in the SageMaker console or by running `aws sagemaker list-mlflow-tracking-servers`\n",
    "# mlflow_tracking_server_arn = tracking_server_arn\n",
    "\n",
    "\n",
    "# IAM role for SageMaker execution\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"SageMaker Role ARN: {sagemaker_role}\")\n",
    "print(f\"MLflow Tracking Server ARN: {mlflow_tracking_server_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd155d9-ea39-4610-924a-2217a155eb8a",
   "metadata": {},
   "source": [
    "## Read and combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b70b8d5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:30:12.719083Z",
     "iopub.status.busy": "2025-08-19T08:30:12.718853Z",
     "iopub.status.idle": "2025-08-19T08:30:12.721837Z",
     "shell.execute_reply": "2025-08-19T08:30:12.721471Z",
     "shell.execute_reply.started": "2025-08-19T08:30:12.719068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Activity mappings\n",
    "adl_labels = {\n",
    "    1: 'Walking slowly',\n",
    "    2: 'Walking quickly',\n",
    "    3: 'Jogging',\n",
    "    4: 'Jumping',\n",
    "    5: 'Climbing up slowly',\n",
    "    6: 'Climbing down slowly',\n",
    "    7: 'Climbing up normally',\n",
    "    8: 'Climbing down normally',\n",
    "    9: 'Slowly sitting on chair',\n",
    "    10: 'Rapidly sitting on chair',\n",
    "    11: 'Nearly sitting on chair and getting up',\n",
    "    12: 'Swinging hands',\n",
    "    13: 'Lying on bed',\n",
    "    14: 'Lying on back and getting up slowly',\n",
    "    15: 'Lying on back and getting up normally',\n",
    "    16: 'Transition form sideways to ones back while lying'\n",
    "}\n",
    "fall_labels = {\n",
    "    1: 'Forward fall landing on knee',\n",
    "    2: 'Right fall',\n",
    "    3: 'Left fall',\n",
    "    4: 'Forward fall',\n",
    "    5: 'Seated on bed and falling on ground',\n",
    "    6: 'Forward fall body weight on hand',\n",
    "    7: 'Backward fall from seated position',\n",
    "    8: 'Grabbing while falling'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f530a69f-ca1e-4566-a4d8-1d5d561bd4a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:30:12.723223Z",
     "iopub.status.busy": "2025-08-19T08:30:12.723000Z",
     "iopub.status.idle": "2025-08-19T08:31:23.580130Z",
     "shell.execute_reply": "2025-08-19T08:31:23.579675Z",
     "shell.execute_reply.started": "2025-08-19T08:30:12.723208Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous combined file removed from S3.\n",
      "Starting to combine CSV...\n",
      "Found: user1_adl1.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl1.csv | user1 | Activity 1 → Walking slowly\n",
      "Found: user1_adl10.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl10.csv | user1 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user1_adl11.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl11.csv | user1 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user1_adl12.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl12.csv | user1 | Activity 12 → Swinging hands\n",
      "Found: user1_adl13.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl13.csv | user1 | Activity 13 → Lying on bed\n",
      "Found: user1_adl14.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl14.csv | user1 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user1_adl15.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl15.csv | user1 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user1_adl16.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl16.csv | user1 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user1_adl2.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl2.csv | user1 | Activity 2 → Walking quickly\n",
      "Found: user1_adl3.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl3.csv | user1 | Activity 3 → Jogging\n",
      "Found: user1_adl4.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl4.csv | user1 | Activity 4 → Jumping\n",
      "Found: user1_adl5.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl5.csv | user1 | Activity 5 → Climbing up slowly\n",
      "Found: user1_adl6.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl6.csv | user1 | Activity 6 → Climbing down slowly\n",
      "Found: user1_adl7.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl7.csv | user1 | Activity 7 → Climbing up normally\n",
      "Found: user1_adl8.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl8.csv | user1 | Activity 8 → Climbing down normally\n",
      "Found: user1_adl9.csv\n",
      "Processing project/datasets/adl/adl/user1/user1_adl9.csv | user1 | Activity 9 → Slowly sitting on chair\n",
      "Found: user10_adl1.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl1.csv | user10 | Activity 1 → Walking slowly\n",
      "Found: user10_adl10.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl10.csv | user10 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user10_adl11.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl11.csv | user10 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user10_adl12.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl12.csv | user10 | Activity 12 → Swinging hands\n",
      "Found: user10_adl13.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl13.csv | user10 | Activity 13 → Lying on bed\n",
      "Found: user10_adl14.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl14.csv | user10 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user10_adl15.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl15.csv | user10 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user10_adl16.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl16.csv | user10 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user10_adl2.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl2.csv | user10 | Activity 2 → Walking quickly\n",
      "Found: user10_adl3.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl3.csv | user10 | Activity 3 → Jogging\n",
      "Found: user10_adl4.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl4.csv | user10 | Activity 4 → Jumping\n",
      "Found: user10_adl5.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl5.csv | user10 | Activity 5 → Climbing up slowly\n",
      "Found: user10_adl6.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl6.csv | user10 | Activity 6 → Climbing down slowly\n",
      "Found: user10_adl7.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl7.csv | user10 | Activity 7 → Climbing up normally\n",
      "Found: user10_adl8.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl8.csv | user10 | Activity 8 → Climbing down normally\n",
      "Found: user10_adl9.csv\n",
      "Processing project/datasets/adl/adl/user10/user10_adl9.csv | user10 | Activity 9 → Slowly sitting on chair\n",
      "Found: user11_adl1.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl1.csv | user11 | Activity 1 → Walking slowly\n",
      "Found: user11_adl10.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl10.csv | user11 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user11_adl11.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl11.csv | user11 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user11_adl12.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl12.csv | user11 | Activity 12 → Swinging hands\n",
      "Found: user11_adl13.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl13.csv | user11 | Activity 13 → Lying on bed\n",
      "Found: user11_adl14.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl14.csv | user11 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user11_adl15.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl15.csv | user11 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user11_adl16.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl16.csv | user11 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user11_adl2.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl2.csv | user11 | Activity 2 → Walking quickly\n",
      "Found: user11_adl3.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl3.csv | user11 | Activity 3 → Jogging\n",
      "Found: user11_adl4.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl4.csv | user11 | Activity 4 → Jumping\n",
      "Found: user11_adl5.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl5.csv | user11 | Activity 5 → Climbing up slowly\n",
      "Found: user11_adl6.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl6.csv | user11 | Activity 6 → Climbing down slowly\n",
      "Found: user11_adl7.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl7.csv | user11 | Activity 7 → Climbing up normally\n",
      "Found: user11_adl8.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl8.csv | user11 | Activity 8 → Climbing down normally\n",
      "Found: user11_adl9.csv\n",
      "Processing project/datasets/adl/adl/user11/user11_adl9.csv | user11 | Activity 9 → Slowly sitting on chair\n",
      "Found: user12_adl1.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl1.csv | user12 | Activity 1 → Walking slowly\n",
      "Found: user12_adl10.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl10.csv | user12 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user12_adl11.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl11.csv | user12 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user12_adl12.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl12.csv | user12 | Activity 12 → Swinging hands\n",
      "Found: user12_adl13.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl13.csv | user12 | Activity 13 → Lying on bed\n",
      "Found: user12_adl14.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl14.csv | user12 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user12_adl15.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl15.csv | user12 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user12_adl16.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl16.csv | user12 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user12_adl2.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl2.csv | user12 | Activity 2 → Walking quickly\n",
      "Found: user12_adl3.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl3.csv | user12 | Activity 3 → Jogging\n",
      "Found: user12_adl4.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl4.csv | user12 | Activity 4 → Jumping\n",
      "Found: user12_adl5.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl5.csv | user12 | Activity 5 → Climbing up slowly\n",
      "Found: user12_adl6.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl6.csv | user12 | Activity 6 → Climbing down slowly\n",
      "Found: user12_adl7.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl7.csv | user12 | Activity 7 → Climbing up normally\n",
      "Found: user12_adl8.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl8.csv | user12 | Activity 8 → Climbing down normally\n",
      "Found: user12_adl9.csv\n",
      "Processing project/datasets/adl/adl/user12/user12_adl9.csv | user12 | Activity 9 → Slowly sitting on chair\n",
      "Found: user13_adl1.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl1.csv | user13 | Activity 1 → Walking slowly\n",
      "Found: user13_adl10.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl10.csv | user13 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user13_adl11.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl11.csv | user13 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user13_adl12.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl12.csv | user13 | Activity 12 → Swinging hands\n",
      "Found: user13_adl13.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl13.csv | user13 | Activity 13 → Lying on bed\n",
      "Found: user13_adl14.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl14.csv | user13 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user13_adl15.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl15.csv | user13 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user13_adl16.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl16.csv | user13 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user13_adl2.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl2.csv | user13 | Activity 2 → Walking quickly\n",
      "Found: user13_adl3.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl3.csv | user13 | Activity 3 → Jogging\n",
      "Found: user13_adl4.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl4.csv | user13 | Activity 4 → Jumping\n",
      "Found: user13_adl5.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl5.csv | user13 | Activity 5 → Climbing up slowly\n",
      "Found: user13_adl6.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl6.csv | user13 | Activity 6 → Climbing down slowly\n",
      "Found: user13_adl7.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl7.csv | user13 | Activity 7 → Climbing up normally\n",
      "Found: user13_adl8.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl8.csv | user13 | Activity 8 → Climbing down normally\n",
      "Found: user13_adl9.csv\n",
      "Processing project/datasets/adl/adl/user13/user13_adl9.csv | user13 | Activity 9 → Slowly sitting on chair\n",
      "Found: user14_adl1.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl1.csv | user14 | Activity 1 → Walking slowly\n",
      "Found: user14_adl10.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl10.csv | user14 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user14_adl11.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl11.csv | user14 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user14_adl12.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl12.csv | user14 | Activity 12 → Swinging hands\n",
      "Found: user14_adl13.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl13.csv | user14 | Activity 13 → Lying on bed\n",
      "Found: user14_adl14.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl14.csv | user14 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user14_adl15.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl15.csv | user14 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user14_adl16.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl16.csv | user14 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user14_adl2.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl2.csv | user14 | Activity 2 → Walking quickly\n",
      "Found: user14_adl3.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl3.csv | user14 | Activity 3 → Jogging\n",
      "Found: user14_adl4.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl4.csv | user14 | Activity 4 → Jumping\n",
      "Found: user14_adl5.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl5.csv | user14 | Activity 5 → Climbing up slowly\n",
      "Found: user14_adl6.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl6.csv | user14 | Activity 6 → Climbing down slowly\n",
      "Found: user14_adl7.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl7.csv | user14 | Activity 7 → Climbing up normally\n",
      "Found: user14_adl8.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl8.csv | user14 | Activity 8 → Climbing down normally\n",
      "Found: user14_adl9.csv\n",
      "Processing project/datasets/adl/adl/user14/user14_adl9.csv | user14 | Activity 9 → Slowly sitting on chair\n",
      "Found: user15_adl1.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl1.csv | user15 | Activity 1 → Walking slowly\n",
      "Found: user15_adl10.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl10.csv | user15 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user15_adl11.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl11.csv | user15 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user15_adl12.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl12.csv | user15 | Activity 12 → Swinging hands\n",
      "Found: user15_adl13.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl13.csv | user15 | Activity 13 → Lying on bed\n",
      "Found: user15_adl14.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl14.csv | user15 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user15_adl15.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl15.csv | user15 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user15_adl16.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl16.csv | user15 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user15_adl2.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl2.csv | user15 | Activity 2 → Walking quickly\n",
      "Found: user15_adl3.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl3.csv | user15 | Activity 3 → Jogging\n",
      "Found: user15_adl4.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl4.csv | user15 | Activity 4 → Jumping\n",
      "Found: user15_adl5.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl5.csv | user15 | Activity 5 → Climbing up slowly\n",
      "Found: user15_adl6.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl6.csv | user15 | Activity 6 → Climbing down slowly\n",
      "Found: user15_adl7.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl7.csv | user15 | Activity 7 → Climbing up normally\n",
      "Found: user15_adl8.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl8.csv | user15 | Activity 8 → Climbing down normally\n",
      "Found: user15_adl9.csv\n",
      "Processing project/datasets/adl/adl/user15/user15_adl9.csv | user15 | Activity 9 → Slowly sitting on chair\n",
      "Found: user16_adl1.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl1.csv | user16 | Activity 1 → Walking slowly\n",
      "Found: user16_adl10.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl10.csv | user16 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user16_adl11.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl11.csv | user16 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user16_adl12.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl12.csv | user16 | Activity 12 → Swinging hands\n",
      "Found: user16_adl13.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl13.csv | user16 | Activity 13 → Lying on bed\n",
      "Found: user16_adl14.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl14.csv | user16 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user16_adl15.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl15.csv | user16 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user16_adl16.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl16.csv | user16 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user16_adl2.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl2.csv | user16 | Activity 2 → Walking quickly\n",
      "Found: user16_adl3.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl3.csv | user16 | Activity 3 → Jogging\n",
      "Found: user16_adl4.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl4.csv | user16 | Activity 4 → Jumping\n",
      "Found: user16_adl5.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl5.csv | user16 | Activity 5 → Climbing up slowly\n",
      "Found: user16_adl6.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl6.csv | user16 | Activity 6 → Climbing down slowly\n",
      "Found: user16_adl7.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl7.csv | user16 | Activity 7 → Climbing up normally\n",
      "Found: user16_adl8.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl8.csv | user16 | Activity 8 → Climbing down normally\n",
      "Found: user16_adl9.csv\n",
      "Processing project/datasets/adl/adl/user16/user16_adl9.csv | user16 | Activity 9 → Slowly sitting on chair\n",
      "Found: user17_adl1.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl1.csv | user17 | Activity 1 → Walking slowly\n",
      "Found: user17_adl10.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl10.csv | user17 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user17_adl11.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl11.csv | user17 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user17_adl12.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl12.csv | user17 | Activity 12 → Swinging hands\n",
      "Found: user17_adl13.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl13.csv | user17 | Activity 13 → Lying on bed\n",
      "Found: user17_adl14.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl14.csv | user17 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user17_adl15.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl15.csv | user17 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user17_adl16.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl16.csv | user17 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user17_adl2.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl2.csv | user17 | Activity 2 → Walking quickly\n",
      "Found: user17_adl3.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl3.csv | user17 | Activity 3 → Jogging\n",
      "Found: user17_adl4.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl4.csv | user17 | Activity 4 → Jumping\n",
      "Found: user17_adl5.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl5.csv | user17 | Activity 5 → Climbing up slowly\n",
      "Found: user17_adl6.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl6.csv | user17 | Activity 6 → Climbing down slowly\n",
      "Found: user17_adl7.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl7.csv | user17 | Activity 7 → Climbing up normally\n",
      "Found: user17_adl8.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl8.csv | user17 | Activity 8 → Climbing down normally\n",
      "Found: user17_adl9.csv\n",
      "Processing project/datasets/adl/adl/user17/user17_adl9.csv | user17 | Activity 9 → Slowly sitting on chair\n",
      "Found: user18_adl1.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl1.csv | user18 | Activity 1 → Walking slowly\n",
      "Found: user18_adl10.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl10.csv | user18 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user18_adl11.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl11.csv | user18 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user18_adl12.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl12.csv | user18 | Activity 12 → Swinging hands\n",
      "Found: user18_adl13.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl13.csv | user18 | Activity 13 → Lying on bed\n",
      "Found: user18_adl14.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl14.csv | user18 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user18_adl15.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl15.csv | user18 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user18_adl16.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl16.csv | user18 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user18_adl2.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl2.csv | user18 | Activity 2 → Walking quickly\n",
      "Found: user18_adl3.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl3.csv | user18 | Activity 3 → Jogging\n",
      "Found: user18_adl4.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl4.csv | user18 | Activity 4 → Jumping\n",
      "Found: user18_adl5.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl5.csv | user18 | Activity 5 → Climbing up slowly\n",
      "Found: user18_adl6.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl6.csv | user18 | Activity 6 → Climbing down slowly\n",
      "Found: user18_adl7.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl7.csv | user18 | Activity 7 → Climbing up normally\n",
      "Found: user18_adl8.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl8.csv | user18 | Activity 8 → Climbing down normally\n",
      "Found: user18_adl9.csv\n",
      "Processing project/datasets/adl/adl/user18/user18_adl9.csv | user18 | Activity 9 → Slowly sitting on chair\n",
      "Found: user19_adl1.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl1.csv | user19 | Activity 1 → Walking slowly\n",
      "Found: user19_adl10.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl10.csv | user19 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user19_adl11.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl11.csv | user19 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user19_adl12.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl12.csv | user19 | Activity 12 → Swinging hands\n",
      "Found: user19_adl13.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl13.csv | user19 | Activity 13 → Lying on bed\n",
      "Found: user19_adl14.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl14.csv | user19 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user19_adl15.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl15.csv | user19 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user19_adl16.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl16.csv | user19 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user19_adl2.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl2.csv | user19 | Activity 2 → Walking quickly\n",
      "Found: user19_adl3.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl3.csv | user19 | Activity 3 → Jogging\n",
      "Found: user19_adl4.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl4.csv | user19 | Activity 4 → Jumping\n",
      "Found: user19_adl5.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl5.csv | user19 | Activity 5 → Climbing up slowly\n",
      "Found: user19_adl6.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl6.csv | user19 | Activity 6 → Climbing down slowly\n",
      "Found: user19_adl7.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl7.csv | user19 | Activity 7 → Climbing up normally\n",
      "Found: user19_adl8.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl8.csv | user19 | Activity 8 → Climbing down normally\n",
      "Found: user19_adl9.csv\n",
      "Processing project/datasets/adl/adl/user19/user19_adl9.csv | user19 | Activity 9 → Slowly sitting on chair\n",
      "Found: user2_adl1.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl1.csv | user2 | Activity 1 → Walking slowly\n",
      "Found: user2_adl10.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl10.csv | user2 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user2_adl11.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl11.csv | user2 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user2_adl12.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl12.csv | user2 | Activity 12 → Swinging hands\n",
      "Found: user2_adl13.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl13.csv | user2 | Activity 13 → Lying on bed\n",
      "Found: user2_adl14.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl14.csv | user2 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user2_adl15.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl15.csv | user2 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user2_adl16.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl16.csv | user2 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user2_adl2.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl2.csv | user2 | Activity 2 → Walking quickly\n",
      "Found: user2_adl3.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl3.csv | user2 | Activity 3 → Jogging\n",
      "Found: user2_adl4.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl4.csv | user2 | Activity 4 → Jumping\n",
      "Found: user2_adl5.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl5.csv | user2 | Activity 5 → Climbing up slowly\n",
      "Found: user2_adl6.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl6.csv | user2 | Activity 6 → Climbing down slowly\n",
      "Found: user2_adl7.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl7.csv | user2 | Activity 7 → Climbing up normally\n",
      "Found: user2_adl8.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl8.csv | user2 | Activity 8 → Climbing down normally\n",
      "Found: user2_adl9.csv\n",
      "Processing project/datasets/adl/adl/user2/user2_adl9.csv | user2 | Activity 9 → Slowly sitting on chair\n",
      "Found: user20_adl1.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl1.csv | user20 | Activity 1 → Walking slowly\n",
      "Found: user20_adl10.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl10.csv | user20 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user20_adl11.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl11.csv | user20 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user20_adl12.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl12.csv | user20 | Activity 12 → Swinging hands\n",
      "Found: user20_adl13.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl13.csv | user20 | Activity 13 → Lying on bed\n",
      "Found: user20_adl14.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl14.csv | user20 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user20_adl15.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl15.csv | user20 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user20_adl16.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl16.csv | user20 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user20_adl2.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl2.csv | user20 | Activity 2 → Walking quickly\n",
      "Found: user20_adl3.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl3.csv | user20 | Activity 3 → Jogging\n",
      "Found: user20_adl4.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl4.csv | user20 | Activity 4 → Jumping\n",
      "Found: user20_adl5.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl5.csv | user20 | Activity 5 → Climbing up slowly\n",
      "Found: user20_adl6.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl6.csv | user20 | Activity 6 → Climbing down slowly\n",
      "Found: user20_adl7.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl7.csv | user20 | Activity 7 → Climbing up normally\n",
      "Found: user20_adl8.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl8.csv | user20 | Activity 8 → Climbing down normally\n",
      "Found: user20_adl9.csv\n",
      "Processing project/datasets/adl/adl/user20/user20_adl9.csv | user20 | Activity 9 → Slowly sitting on chair\n",
      "Found: user21_adl1.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl1.csv | user21 | Activity 1 → Walking slowly\n",
      "Found: user21_adl10.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl10.csv | user21 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user21_adl11.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl11.csv | user21 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user21_adl12.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl12.csv | user21 | Activity 12 → Swinging hands\n",
      "Found: user21_adl13.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl13.csv | user21 | Activity 13 → Lying on bed\n",
      "Found: user21_adl14.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl14.csv | user21 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user21_adl15.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl15.csv | user21 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user21_adl16.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl16.csv | user21 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user21_adl2.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl2.csv | user21 | Activity 2 → Walking quickly\n",
      "Found: user21_adl3.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl3.csv | user21 | Activity 3 → Jogging\n",
      "Found: user21_adl4.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl4.csv | user21 | Activity 4 → Jumping\n",
      "Found: user21_adl5.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl5.csv | user21 | Activity 5 → Climbing up slowly\n",
      "Found: user21_adl6.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl6.csv | user21 | Activity 6 → Climbing down slowly\n",
      "Found: user21_adl7.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl7.csv | user21 | Activity 7 → Climbing up normally\n",
      "Found: user21_adl8.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl8.csv | user21 | Activity 8 → Climbing down normally\n",
      "Found: user21_adl9.csv\n",
      "Processing project/datasets/adl/adl/user21/user21_adl9.csv | user21 | Activity 9 → Slowly sitting on chair\n",
      "Found: user22_adl1.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl1.csv | user22 | Activity 1 → Walking slowly\n",
      "Found: user22_adl10.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl10.csv | user22 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user22_adl11.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl11.csv | user22 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user22_adl12.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl12.csv | user22 | Activity 12 → Swinging hands\n",
      "Found: user22_adl13.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl13.csv | user22 | Activity 13 → Lying on bed\n",
      "Found: user22_adl14.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl14.csv | user22 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user22_adl15.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl15.csv | user22 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user22_adl16.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl16.csv | user22 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user22_adl2.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl2.csv | user22 | Activity 2 → Walking quickly\n",
      "Found: user22_adl3.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl3.csv | user22 | Activity 3 → Jogging\n",
      "Found: user22_adl4.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl4.csv | user22 | Activity 4 → Jumping\n",
      "Found: user22_adl5.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl5.csv | user22 | Activity 5 → Climbing up slowly\n",
      "Found: user22_adl6.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl6.csv | user22 | Activity 6 → Climbing down slowly\n",
      "Found: user22_adl7.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl7.csv | user22 | Activity 7 → Climbing up normally\n",
      "Found: user22_adl8.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl8.csv | user22 | Activity 8 → Climbing down normally\n",
      "Found: user22_adl9.csv\n",
      "Processing project/datasets/adl/adl/user22/user22_adl9.csv | user22 | Activity 9 → Slowly sitting on chair\n",
      "Found: user23_adl1.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl1.csv | user23 | Activity 1 → Walking slowly\n",
      "Found: user23_adl10.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl10.csv | user23 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user23_adl11.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl11.csv | user23 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user23_adl12.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl12.csv | user23 | Activity 12 → Swinging hands\n",
      "Found: user23_adl13.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl13.csv | user23 | Activity 13 → Lying on bed\n",
      "Found: user23_adl14.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl14.csv | user23 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user23_adl15.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl15.csv | user23 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user23_adl16.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl16.csv | user23 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user23_adl2.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl2.csv | user23 | Activity 2 → Walking quickly\n",
      "Found: user23_adl3.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl3.csv | user23 | Activity 3 → Jogging\n",
      "Found: user23_adl4.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl4.csv | user23 | Activity 4 → Jumping\n",
      "Found: user23_adl5.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl5.csv | user23 | Activity 5 → Climbing up slowly\n",
      "Found: user23_adl6.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl6.csv | user23 | Activity 6 → Climbing down slowly\n",
      "Found: user23_adl7.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl7.csv | user23 | Activity 7 → Climbing up normally\n",
      "Found: user23_adl8.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl8.csv | user23 | Activity 8 → Climbing down normally\n",
      "Found: user23_adl9.csv\n",
      "Processing project/datasets/adl/adl/user23/user23_adl9.csv | user23 | Activity 9 → Slowly sitting on chair\n",
      "Found: user24_adl1.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl1.csv | user24 | Activity 1 → Walking slowly\n",
      "Found: user24_adl10.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl10.csv | user24 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user24_adl11.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl11.csv | user24 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user24_adl12.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl12.csv | user24 | Activity 12 → Swinging hands\n",
      "Found: user24_adl13.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl13.csv | user24 | Activity 13 → Lying on bed\n",
      "Found: user24_adl14.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl14.csv | user24 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user24_adl15.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl15.csv | user24 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user24_adl16.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl16.csv | user24 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user24_adl2.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl2.csv | user24 | Activity 2 → Walking quickly\n",
      "Found: user24_adl3.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl3.csv | user24 | Activity 3 → Jogging\n",
      "Found: user24_adl4.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl4.csv | user24 | Activity 4 → Jumping\n",
      "Found: user24_adl5.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl5.csv | user24 | Activity 5 → Climbing up slowly\n",
      "Found: user24_adl6.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl6.csv | user24 | Activity 6 → Climbing down slowly\n",
      "Found: user24_adl7.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl7.csv | user24 | Activity 7 → Climbing up normally\n",
      "Found: user24_adl8.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl8.csv | user24 | Activity 8 → Climbing down normally\n",
      "Found: user24_adl9.csv\n",
      "Processing project/datasets/adl/adl/user24/user24_adl9.csv | user24 | Activity 9 → Slowly sitting on chair\n",
      "Found: user25_adl1.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl1.csv | user25 | Activity 1 → Walking slowly\n",
      "Found: user25_adl10.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl10.csv | user25 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user25_adl11.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl11.csv | user25 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user25_adl12.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl12.csv | user25 | Activity 12 → Swinging hands\n",
      "Found: user25_adl13.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl13.csv | user25 | Activity 13 → Lying on bed\n",
      "Found: user25_adl14.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl14.csv | user25 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user25_adl15.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl15.csv | user25 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user25_adl16.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl16.csv | user25 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user25_adl2.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl2.csv | user25 | Activity 2 → Walking quickly\n",
      "Found: user25_adl3.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl3.csv | user25 | Activity 3 → Jogging\n",
      "Found: user25_adl4.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl4.csv | user25 | Activity 4 → Jumping\n",
      "Found: user25_adl5.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl5.csv | user25 | Activity 5 → Climbing up slowly\n",
      "Found: user25_adl6.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl6.csv | user25 | Activity 6 → Climbing down slowly\n",
      "Found: user25_adl7.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl7.csv | user25 | Activity 7 → Climbing up normally\n",
      "Found: user25_adl8.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl8.csv | user25 | Activity 8 → Climbing down normally\n",
      "Found: user25_adl9.csv\n",
      "Processing project/datasets/adl/adl/user25/user25_adl9.csv | user25 | Activity 9 → Slowly sitting on chair\n",
      "Found: user26_adl1.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl1.csv | user26 | Activity 1 → Walking slowly\n",
      "Found: user26_adl10.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl10.csv | user26 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user26_adl11.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl11.csv | user26 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user26_adl12.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl12.csv | user26 | Activity 12 → Swinging hands\n",
      "Found: user26_adl13.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl13.csv | user26 | Activity 13 → Lying on bed\n",
      "Found: user26_adl14.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl14.csv | user26 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user26_adl15.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl15.csv | user26 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user26_adl16.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl16.csv | user26 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user26_adl2.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl2.csv | user26 | Activity 2 → Walking quickly\n",
      "Found: user26_adl3.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl3.csv | user26 | Activity 3 → Jogging\n",
      "Found: user26_adl4.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl4.csv | user26 | Activity 4 → Jumping\n",
      "Found: user26_adl5.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl5.csv | user26 | Activity 5 → Climbing up slowly\n",
      "Found: user26_adl6.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl6.csv | user26 | Activity 6 → Climbing down slowly\n",
      "Found: user26_adl7.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl7.csv | user26 | Activity 7 → Climbing up normally\n",
      "Found: user26_adl8.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl8.csv | user26 | Activity 8 → Climbing down normally\n",
      "Found: user26_adl9.csv\n",
      "Processing project/datasets/adl/adl/user26/user26_adl9.csv | user26 | Activity 9 → Slowly sitting on chair\n",
      "Found: user27_adl1.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl1.csv | user27 | Activity 1 → Walking slowly\n",
      "Found: user27_adl10.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl10.csv | user27 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user27_adl11.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl11.csv | user27 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user27_adl12.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl12.csv | user27 | Activity 12 → Swinging hands\n",
      "Found: user27_adl13.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl13.csv | user27 | Activity 13 → Lying on bed\n",
      "Found: user27_adl14.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl14.csv | user27 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user27_adl15.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl15.csv | user27 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user27_adl16.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl16.csv | user27 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user27_adl2.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl2.csv | user27 | Activity 2 → Walking quickly\n",
      "Found: user27_adl3.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl3.csv | user27 | Activity 3 → Jogging\n",
      "Found: user27_adl4.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl4.csv | user27 | Activity 4 → Jumping\n",
      "Found: user27_adl5.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl5.csv | user27 | Activity 5 → Climbing up slowly\n",
      "Found: user27_adl6.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl6.csv | user27 | Activity 6 → Climbing down slowly\n",
      "Found: user27_adl7.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl7.csv | user27 | Activity 7 → Climbing up normally\n",
      "Found: user27_adl8.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl8.csv | user27 | Activity 8 → Climbing down normally\n",
      "Found: user27_adl9.csv\n",
      "Processing project/datasets/adl/adl/user27/user27_adl9.csv | user27 | Activity 9 → Slowly sitting on chair\n",
      "Found: user28_adl1.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl1.csv | user28 | Activity 1 → Walking slowly\n",
      "Found: user28_adl10.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl10.csv | user28 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user28_adl11.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl11.csv | user28 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user28_adl12.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl12.csv | user28 | Activity 12 → Swinging hands\n",
      "Found: user28_adl13.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl13.csv | user28 | Activity 13 → Lying on bed\n",
      "Found: user28_adl14.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl14.csv | user28 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user28_adl15.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl15.csv | user28 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user28_adl16.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl16.csv | user28 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user28_adl2.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl2.csv | user28 | Activity 2 → Walking quickly\n",
      "Found: user28_adl3.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl3.csv | user28 | Activity 3 → Jogging\n",
      "Found: user28_adl4.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl4.csv | user28 | Activity 4 → Jumping\n",
      "Found: user28_adl5.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl5.csv | user28 | Activity 5 → Climbing up slowly\n",
      "Found: user28_adl6.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl6.csv | user28 | Activity 6 → Climbing down slowly\n",
      "Found: user28_adl7.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl7.csv | user28 | Activity 7 → Climbing up normally\n",
      "Found: user28_adl8.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl8.csv | user28 | Activity 8 → Climbing down normally\n",
      "Found: user28_adl9.csv\n",
      "Processing project/datasets/adl/adl/user28/user28_adl9.csv | user28 | Activity 9 → Slowly sitting on chair\n",
      "Found: user29_adl1.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl1.csv | user29 | Activity 1 → Walking slowly\n",
      "Found: user29_adl10.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl10.csv | user29 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user29_adl11.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl11.csv | user29 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user29_adl12.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl12.csv | user29 | Activity 12 → Swinging hands\n",
      "Found: user29_adl13.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl13.csv | user29 | Activity 13 → Lying on bed\n",
      "Found: user29_adl14.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl14.csv | user29 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user29_adl15.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl15.csv | user29 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user29_adl16.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl16.csv | user29 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user29_adl2.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl2.csv | user29 | Activity 2 → Walking quickly\n",
      "Found: user29_adl3.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl3.csv | user29 | Activity 3 → Jogging\n",
      "Found: user29_adl4.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl4.csv | user29 | Activity 4 → Jumping\n",
      "Found: user29_adl5.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl5.csv | user29 | Activity 5 → Climbing up slowly\n",
      "Found: user29_adl6.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl6.csv | user29 | Activity 6 → Climbing down slowly\n",
      "Found: user29_adl7.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl7.csv | user29 | Activity 7 → Climbing up normally\n",
      "Found: user29_adl8.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl8.csv | user29 | Activity 8 → Climbing down normally\n",
      "Found: user29_adl9.csv\n",
      "Processing project/datasets/adl/adl/user29/user29_adl9.csv | user29 | Activity 9 → Slowly sitting on chair\n",
      "Found: user3_adl1.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl1.csv | user3 | Activity 1 → Walking slowly\n",
      "Found: user3_adl10.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl10.csv | user3 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user3_adl11.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl11.csv | user3 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user3_adl12.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl12.csv | user3 | Activity 12 → Swinging hands\n",
      "Found: user3_adl13.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl13.csv | user3 | Activity 13 → Lying on bed\n",
      "Found: user3_adl14.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl14.csv | user3 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user3_adl15.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl15.csv | user3 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user3_adl16.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl16.csv | user3 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user3_adl2.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl2.csv | user3 | Activity 2 → Walking quickly\n",
      "Found: user3_adl3.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl3.csv | user3 | Activity 3 → Jogging\n",
      "Found: user3_adl4.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl4.csv | user3 | Activity 4 → Jumping\n",
      "Found: user3_adl5.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl5.csv | user3 | Activity 5 → Climbing up slowly\n",
      "Found: user3_adl6.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl6.csv | user3 | Activity 6 → Climbing down slowly\n",
      "Found: user3_adl7.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl7.csv | user3 | Activity 7 → Climbing up normally\n",
      "Found: user3_adl8.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl8.csv | user3 | Activity 8 → Climbing down normally\n",
      "Found: user3_adl9.csv\n",
      "Processing project/datasets/adl/adl/user3/user3_adl9.csv | user3 | Activity 9 → Slowly sitting on chair\n",
      "Found: user30_adl1.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl1.csv | user30 | Activity 1 → Walking slowly\n",
      "Found: user30_adl10.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl10.csv | user30 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user30_adl11.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl11.csv | user30 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user30_adl12.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl12.csv | user30 | Activity 12 → Swinging hands\n",
      "Found: user30_adl13.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl13.csv | user30 | Activity 13 → Lying on bed\n",
      "Found: user30_adl14.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl14.csv | user30 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user30_adl15.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl15.csv | user30 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user30_adl16.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl16.csv | user30 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user30_adl2.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl2.csv | user30 | Activity 2 → Walking quickly\n",
      "Found: user30_adl3.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl3.csv | user30 | Activity 3 → Jogging\n",
      "Found: user30_adl4.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl4.csv | user30 | Activity 4 → Jumping\n",
      "Found: user30_adl5.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl5.csv | user30 | Activity 5 → Climbing up slowly\n",
      "Found: user30_adl6.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl6.csv | user30 | Activity 6 → Climbing down slowly\n",
      "Found: user30_adl7.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl7.csv | user30 | Activity 7 → Climbing up normally\n",
      "Found: user30_adl8.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl8.csv | user30 | Activity 8 → Climbing down normally\n",
      "Found: user30_adl9.csv\n",
      "Processing project/datasets/adl/adl/user30/user30_adl9.csv | user30 | Activity 9 → Slowly sitting on chair\n",
      "Found: user31_adl1.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl1.csv | user31 | Activity 1 → Walking slowly\n",
      "Found: user31_adl10.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl10.csv | user31 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user31_adl11.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl11.csv | user31 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user31_adl12.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl12.csv | user31 | Activity 12 → Swinging hands\n",
      "Found: user31_adl13.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl13.csv | user31 | Activity 13 → Lying on bed\n",
      "Found: user31_adl14.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl14.csv | user31 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user31_adl15.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl15.csv | user31 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user31_adl16.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl16.csv | user31 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user31_adl2.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl2.csv | user31 | Activity 2 → Walking quickly\n",
      "Found: user31_adl3.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl3.csv | user31 | Activity 3 → Jogging\n",
      "Found: user31_adl4.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl4.csv | user31 | Activity 4 → Jumping\n",
      "Found: user31_adl5.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl5.csv | user31 | Activity 5 → Climbing up slowly\n",
      "Found: user31_adl6.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl6.csv | user31 | Activity 6 → Climbing down slowly\n",
      "Found: user31_adl7.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl7.csv | user31 | Activity 7 → Climbing up normally\n",
      "Found: user31_adl8.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl8.csv | user31 | Activity 8 → Climbing down normally\n",
      "Found: user31_adl9.csv\n",
      "Processing project/datasets/adl/adl/user31/user31_adl9.csv | user31 | Activity 9 → Slowly sitting on chair\n",
      "Found: user32_adl1.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl1.csv | user32 | Activity 1 → Walking slowly\n",
      "Found: user32_adl10.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl10.csv | user32 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user32_adl11.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl11.csv | user32 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user32_adl12.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl12.csv | user32 | Activity 12 → Swinging hands\n",
      "Found: user32_adl13.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl13.csv | user32 | Activity 13 → Lying on bed\n",
      "Found: user32_adl14.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl14.csv | user32 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user32_adl15.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl15.csv | user32 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user32_adl16.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl16.csv | user32 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user32_adl2.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl2.csv | user32 | Activity 2 → Walking quickly\n",
      "Found: user32_adl3.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl3.csv | user32 | Activity 3 → Jogging\n",
      "Found: user32_adl4.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl4.csv | user32 | Activity 4 → Jumping\n",
      "Found: user32_adl5.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl5.csv | user32 | Activity 5 → Climbing up slowly\n",
      "Found: user32_adl6.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl6.csv | user32 | Activity 6 → Climbing down slowly\n",
      "Found: user32_adl7.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl7.csv | user32 | Activity 7 → Climbing up normally\n",
      "Found: user32_adl8.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl8.csv | user32 | Activity 8 → Climbing down normally\n",
      "Found: user32_adl9.csv\n",
      "Processing project/datasets/adl/adl/user32/user32_adl9.csv | user32 | Activity 9 → Slowly sitting on chair\n",
      "Found: user33_adl1.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl1.csv | user33 | Activity 1 → Walking slowly\n",
      "Found: user33_adl10.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl10.csv | user33 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user33_adl11.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl11.csv | user33 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user33_adl12.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl12.csv | user33 | Activity 12 → Swinging hands\n",
      "Found: user33_adl13.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl13.csv | user33 | Activity 13 → Lying on bed\n",
      "Found: user33_adl14.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl14.csv | user33 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user33_adl15.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl15.csv | user33 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user33_adl16.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl16.csv | user33 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user33_adl2.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl2.csv | user33 | Activity 2 → Walking quickly\n",
      "Found: user33_adl3.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl3.csv | user33 | Activity 3 → Jogging\n",
      "Found: user33_adl4.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl4.csv | user33 | Activity 4 → Jumping\n",
      "Found: user33_adl5.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl5.csv | user33 | Activity 5 → Climbing up slowly\n",
      "Found: user33_adl6.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl6.csv | user33 | Activity 6 → Climbing down slowly\n",
      "Found: user33_adl7.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl7.csv | user33 | Activity 7 → Climbing up normally\n",
      "Found: user33_adl8.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl8.csv | user33 | Activity 8 → Climbing down normally\n",
      "Found: user33_adl9.csv\n",
      "Processing project/datasets/adl/adl/user33/user33_adl9.csv | user33 | Activity 9 → Slowly sitting on chair\n",
      "Found: user34_adl1.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl1.csv | user34 | Activity 1 → Walking slowly\n",
      "Found: user34_adl10.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl10.csv | user34 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user34_adl11.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl11.csv | user34 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user34_adl12.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl12.csv | user34 | Activity 12 → Swinging hands\n",
      "Found: user34_adl13.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl13.csv | user34 | Activity 13 → Lying on bed\n",
      "Found: user34_adl14.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl14.csv | user34 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user34_adl15.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl15.csv | user34 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user34_adl16.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl16.csv | user34 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user34_adl2.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl2.csv | user34 | Activity 2 → Walking quickly\n",
      "Found: user34_adl3.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl3.csv | user34 | Activity 3 → Jogging\n",
      "Found: user34_adl4.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl4.csv | user34 | Activity 4 → Jumping\n",
      "Found: user34_adl5.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl5.csv | user34 | Activity 5 → Climbing up slowly\n",
      "Found: user34_adl6.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl6.csv | user34 | Activity 6 → Climbing down slowly\n",
      "Found: user34_adl7.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl7.csv | user34 | Activity 7 → Climbing up normally\n",
      "Found: user34_adl8.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl8.csv | user34 | Activity 8 → Climbing down normally\n",
      "Found: user34_adl9.csv\n",
      "Processing project/datasets/adl/adl/user34/user34_adl9.csv | user34 | Activity 9 → Slowly sitting on chair\n",
      "Found: user35_adl1.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl1.csv | user35 | Activity 1 → Walking slowly\n",
      "Found: user35_adl10.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl10.csv | user35 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user35_adl11.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl11.csv | user35 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user35_adl12.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl12.csv | user35 | Activity 12 → Swinging hands\n",
      "Found: user35_adl13.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl13.csv | user35 | Activity 13 → Lying on bed\n",
      "Found: user35_adl14.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl14.csv | user35 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user35_adl15.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl15.csv | user35 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user35_adl16.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl16.csv | user35 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user35_adl2.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl2.csv | user35 | Activity 2 → Walking quickly\n",
      "Found: user35_adl3.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl3.csv | user35 | Activity 3 → Jogging\n",
      "Found: user35_adl4.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl4.csv | user35 | Activity 4 → Jumping\n",
      "Found: user35_adl5.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl5.csv | user35 | Activity 5 → Climbing up slowly\n",
      "Found: user35_adl6.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl6.csv | user35 | Activity 6 → Climbing down slowly\n",
      "Found: user35_adl7.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl7.csv | user35 | Activity 7 → Climbing up normally\n",
      "Found: user35_adl8.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl8.csv | user35 | Activity 8 → Climbing down normally\n",
      "Found: user35_adl9.csv\n",
      "Processing project/datasets/adl/adl/user35/user35_adl9.csv | user35 | Activity 9 → Slowly sitting on chair\n",
      "Found: user36_adl1.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl1.csv | user36 | Activity 1 → Walking slowly\n",
      "Found: user36_adl10.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl10.csv | user36 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user36_adl11.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl11.csv | user36 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user36_adl12.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl12.csv | user36 | Activity 12 → Swinging hands\n",
      "Found: user36_adl13.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl13.csv | user36 | Activity 13 → Lying on bed\n",
      "Found: user36_adl14.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl14.csv | user36 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user36_adl15.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl15.csv | user36 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user36_adl16.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl16.csv | user36 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user36_adl2.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl2.csv | user36 | Activity 2 → Walking quickly\n",
      "Found: user36_adl3.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl3.csv | user36 | Activity 3 → Jogging\n",
      "Found: user36_adl4.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl4.csv | user36 | Activity 4 → Jumping\n",
      "Found: user36_adl5.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl5.csv | user36 | Activity 5 → Climbing up slowly\n",
      "Found: user36_adl6.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl6.csv | user36 | Activity 6 → Climbing down slowly\n",
      "Found: user36_adl7.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl7.csv | user36 | Activity 7 → Climbing up normally\n",
      "Found: user36_adl8.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl8.csv | user36 | Activity 8 → Climbing down normally\n",
      "Found: user36_adl9.csv\n",
      "Processing project/datasets/adl/adl/user36/user36_adl9.csv | user36 | Activity 9 → Slowly sitting on chair\n",
      "Found: user37_adl1.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl1.csv | user37 | Activity 1 → Walking slowly\n",
      "Found: user37_adl10.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl10.csv | user37 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user37_adl11.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl11.csv | user37 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user37_adl12.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl12.csv | user37 | Activity 12 → Swinging hands\n",
      "Found: user37_adl13.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl13.csv | user37 | Activity 13 → Lying on bed\n",
      "Found: user37_adl14.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl14.csv | user37 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user37_adl15.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl15.csv | user37 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user37_adl16.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl16.csv | user37 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user37_adl2.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl2.csv | user37 | Activity 2 → Walking quickly\n",
      "Found: user37_adl3.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl3.csv | user37 | Activity 3 → Jogging\n",
      "Found: user37_adl4.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl4.csv | user37 | Activity 4 → Jumping\n",
      "Found: user37_adl5.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl5.csv | user37 | Activity 5 → Climbing up slowly\n",
      "Found: user37_adl6.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl6.csv | user37 | Activity 6 → Climbing down slowly\n",
      "Found: user37_adl7.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl7.csv | user37 | Activity 7 → Climbing up normally\n",
      "Found: user37_adl8.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl8.csv | user37 | Activity 8 → Climbing down normally\n",
      "Found: user37_adl9.csv\n",
      "Processing project/datasets/adl/adl/user37/user37_adl9.csv | user37 | Activity 9 → Slowly sitting on chair\n",
      "Found: user38_adl1.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl1.csv | user38 | Activity 1 → Walking slowly\n",
      "Found: user38_adl10.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl10.csv | user38 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user38_adl11.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl11.csv | user38 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user38_adl12.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl12.csv | user38 | Activity 12 → Swinging hands\n",
      "Found: user38_adl13.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl13.csv | user38 | Activity 13 → Lying on bed\n",
      "Found: user38_adl14.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl14.csv | user38 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user38_adl15.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl15.csv | user38 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user38_adl16.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl16.csv | user38 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user38_adl2.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl2.csv | user38 | Activity 2 → Walking quickly\n",
      "Found: user38_adl3.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl3.csv | user38 | Activity 3 → Jogging\n",
      "Found: user38_adl4.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl4.csv | user38 | Activity 4 → Jumping\n",
      "Found: user38_adl5.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl5.csv | user38 | Activity 5 → Climbing up slowly\n",
      "Found: user38_adl6.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl6.csv | user38 | Activity 6 → Climbing down slowly\n",
      "Found: user38_adl7.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl7.csv | user38 | Activity 7 → Climbing up normally\n",
      "Found: user38_adl8.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl8.csv | user38 | Activity 8 → Climbing down normally\n",
      "Found: user38_adl9.csv\n",
      "Processing project/datasets/adl/adl/user38/user38_adl9.csv | user38 | Activity 9 → Slowly sitting on chair\n",
      "Found: user39_adl1.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl1.csv | user39 | Activity 1 → Walking slowly\n",
      "Found: user39_adl10.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl10.csv | user39 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user39_adl11.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl11.csv | user39 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user39_adl12.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl12.csv | user39 | Activity 12 → Swinging hands\n",
      "Found: user39_adl13.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl13.csv | user39 | Activity 13 → Lying on bed\n",
      "Found: user39_adl14.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl14.csv | user39 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user39_adl15.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl15.csv | user39 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user39_adl16.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl16.csv | user39 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user39_adl2.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl2.csv | user39 | Activity 2 → Walking quickly\n",
      "Found: user39_adl3.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl3.csv | user39 | Activity 3 → Jogging\n",
      "Found: user39_adl4.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl4.csv | user39 | Activity 4 → Jumping\n",
      "Found: user39_adl5.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl5.csv | user39 | Activity 5 → Climbing up slowly\n",
      "Found: user39_adl6.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl6.csv | user39 | Activity 6 → Climbing down slowly\n",
      "Found: user39_adl7.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl7.csv | user39 | Activity 7 → Climbing up normally\n",
      "Found: user39_adl8.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl8.csv | user39 | Activity 8 → Climbing down normally\n",
      "Found: user39_adl9.csv\n",
      "Processing project/datasets/adl/adl/user39/user39_adl9.csv | user39 | Activity 9 → Slowly sitting on chair\n",
      "Found: user4_adl1.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl1.csv | user4 | Activity 1 → Walking slowly\n",
      "Found: user4_adl10.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl10.csv | user4 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user4_adl11.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl11.csv | user4 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user4_adl12.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl12.csv | user4 | Activity 12 → Swinging hands\n",
      "Found: user4_adl13.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl13.csv | user4 | Activity 13 → Lying on bed\n",
      "Found: user4_adl14.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl14.csv | user4 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user4_adl15.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl15.csv | user4 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user4_adl16.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl16.csv | user4 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user4_adl2.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl2.csv | user4 | Activity 2 → Walking quickly\n",
      "Found: user4_adl3.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl3.csv | user4 | Activity 3 → Jogging\n",
      "Found: user4_adl4.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl4.csv | user4 | Activity 4 → Jumping\n",
      "Found: user4_adl5.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl5.csv | user4 | Activity 5 → Climbing up slowly\n",
      "Found: user4_adl6.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl6.csv | user4 | Activity 6 → Climbing down slowly\n",
      "Found: user4_adl7.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl7.csv | user4 | Activity 7 → Climbing up normally\n",
      "Found: user4_adl8.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl8.csv | user4 | Activity 8 → Climbing down normally\n",
      "Found: user4_adl9.csv\n",
      "Processing project/datasets/adl/adl/user4/user4_adl9.csv | user4 | Activity 9 → Slowly sitting on chair\n",
      "Found: user40_adl1.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl1.csv | user40 | Activity 1 → Walking slowly\n",
      "Found: user40_adl10.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl10.csv | user40 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user40_adl11.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl11.csv | user40 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user40_adl12.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl12.csv | user40 | Activity 12 → Swinging hands\n",
      "Found: user40_adl13.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl13.csv | user40 | Activity 13 → Lying on bed\n",
      "Found: user40_adl14.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl14.csv | user40 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user40_adl15.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl15.csv | user40 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user40_adl16.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl16.csv | user40 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user40_adl2.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl2.csv | user40 | Activity 2 → Walking quickly\n",
      "Found: user40_adl3.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl3.csv | user40 | Activity 3 → Jogging\n",
      "Found: user40_adl4.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl4.csv | user40 | Activity 4 → Jumping\n",
      "Found: user40_adl5.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl5.csv | user40 | Activity 5 → Climbing up slowly\n",
      "Found: user40_adl6.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl6.csv | user40 | Activity 6 → Climbing down slowly\n",
      "Found: user40_adl7.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl7.csv | user40 | Activity 7 → Climbing up normally\n",
      "Found: user40_adl8.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl8.csv | user40 | Activity 8 → Climbing down normally\n",
      "Found: user40_adl9.csv\n",
      "Processing project/datasets/adl/adl/user40/user40_adl9.csv | user40 | Activity 9 → Slowly sitting on chair\n",
      "Found: user41_adl1.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl1.csv | user41 | Activity 1 → Walking slowly\n",
      "Found: user41_adl10.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl10.csv | user41 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user41_adl11.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl11.csv | user41 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user41_adl12.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl12.csv | user41 | Activity 12 → Swinging hands\n",
      "Found: user41_adl13.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl13.csv | user41 | Activity 13 → Lying on bed\n",
      "Found: user41_adl14.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl14.csv | user41 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user41_adl15.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl15.csv | user41 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user41_adl16.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl16.csv | user41 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user41_adl2.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl2.csv | user41 | Activity 2 → Walking quickly\n",
      "Found: user41_adl3.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl3.csv | user41 | Activity 3 → Jogging\n",
      "Found: user41_adl4.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl4.csv | user41 | Activity 4 → Jumping\n",
      "Found: user41_adl5.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl5.csv | user41 | Activity 5 → Climbing up slowly\n",
      "Found: user41_adl6.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl6.csv | user41 | Activity 6 → Climbing down slowly\n",
      "Found: user41_adl7.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl7.csv | user41 | Activity 7 → Climbing up normally\n",
      "Found: user41_adl8.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl8.csv | user41 | Activity 8 → Climbing down normally\n",
      "Found: user41_adl9.csv\n",
      "Processing project/datasets/adl/adl/user41/user41_adl9.csv | user41 | Activity 9 → Slowly sitting on chair\n",
      "Found: user5_adl1.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl1.csv | user5 | Activity 1 → Walking slowly\n",
      "Found: user5_adl10.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl10.csv | user5 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user5_adl11.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl11.csv | user5 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user5_adl12.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl12.csv | user5 | Activity 12 → Swinging hands\n",
      "Found: user5_adl13.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl13.csv | user5 | Activity 13 → Lying on bed\n",
      "Found: user5_adl14.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl14.csv | user5 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user5_adl15.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl15.csv | user5 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user5_adl16.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl16.csv | user5 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user5_adl2.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl2.csv | user5 | Activity 2 → Walking quickly\n",
      "Found: user5_adl3.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl3.csv | user5 | Activity 3 → Jogging\n",
      "Found: user5_adl4.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl4.csv | user5 | Activity 4 → Jumping\n",
      "Found: user5_adl5.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl5.csv | user5 | Activity 5 → Climbing up slowly\n",
      "Found: user5_adl6.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl6.csv | user5 | Activity 6 → Climbing down slowly\n",
      "Found: user5_adl7.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl7.csv | user5 | Activity 7 → Climbing up normally\n",
      "Found: user5_adl8.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl8.csv | user5 | Activity 8 → Climbing down normally\n",
      "Found: user5_adl9.csv\n",
      "Processing project/datasets/adl/adl/user5/user5_adl9.csv | user5 | Activity 9 → Slowly sitting on chair\n",
      "Found: user6_adl1.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl1.csv | user6 | Activity 1 → Walking slowly\n",
      "Found: user6_adl10.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl10.csv | user6 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user6_adl11.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl11.csv | user6 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user6_adl12.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl12.csv | user6 | Activity 12 → Swinging hands\n",
      "Found: user6_adl13.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl13.csv | user6 | Activity 13 → Lying on bed\n",
      "Found: user6_adl14.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl14.csv | user6 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user6_adl15.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl15.csv | user6 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user6_adl16.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl16.csv | user6 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user6_adl2.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl2.csv | user6 | Activity 2 → Walking quickly\n",
      "Found: user6_adl3.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl3.csv | user6 | Activity 3 → Jogging\n",
      "Found: user6_adl4.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl4.csv | user6 | Activity 4 → Jumping\n",
      "Found: user6_adl5.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl5.csv | user6 | Activity 5 → Climbing up slowly\n",
      "Found: user6_adl6.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl6.csv | user6 | Activity 6 → Climbing down slowly\n",
      "Found: user6_adl7.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl7.csv | user6 | Activity 7 → Climbing up normally\n",
      "Found: user6_adl8.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl8.csv | user6 | Activity 8 → Climbing down normally\n",
      "Found: user6_adl9.csv\n",
      "Processing project/datasets/adl/adl/user6/user6_adl9.csv | user6 | Activity 9 → Slowly sitting on chair\n",
      "Found: user7_adl1.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl1.csv | user7 | Activity 1 → Walking slowly\n",
      "Found: user7_adl10.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl10.csv | user7 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user7_adl11.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl11.csv | user7 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user7_adl12.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl12.csv | user7 | Activity 12 → Swinging hands\n",
      "Found: user7_adl13.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl13.csv | user7 | Activity 13 → Lying on bed\n",
      "Found: user7_adl14.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl14.csv | user7 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user7_adl15.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl15.csv | user7 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user7_adl16.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl16.csv | user7 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user7_adl2.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl2.csv | user7 | Activity 2 → Walking quickly\n",
      "Found: user7_adl3.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl3.csv | user7 | Activity 3 → Jogging\n",
      "Found: user7_adl4.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl4.csv | user7 | Activity 4 → Jumping\n",
      "Found: user7_adl5.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl5.csv | user7 | Activity 5 → Climbing up slowly\n",
      "Found: user7_adl6.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl6.csv | user7 | Activity 6 → Climbing down slowly\n",
      "Found: user7_adl7.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl7.csv | user7 | Activity 7 → Climbing up normally\n",
      "Found: user7_adl8.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl8.csv | user7 | Activity 8 → Climbing down normally\n",
      "Found: user7_adl9.csv\n",
      "Processing project/datasets/adl/adl/user7/user7_adl9.csv | user7 | Activity 9 → Slowly sitting on chair\n",
      "Found: user8_adl1.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl1.csv | user8 | Activity 1 → Walking slowly\n",
      "Found: user8_adl10.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl10.csv | user8 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user8_adl11.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl11.csv | user8 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user8_adl12.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl12.csv | user8 | Activity 12 → Swinging hands\n",
      "Found: user8_adl13.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl13.csv | user8 | Activity 13 → Lying on bed\n",
      "Found: user8_adl14.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl14.csv | user8 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user8_adl15.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl15.csv | user8 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user8_adl16.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl16.csv | user8 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user8_adl2.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl2.csv | user8 | Activity 2 → Walking quickly\n",
      "Found: user8_adl3.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl3.csv | user8 | Activity 3 → Jogging\n",
      "Found: user8_adl4.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl4.csv | user8 | Activity 4 → Jumping\n",
      "Found: user8_adl5.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl5.csv | user8 | Activity 5 → Climbing up slowly\n",
      "Found: user8_adl6.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl6.csv | user8 | Activity 6 → Climbing down slowly\n",
      "Found: user8_adl7.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl7.csv | user8 | Activity 7 → Climbing up normally\n",
      "Found: user8_adl8.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl8.csv | user8 | Activity 8 → Climbing down normally\n",
      "Found: user8_adl9.csv\n",
      "Processing project/datasets/adl/adl/user8/user8_adl9.csv | user8 | Activity 9 → Slowly sitting on chair\n",
      "Found: user9_adl1.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl1.csv | user9 | Activity 1 → Walking slowly\n",
      "Found: user9_adl10.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl10.csv | user9 | Activity 10 → Rapidly sitting on chair\n",
      "Found: user9_adl11.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl11.csv | user9 | Activity 11 → Nearly sitting on chair and getting up\n",
      "Found: user9_adl12.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl12.csv | user9 | Activity 12 → Swinging hands\n",
      "Found: user9_adl13.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl13.csv | user9 | Activity 13 → Lying on bed\n",
      "Found: user9_adl14.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl14.csv | user9 | Activity 14 → Lying on back and getting up slowly\n",
      "Found: user9_adl15.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl15.csv | user9 | Activity 15 → Lying on back and getting up normally\n",
      "Found: user9_adl16.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl16.csv | user9 | Activity 16 → Transition form sideways to ones back while lying\n",
      "Found: user9_adl2.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl2.csv | user9 | Activity 2 → Walking quickly\n",
      "Found: user9_adl3.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl3.csv | user9 | Activity 3 → Jogging\n",
      "Found: user9_adl4.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl4.csv | user9 | Activity 4 → Jumping\n",
      "Found: user9_adl5.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl5.csv | user9 | Activity 5 → Climbing up slowly\n",
      "Found: user9_adl6.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl6.csv | user9 | Activity 6 → Climbing down slowly\n",
      "Found: user9_adl7.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl7.csv | user9 | Activity 7 → Climbing up normally\n",
      "Found: user9_adl8.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl8.csv | user9 | Activity 8 → Climbing down normally\n",
      "Found: user9_adl9.csv\n",
      "Processing project/datasets/adl/adl/user9/user9_adl9.csv | user9 | Activity 9 → Slowly sitting on chair\n",
      "Found: combined_ADLfalls_engineered.csv\n",
      "Skipping combined_ADLfalls_engineered.csv — invalid format\n",
      "Found: user1_fall1.csv\n",
      "Processing project/datasets/adl/fall/user1/user1_fall1.csv | user1 | Activity 1 → Forward fall landing on knee\n",
      "Found: user1_fall2.csv\n",
      "Processing project/datasets/adl/fall/user1/user1_fall2.csv | user1 | Activity 2 → Right fall\n",
      "Found: user1_fall3.csv\n",
      "Processing project/datasets/adl/fall/user1/user1_fall3.csv | user1 | Activity 3 → Left fall\n",
      "Found: user1_fall4.csv\n",
      "Processing project/datasets/adl/fall/user1/user1_fall4.csv | user1 | Activity 4 → Forward fall\n",
      "Found: user1_fall5.csv\n",
      "Processing project/datasets/adl/fall/user1/user1_fall5.csv | user1 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user1_fall6.csv\n",
      "Processing project/datasets/adl/fall/user1/user1_fall6.csv | user1 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user1_fall7.csv\n",
      "Processing project/datasets/adl/fall/user1/user1_fall7.csv | user1 | Activity 7 → Backward fall from seated position\n",
      "Found: user1_fall8.csv\n",
      "Processing project/datasets/adl/fall/user1/user1_fall8.csv | user1 | Activity 8 → Grabbing while falling\n",
      "Found: user10_fall1.csv\n",
      "Processing project/datasets/adl/fall/user10/user10_fall1.csv | user10 | Activity 1 → Forward fall landing on knee\n",
      "Found: user10_fall2.csv\n",
      "Processing project/datasets/adl/fall/user10/user10_fall2.csv | user10 | Activity 2 → Right fall\n",
      "Found: user10_fall3.csv\n",
      "Processing project/datasets/adl/fall/user10/user10_fall3.csv | user10 | Activity 3 → Left fall\n",
      "Found: user10_fall4.csv\n",
      "Processing project/datasets/adl/fall/user10/user10_fall4.csv | user10 | Activity 4 → Forward fall\n",
      "Found: user10_fall5.csv\n",
      "Processing project/datasets/adl/fall/user10/user10_fall5.csv | user10 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user10_fall6.csv\n",
      "Processing project/datasets/adl/fall/user10/user10_fall6.csv | user10 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user10_fall7.csv\n",
      "Processing project/datasets/adl/fall/user10/user10_fall7.csv | user10 | Activity 7 → Backward fall from seated position\n",
      "Found: user10_fall8.csv\n",
      "Processing project/datasets/adl/fall/user10/user10_fall8.csv | user10 | Activity 8 → Grabbing while falling\n",
      "Found: user11_fall1.csv\n",
      "Processing project/datasets/adl/fall/user11/user11_fall1.csv | user11 | Activity 1 → Forward fall landing on knee\n",
      "Found: user11_fall2.csv\n",
      "Processing project/datasets/adl/fall/user11/user11_fall2.csv | user11 | Activity 2 → Right fall\n",
      "Found: user11_fall3.csv\n",
      "Processing project/datasets/adl/fall/user11/user11_fall3.csv | user11 | Activity 3 → Left fall\n",
      "Found: user11_fall4.csv\n",
      "Processing project/datasets/adl/fall/user11/user11_fall4.csv | user11 | Activity 4 → Forward fall\n",
      "Found: user11_fall5.csv\n",
      "Processing project/datasets/adl/fall/user11/user11_fall5.csv | user11 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user11_fall6.csv\n",
      "Processing project/datasets/adl/fall/user11/user11_fall6.csv | user11 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user11_fall7.csv\n",
      "Processing project/datasets/adl/fall/user11/user11_fall7.csv | user11 | Activity 7 → Backward fall from seated position\n",
      "Found: user11_fall8.csv\n",
      "Processing project/datasets/adl/fall/user11/user11_fall8.csv | user11 | Activity 8 → Grabbing while falling\n",
      "Found: user12_fall1.csv\n",
      "Processing project/datasets/adl/fall/user12/user12_fall1.csv | user12 | Activity 1 → Forward fall landing on knee\n",
      "Found: user12_fall2.csv\n",
      "Processing project/datasets/adl/fall/user12/user12_fall2.csv | user12 | Activity 2 → Right fall\n",
      "Found: user12_fall3.csv\n",
      "Processing project/datasets/adl/fall/user12/user12_fall3.csv | user12 | Activity 3 → Left fall\n",
      "Found: user12_fall4.csv\n",
      "Processing project/datasets/adl/fall/user12/user12_fall4.csv | user12 | Activity 4 → Forward fall\n",
      "Found: user12_fall5.csv\n",
      "Processing project/datasets/adl/fall/user12/user12_fall5.csv | user12 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user12_fall6.csv\n",
      "Processing project/datasets/adl/fall/user12/user12_fall6.csv | user12 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user12_fall7.csv\n",
      "Processing project/datasets/adl/fall/user12/user12_fall7.csv | user12 | Activity 7 → Backward fall from seated position\n",
      "Found: user12_fall8.csv\n",
      "Processing project/datasets/adl/fall/user12/user12_fall8.csv | user12 | Activity 8 → Grabbing while falling\n",
      "Found: user13_fall1.csv\n",
      "Processing project/datasets/adl/fall/user13/user13_fall1.csv | user13 | Activity 1 → Forward fall landing on knee\n",
      "Found: user13_fall2.csv\n",
      "Processing project/datasets/adl/fall/user13/user13_fall2.csv | user13 | Activity 2 → Right fall\n",
      "Found: user13_fall3.csv\n",
      "Processing project/datasets/adl/fall/user13/user13_fall3.csv | user13 | Activity 3 → Left fall\n",
      "Found: user13_fall4.csv\n",
      "Processing project/datasets/adl/fall/user13/user13_fall4.csv | user13 | Activity 4 → Forward fall\n",
      "Found: user13_fall5.csv\n",
      "Processing project/datasets/adl/fall/user13/user13_fall5.csv | user13 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user13_fall6.csv\n",
      "Processing project/datasets/adl/fall/user13/user13_fall6.csv | user13 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user13_fall7.csv\n",
      "Processing project/datasets/adl/fall/user13/user13_fall7.csv | user13 | Activity 7 → Backward fall from seated position\n",
      "Found: user13_fall8.csv\n",
      "Processing project/datasets/adl/fall/user13/user13_fall8.csv | user13 | Activity 8 → Grabbing while falling\n",
      "Found: user14_fall1.csv\n",
      "Processing project/datasets/adl/fall/user14/user14_fall1.csv | user14 | Activity 1 → Forward fall landing on knee\n",
      "Found: user14_fall2.csv\n",
      "Processing project/datasets/adl/fall/user14/user14_fall2.csv | user14 | Activity 2 → Right fall\n",
      "Found: user14_fall3.csv\n",
      "Processing project/datasets/adl/fall/user14/user14_fall3.csv | user14 | Activity 3 → Left fall\n",
      "Found: user14_fall4.csv\n",
      "Processing project/datasets/adl/fall/user14/user14_fall4.csv | user14 | Activity 4 → Forward fall\n",
      "Found: user14_fall5.csv\n",
      "Processing project/datasets/adl/fall/user14/user14_fall5.csv | user14 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user14_fall6.csv\n",
      "Processing project/datasets/adl/fall/user14/user14_fall6.csv | user14 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user14_fall7.csv\n",
      "Processing project/datasets/adl/fall/user14/user14_fall7.csv | user14 | Activity 7 → Backward fall from seated position\n",
      "Found: user14_fall8.csv\n",
      "Processing project/datasets/adl/fall/user14/user14_fall8.csv | user14 | Activity 8 → Grabbing while falling\n",
      "Found: user15_fall1.csv\n",
      "Processing project/datasets/adl/fall/user15/user15_fall1.csv | user15 | Activity 1 → Forward fall landing on knee\n",
      "Found: user15_fall2.csv\n",
      "Processing project/datasets/adl/fall/user15/user15_fall2.csv | user15 | Activity 2 → Right fall\n",
      "Found: user15_fall3.csv\n",
      "Processing project/datasets/adl/fall/user15/user15_fall3.csv | user15 | Activity 3 → Left fall\n",
      "Found: user15_fall4.csv\n",
      "Processing project/datasets/adl/fall/user15/user15_fall4.csv | user15 | Activity 4 → Forward fall\n",
      "Found: user15_fall5.csv\n",
      "Processing project/datasets/adl/fall/user15/user15_fall5.csv | user15 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user15_fall6.csv\n",
      "Processing project/datasets/adl/fall/user15/user15_fall6.csv | user15 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user15_fall7.csv\n",
      "Processing project/datasets/adl/fall/user15/user15_fall7.csv | user15 | Activity 7 → Backward fall from seated position\n",
      "Found: user15_fall8.csv\n",
      "Processing project/datasets/adl/fall/user15/user15_fall8.csv | user15 | Activity 8 → Grabbing while falling\n",
      "Found: user16_fall1.csv\n",
      "Processing project/datasets/adl/fall/user16/user16_fall1.csv | user16 | Activity 1 → Forward fall landing on knee\n",
      "Found: user16_fall2.csv\n",
      "Processing project/datasets/adl/fall/user16/user16_fall2.csv | user16 | Activity 2 → Right fall\n",
      "Found: user16_fall3.csv\n",
      "Processing project/datasets/adl/fall/user16/user16_fall3.csv | user16 | Activity 3 → Left fall\n",
      "Found: user16_fall4.csv\n",
      "Processing project/datasets/adl/fall/user16/user16_fall4.csv | user16 | Activity 4 → Forward fall\n",
      "Found: user16_fall5.csv\n",
      "Processing project/datasets/adl/fall/user16/user16_fall5.csv | user16 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user16_fall6.csv\n",
      "Processing project/datasets/adl/fall/user16/user16_fall6.csv | user16 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user16_fall7.csv\n",
      "Processing project/datasets/adl/fall/user16/user16_fall7.csv | user16 | Activity 7 → Backward fall from seated position\n",
      "Found: user16_fall8.csv\n",
      "Processing project/datasets/adl/fall/user16/user16_fall8.csv | user16 | Activity 8 → Grabbing while falling\n",
      "Found: user17_fall1.csv\n",
      "Processing project/datasets/adl/fall/user17/user17_fall1.csv | user17 | Activity 1 → Forward fall landing on knee\n",
      "Found: user17_fall2.csv\n",
      "Processing project/datasets/adl/fall/user17/user17_fall2.csv | user17 | Activity 2 → Right fall\n",
      "Found: user17_fall3.csv\n",
      "Processing project/datasets/adl/fall/user17/user17_fall3.csv | user17 | Activity 3 → Left fall\n",
      "Found: user17_fall4.csv\n",
      "Processing project/datasets/adl/fall/user17/user17_fall4.csv | user17 | Activity 4 → Forward fall\n",
      "Found: user17_fall5.csv\n",
      "Processing project/datasets/adl/fall/user17/user17_fall5.csv | user17 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user17_fall6.csv\n",
      "Processing project/datasets/adl/fall/user17/user17_fall6.csv | user17 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user17_fall7.csv\n",
      "Processing project/datasets/adl/fall/user17/user17_fall7.csv | user17 | Activity 7 → Backward fall from seated position\n",
      "Found: user17_fall8.csv\n",
      "Processing project/datasets/adl/fall/user17/user17_fall8.csv | user17 | Activity 8 → Grabbing while falling\n",
      "Found: user18_fall1.csv\n",
      "Processing project/datasets/adl/fall/user18/user18_fall1.csv | user18 | Activity 1 → Forward fall landing on knee\n",
      "Found: user18_fall2.csv\n",
      "Processing project/datasets/adl/fall/user18/user18_fall2.csv | user18 | Activity 2 → Right fall\n",
      "Found: user18_fall3.csv\n",
      "Processing project/datasets/adl/fall/user18/user18_fall3.csv | user18 | Activity 3 → Left fall\n",
      "Found: user18_fall4.csv\n",
      "Processing project/datasets/adl/fall/user18/user18_fall4.csv | user18 | Activity 4 → Forward fall\n",
      "Found: user18_fall5.csv\n",
      "Processing project/datasets/adl/fall/user18/user18_fall5.csv | user18 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user18_fall6.csv\n",
      "Processing project/datasets/adl/fall/user18/user18_fall6.csv | user18 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user18_fall7.csv\n",
      "Processing project/datasets/adl/fall/user18/user18_fall7.csv | user18 | Activity 7 → Backward fall from seated position\n",
      "Found: user18_fall8.csv\n",
      "Processing project/datasets/adl/fall/user18/user18_fall8.csv | user18 | Activity 8 → Grabbing while falling\n",
      "Found: user19_fall1.csv\n",
      "Processing project/datasets/adl/fall/user19/user19_fall1.csv | user19 | Activity 1 → Forward fall landing on knee\n",
      "Found: user19_fall2.csv\n",
      "Processing project/datasets/adl/fall/user19/user19_fall2.csv | user19 | Activity 2 → Right fall\n",
      "Found: user19_fall3.csv\n",
      "Processing project/datasets/adl/fall/user19/user19_fall3.csv | user19 | Activity 3 → Left fall\n",
      "Found: user19_fall4.csv\n",
      "Processing project/datasets/adl/fall/user19/user19_fall4.csv | user19 | Activity 4 → Forward fall\n",
      "Found: user19_fall5.csv\n",
      "Processing project/datasets/adl/fall/user19/user19_fall5.csv | user19 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user19_fall6.csv\n",
      "Processing project/datasets/adl/fall/user19/user19_fall6.csv | user19 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user19_fall7.csv\n",
      "Processing project/datasets/adl/fall/user19/user19_fall7.csv | user19 | Activity 7 → Backward fall from seated position\n",
      "Found: user19_fall8.csv\n",
      "Processing project/datasets/adl/fall/user19/user19_fall8.csv | user19 | Activity 8 → Grabbing while falling\n",
      "Found: user2_fall1.csv\n",
      "Processing project/datasets/adl/fall/user2/user2_fall1.csv | user2 | Activity 1 → Forward fall landing on knee\n",
      "Found: user2_fall2.csv\n",
      "Processing project/datasets/adl/fall/user2/user2_fall2.csv | user2 | Activity 2 → Right fall\n",
      "Found: user2_fall3.csv\n",
      "Processing project/datasets/adl/fall/user2/user2_fall3.csv | user2 | Activity 3 → Left fall\n",
      "Found: user2_fall4.csv\n",
      "Processing project/datasets/adl/fall/user2/user2_fall4.csv | user2 | Activity 4 → Forward fall\n",
      "Found: user2_fall5.csv\n",
      "Processing project/datasets/adl/fall/user2/user2_fall5.csv | user2 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user2_fall6.csv\n",
      "Processing project/datasets/adl/fall/user2/user2_fall6.csv | user2 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user2_fall7.csv\n",
      "Processing project/datasets/adl/fall/user2/user2_fall7.csv | user2 | Activity 7 → Backward fall from seated position\n",
      "Found: user2_fall8.csv\n",
      "Processing project/datasets/adl/fall/user2/user2_fall8.csv | user2 | Activity 8 → Grabbing while falling\n",
      "Found: user20_fall1.csv\n",
      "Processing project/datasets/adl/fall/user20/user20_fall1.csv | user20 | Activity 1 → Forward fall landing on knee\n",
      "Found: user20_fall2.csv\n",
      "Processing project/datasets/adl/fall/user20/user20_fall2.csv | user20 | Activity 2 → Right fall\n",
      "Found: user20_fall3.csv\n",
      "Processing project/datasets/adl/fall/user20/user20_fall3.csv | user20 | Activity 3 → Left fall\n",
      "Found: user20_fall4.csv\n",
      "Processing project/datasets/adl/fall/user20/user20_fall4.csv | user20 | Activity 4 → Forward fall\n",
      "Found: user20_fall5.csv\n",
      "Processing project/datasets/adl/fall/user20/user20_fall5.csv | user20 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user20_fall6.csv\n",
      "Processing project/datasets/adl/fall/user20/user20_fall6.csv | user20 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user20_fall7.csv\n",
      "Processing project/datasets/adl/fall/user20/user20_fall7.csv | user20 | Activity 7 → Backward fall from seated position\n",
      "Found: user20_fall8.csv\n",
      "Processing project/datasets/adl/fall/user20/user20_fall8.csv | user20 | Activity 8 → Grabbing while falling\n",
      "Found: user21_fall1.csv\n",
      "Processing project/datasets/adl/fall/user21/user21_fall1.csv | user21 | Activity 1 → Forward fall landing on knee\n",
      "Found: user21_fall2.csv\n",
      "Processing project/datasets/adl/fall/user21/user21_fall2.csv | user21 | Activity 2 → Right fall\n",
      "Found: user21_fall3.csv\n",
      "Processing project/datasets/adl/fall/user21/user21_fall3.csv | user21 | Activity 3 → Left fall\n",
      "Found: user21_fall4.csv\n",
      "Processing project/datasets/adl/fall/user21/user21_fall4.csv | user21 | Activity 4 → Forward fall\n",
      "Found: user21_fall5.csv\n",
      "Processing project/datasets/adl/fall/user21/user21_fall5.csv | user21 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user21_fall6.csv\n",
      "Processing project/datasets/adl/fall/user21/user21_fall6.csv | user21 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user21_fall7.csv\n",
      "Processing project/datasets/adl/fall/user21/user21_fall7.csv | user21 | Activity 7 → Backward fall from seated position\n",
      "Found: user21_fall8.csv\n",
      "Processing project/datasets/adl/fall/user21/user21_fall8.csv | user21 | Activity 8 → Grabbing while falling\n",
      "Found: user22_fall1.csv\n",
      "Processing project/datasets/adl/fall/user22/user22_fall1.csv | user22 | Activity 1 → Forward fall landing on knee\n",
      "Found: user22_fall2.csv\n",
      "Processing project/datasets/adl/fall/user22/user22_fall2.csv | user22 | Activity 2 → Right fall\n",
      "Found: user22_fall3.csv\n",
      "Processing project/datasets/adl/fall/user22/user22_fall3.csv | user22 | Activity 3 → Left fall\n",
      "Found: user22_fall4.csv\n",
      "Processing project/datasets/adl/fall/user22/user22_fall4.csv | user22 | Activity 4 → Forward fall\n",
      "Found: user22_fall5.csv\n",
      "Processing project/datasets/adl/fall/user22/user22_fall5.csv | user22 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user22_fall6.csv\n",
      "Processing project/datasets/adl/fall/user22/user22_fall6.csv | user22 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user22_fall7.csv\n",
      "Processing project/datasets/adl/fall/user22/user22_fall7.csv | user22 | Activity 7 → Backward fall from seated position\n",
      "Found: user22_fall8.csv\n",
      "Processing project/datasets/adl/fall/user22/user22_fall8.csv | user22 | Activity 8 → Grabbing while falling\n",
      "Found: user23_fall1.csv\n",
      "Processing project/datasets/adl/fall/user23/user23_fall1.csv | user23 | Activity 1 → Forward fall landing on knee\n",
      "Found: user23_fall2.csv\n",
      "Processing project/datasets/adl/fall/user23/user23_fall2.csv | user23 | Activity 2 → Right fall\n",
      "Found: user23_fall3.csv\n",
      "Processing project/datasets/adl/fall/user23/user23_fall3.csv | user23 | Activity 3 → Left fall\n",
      "Found: user23_fall4.csv\n",
      "Processing project/datasets/adl/fall/user23/user23_fall4.csv | user23 | Activity 4 → Forward fall\n",
      "Found: user23_fall5.csv\n",
      "Processing project/datasets/adl/fall/user23/user23_fall5.csv | user23 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user23_fall6.csv\n",
      "Processing project/datasets/adl/fall/user23/user23_fall6.csv | user23 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user23_fall7.csv\n",
      "Processing project/datasets/adl/fall/user23/user23_fall7.csv | user23 | Activity 7 → Backward fall from seated position\n",
      "Found: user23_fall8.csv\n",
      "Processing project/datasets/adl/fall/user23/user23_fall8.csv | user23 | Activity 8 → Grabbing while falling\n",
      "Found: user24_fall1.csv\n",
      "Processing project/datasets/adl/fall/user24/user24_fall1.csv | user24 | Activity 1 → Forward fall landing on knee\n",
      "Found: user24_fall2.csv\n",
      "Processing project/datasets/adl/fall/user24/user24_fall2.csv | user24 | Activity 2 → Right fall\n",
      "Found: user24_fall3.csv\n",
      "Processing project/datasets/adl/fall/user24/user24_fall3.csv | user24 | Activity 3 → Left fall\n",
      "Found: user24_fall4.csv\n",
      "Processing project/datasets/adl/fall/user24/user24_fall4.csv | user24 | Activity 4 → Forward fall\n",
      "Found: user24_fall5.csv\n",
      "Processing project/datasets/adl/fall/user24/user24_fall5.csv | user24 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user24_fall6.csv\n",
      "Processing project/datasets/adl/fall/user24/user24_fall6.csv | user24 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user24_fall7.csv\n",
      "Processing project/datasets/adl/fall/user24/user24_fall7.csv | user24 | Activity 7 → Backward fall from seated position\n",
      "Found: user24_fall8.csv\n",
      "Processing project/datasets/adl/fall/user24/user24_fall8.csv | user24 | Activity 8 → Grabbing while falling\n",
      "Found: user25_fall1.csv\n",
      "Processing project/datasets/adl/fall/user25/user25_fall1.csv | user25 | Activity 1 → Forward fall landing on knee\n",
      "Found: user25_fall2.csv\n",
      "Processing project/datasets/adl/fall/user25/user25_fall2.csv | user25 | Activity 2 → Right fall\n",
      "Found: user25_fall3.csv\n",
      "Processing project/datasets/adl/fall/user25/user25_fall3.csv | user25 | Activity 3 → Left fall\n",
      "Found: user25_fall4.csv\n",
      "Processing project/datasets/adl/fall/user25/user25_fall4.csv | user25 | Activity 4 → Forward fall\n",
      "Found: user25_fall5.csv\n",
      "Processing project/datasets/adl/fall/user25/user25_fall5.csv | user25 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user25_fall6.csv\n",
      "Processing project/datasets/adl/fall/user25/user25_fall6.csv | user25 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user25_fall7.csv\n",
      "Processing project/datasets/adl/fall/user25/user25_fall7.csv | user25 | Activity 7 → Backward fall from seated position\n",
      "Found: user25_fall8.csv\n",
      "Processing project/datasets/adl/fall/user25/user25_fall8.csv | user25 | Activity 8 → Grabbing while falling\n",
      "Found: user26_fall1.csv\n",
      "Processing project/datasets/adl/fall/user26/user26_fall1.csv | user26 | Activity 1 → Forward fall landing on knee\n",
      "Found: user26_fall2.csv\n",
      "Processing project/datasets/adl/fall/user26/user26_fall2.csv | user26 | Activity 2 → Right fall\n",
      "Found: user26_fall3.csv\n",
      "Processing project/datasets/adl/fall/user26/user26_fall3.csv | user26 | Activity 3 → Left fall\n",
      "Found: user26_fall4.csv\n",
      "Processing project/datasets/adl/fall/user26/user26_fall4.csv | user26 | Activity 4 → Forward fall\n",
      "Found: user26_fall5.csv\n",
      "Processing project/datasets/adl/fall/user26/user26_fall5.csv | user26 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user26_fall6.csv\n",
      "Processing project/datasets/adl/fall/user26/user26_fall6.csv | user26 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user26_fall7.csv\n",
      "Processing project/datasets/adl/fall/user26/user26_fall7.csv | user26 | Activity 7 → Backward fall from seated position\n",
      "Found: user26_fall8.csv\n",
      "Processing project/datasets/adl/fall/user26/user26_fall8.csv | user26 | Activity 8 → Grabbing while falling\n",
      "Found: user27_fall1.csv\n",
      "Processing project/datasets/adl/fall/user27/user27_fall1.csv | user27 | Activity 1 → Forward fall landing on knee\n",
      "Found: user27_fall2.csv\n",
      "Processing project/datasets/adl/fall/user27/user27_fall2.csv | user27 | Activity 2 → Right fall\n",
      "Found: user27_fall3.csv\n",
      "Processing project/datasets/adl/fall/user27/user27_fall3.csv | user27 | Activity 3 → Left fall\n",
      "Found: user27_fall4.csv\n",
      "Processing project/datasets/adl/fall/user27/user27_fall4.csv | user27 | Activity 4 → Forward fall\n",
      "Found: user27_fall5.csv\n",
      "Processing project/datasets/adl/fall/user27/user27_fall5.csv | user27 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user27_fall6.csv\n",
      "Processing project/datasets/adl/fall/user27/user27_fall6.csv | user27 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user27_fall7.csv\n",
      "Processing project/datasets/adl/fall/user27/user27_fall7.csv | user27 | Activity 7 → Backward fall from seated position\n",
      "Found: user27_fall8.csv\n",
      "Processing project/datasets/adl/fall/user27/user27_fall8.csv | user27 | Activity 8 → Grabbing while falling\n",
      "Found: user28_fall1.csv\n",
      "Processing project/datasets/adl/fall/user28/user28_fall1.csv | user28 | Activity 1 → Forward fall landing on knee\n",
      "Found: user28_fall2.csv\n",
      "Processing project/datasets/adl/fall/user28/user28_fall2.csv | user28 | Activity 2 → Right fall\n",
      "Found: user28_fall3.csv\n",
      "Processing project/datasets/adl/fall/user28/user28_fall3.csv | user28 | Activity 3 → Left fall\n",
      "Found: user28_fall4.csv\n",
      "Processing project/datasets/adl/fall/user28/user28_fall4.csv | user28 | Activity 4 → Forward fall\n",
      "Found: user28_fall5.csv\n",
      "Processing project/datasets/adl/fall/user28/user28_fall5.csv | user28 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user28_fall6.csv\n",
      "Processing project/datasets/adl/fall/user28/user28_fall6.csv | user28 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user28_fall7.csv\n",
      "Processing project/datasets/adl/fall/user28/user28_fall7.csv | user28 | Activity 7 → Backward fall from seated position\n",
      "Found: user28_fall8.csv\n",
      "Processing project/datasets/adl/fall/user28/user28_fall8.csv | user28 | Activity 8 → Grabbing while falling\n",
      "Found: user29_fall1.csv\n",
      "Processing project/datasets/adl/fall/user29/user29_fall1.csv | user29 | Activity 1 → Forward fall landing on knee\n",
      "Found: user29_fall2.csv\n",
      "Processing project/datasets/adl/fall/user29/user29_fall2.csv | user29 | Activity 2 → Right fall\n",
      "Found: user29_fall3.csv\n",
      "Processing project/datasets/adl/fall/user29/user29_fall3.csv | user29 | Activity 3 → Left fall\n",
      "Found: user29_fall4.csv\n",
      "Processing project/datasets/adl/fall/user29/user29_fall4.csv | user29 | Activity 4 → Forward fall\n",
      "Found: user29_fall5.csv\n",
      "Processing project/datasets/adl/fall/user29/user29_fall5.csv | user29 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user29_fall6.csv\n",
      "Processing project/datasets/adl/fall/user29/user29_fall6.csv | user29 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user29_fall7.csv\n",
      "Processing project/datasets/adl/fall/user29/user29_fall7.csv | user29 | Activity 7 → Backward fall from seated position\n",
      "Found: user29_fall8.csv\n",
      "Processing project/datasets/adl/fall/user29/user29_fall8.csv | user29 | Activity 8 → Grabbing while falling\n",
      "Found: user3_fall1.csv\n",
      "Processing project/datasets/adl/fall/user3/user3_fall1.csv | user3 | Activity 1 → Forward fall landing on knee\n",
      "Found: user3_fall2.csv\n",
      "Processing project/datasets/adl/fall/user3/user3_fall2.csv | user3 | Activity 2 → Right fall\n",
      "Found: user3_fall3.csv\n",
      "Processing project/datasets/adl/fall/user3/user3_fall3.csv | user3 | Activity 3 → Left fall\n",
      "Found: user3_fall4.csv\n",
      "Processing project/datasets/adl/fall/user3/user3_fall4.csv | user3 | Activity 4 → Forward fall\n",
      "Found: user3_fall5.csv\n",
      "Processing project/datasets/adl/fall/user3/user3_fall5.csv | user3 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user3_fall6.csv\n",
      "Processing project/datasets/adl/fall/user3/user3_fall6.csv | user3 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user3_fall7.csv\n",
      "Processing project/datasets/adl/fall/user3/user3_fall7.csv | user3 | Activity 7 → Backward fall from seated position\n",
      "Found: user3_fall8.csv\n",
      "Processing project/datasets/adl/fall/user3/user3_fall8.csv | user3 | Activity 8 → Grabbing while falling\n",
      "Found: user30_fall1.csv\n",
      "Processing project/datasets/adl/fall/user30/user30_fall1.csv | user30 | Activity 1 → Forward fall landing on knee\n",
      "Found: user30_fall2.csv\n",
      "Processing project/datasets/adl/fall/user30/user30_fall2.csv | user30 | Activity 2 → Right fall\n",
      "Found: user30_fall3.csv\n",
      "Processing project/datasets/adl/fall/user30/user30_fall3.csv | user30 | Activity 3 → Left fall\n",
      "Found: user30_fall4.csv\n",
      "Processing project/datasets/adl/fall/user30/user30_fall4.csv | user30 | Activity 4 → Forward fall\n",
      "Found: user30_fall5.csv\n",
      "Processing project/datasets/adl/fall/user30/user30_fall5.csv | user30 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user30_fall6.csv\n",
      "Processing project/datasets/adl/fall/user30/user30_fall6.csv | user30 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user30_fall7.csv\n",
      "Processing project/datasets/adl/fall/user30/user30_fall7.csv | user30 | Activity 7 → Backward fall from seated position\n",
      "Found: user30_fall8.csv\n",
      "Processing project/datasets/adl/fall/user30/user30_fall8.csv | user30 | Activity 8 → Grabbing while falling\n",
      "Found: user31_.fall1.csv\n",
      "Skipping user31_.fall1.csv — invalid format\n",
      "Found: user31_.fall3.csv\n",
      "Skipping user31_.fall3.csv — invalid format\n",
      "Found: user31_.fall4.csv\n",
      "Skipping user31_.fall4.csv — invalid format\n",
      "Found: user31_.fall6.csv\n",
      "Skipping user31_.fall6.csv — invalid format\n",
      "Found: user31_fall2.csv\n",
      "Processing project/datasets/adl/fall/user31/user31_fall2.csv | user31 | Activity 2 → Right fall\n",
      "Found: user31_fall5.csv\n",
      "Processing project/datasets/adl/fall/user31/user31_fall5.csv | user31 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user31_fall7.csv\n",
      "Processing project/datasets/adl/fall/user31/user31_fall7.csv | user31 | Activity 7 → Backward fall from seated position\n",
      "Found: user31_fall8.csv\n",
      "Processing project/datasets/adl/fall/user31/user31_fall8.csv | user31 | Activity 8 → Grabbing while falling\n",
      "Found: user32_fall1.csv\n",
      "Processing project/datasets/adl/fall/user32/user32_fall1.csv | user32 | Activity 1 → Forward fall landing on knee\n",
      "Found: user32_fall2.csv\n",
      "Processing project/datasets/adl/fall/user32/user32_fall2.csv | user32 | Activity 2 → Right fall\n",
      "Found: user32_fall3.csv\n",
      "Processing project/datasets/adl/fall/user32/user32_fall3.csv | user32 | Activity 3 → Left fall\n",
      "Found: user32_fall4.csv\n",
      "Processing project/datasets/adl/fall/user32/user32_fall4.csv | user32 | Activity 4 → Forward fall\n",
      "Found: user32_fall5.csv\n",
      "Processing project/datasets/adl/fall/user32/user32_fall5.csv | user32 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user32_fall6.csv\n",
      "Processing project/datasets/adl/fall/user32/user32_fall6.csv | user32 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user32_fall7.csv\n",
      "Processing project/datasets/adl/fall/user32/user32_fall7.csv | user32 | Activity 7 → Backward fall from seated position\n",
      "Found: user32_fall8.csv\n",
      "Processing project/datasets/adl/fall/user32/user32_fall8.csv | user32 | Activity 8 → Grabbing while falling\n",
      "Found: user33_fall1.csv\n",
      "Processing project/datasets/adl/fall/user33/user33_fall1.csv | user33 | Activity 1 → Forward fall landing on knee\n",
      "Found: user33_fall2.csv\n",
      "Processing project/datasets/adl/fall/user33/user33_fall2.csv | user33 | Activity 2 → Right fall\n",
      "Found: user33_fall3.csv\n",
      "Processing project/datasets/adl/fall/user33/user33_fall3.csv | user33 | Activity 3 → Left fall\n",
      "Found: user33_fall4.csv\n",
      "Processing project/datasets/adl/fall/user33/user33_fall4.csv | user33 | Activity 4 → Forward fall\n",
      "Found: user33_fall5.csv\n",
      "Processing project/datasets/adl/fall/user33/user33_fall5.csv | user33 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user33_fall6.csv\n",
      "Processing project/datasets/adl/fall/user33/user33_fall6.csv | user33 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user33_fall7.csv\n",
      "Processing project/datasets/adl/fall/user33/user33_fall7.csv | user33 | Activity 7 → Backward fall from seated position\n",
      "Found: user33_fall8.csv\n",
      "Processing project/datasets/adl/fall/user33/user33_fall8.csv | user33 | Activity 8 → Grabbing while falling\n",
      "Found: user34_fall1.csv\n",
      "Processing project/datasets/adl/fall/user34/user34_fall1.csv | user34 | Activity 1 → Forward fall landing on knee\n",
      "Found: user34_fall2.csv\n",
      "Processing project/datasets/adl/fall/user34/user34_fall2.csv | user34 | Activity 2 → Right fall\n",
      "Found: user34_fall3.csv\n",
      "Processing project/datasets/adl/fall/user34/user34_fall3.csv | user34 | Activity 3 → Left fall\n",
      "Found: user34_fall4.csv\n",
      "Processing project/datasets/adl/fall/user34/user34_fall4.csv | user34 | Activity 4 → Forward fall\n",
      "Found: user34_fall5.csv\n",
      "Processing project/datasets/adl/fall/user34/user34_fall5.csv | user34 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user34_fall6.csv\n",
      "Processing project/datasets/adl/fall/user34/user34_fall6.csv | user34 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user34_fall7.csv\n",
      "Processing project/datasets/adl/fall/user34/user34_fall7.csv | user34 | Activity 7 → Backward fall from seated position\n",
      "Found: user34_fall8.csv\n",
      "Processing project/datasets/adl/fall/user34/user34_fall8.csv | user34 | Activity 8 → Grabbing while falling\n",
      "Found: user35_fall1.csv\n",
      "Processing project/datasets/adl/fall/user35/user35_fall1.csv | user35 | Activity 1 → Forward fall landing on knee\n",
      "Found: user35_fall2.csv\n",
      "Processing project/datasets/adl/fall/user35/user35_fall2.csv | user35 | Activity 2 → Right fall\n",
      "Found: user35_fall3.csv\n",
      "Processing project/datasets/adl/fall/user35/user35_fall3.csv | user35 | Activity 3 → Left fall\n",
      "Found: user35_fall4.csv\n",
      "Processing project/datasets/adl/fall/user35/user35_fall4.csv | user35 | Activity 4 → Forward fall\n",
      "Found: user35_fall5.csv\n",
      "Processing project/datasets/adl/fall/user35/user35_fall5.csv | user35 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user35_fall6.csv\n",
      "Processing project/datasets/adl/fall/user35/user35_fall6.csv | user35 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user35_fall7.csv\n",
      "Processing project/datasets/adl/fall/user35/user35_fall7.csv | user35 | Activity 7 → Backward fall from seated position\n",
      "Found: user35_fall8.csv\n",
      "Processing project/datasets/adl/fall/user35/user35_fall8.csv | user35 | Activity 8 → Grabbing while falling\n",
      "Found: user36_fall1.csv\n",
      "Processing project/datasets/adl/fall/user36/user36_fall1.csv | user36 | Activity 1 → Forward fall landing on knee\n",
      "Found: user36_fall2.csv\n",
      "Processing project/datasets/adl/fall/user36/user36_fall2.csv | user36 | Activity 2 → Right fall\n",
      "Found: user36_fall3.csv\n",
      "Processing project/datasets/adl/fall/user36/user36_fall3.csv | user36 | Activity 3 → Left fall\n",
      "Found: user36_fall4.csv\n",
      "Processing project/datasets/adl/fall/user36/user36_fall4.csv | user36 | Activity 4 → Forward fall\n",
      "Found: user36_fall5.csv\n",
      "Processing project/datasets/adl/fall/user36/user36_fall5.csv | user36 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user36_fall6.csv\n",
      "Processing project/datasets/adl/fall/user36/user36_fall6.csv | user36 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user36_fall7.csv\n",
      "Processing project/datasets/adl/fall/user36/user36_fall7.csv | user36 | Activity 7 → Backward fall from seated position\n",
      "Found: user36_fall8.csv\n",
      "Processing project/datasets/adl/fall/user36/user36_fall8.csv | user36 | Activity 8 → Grabbing while falling\n",
      "Found: user37_fall1.csv\n",
      "Processing project/datasets/adl/fall/user37/user37_fall1.csv | user37 | Activity 1 → Forward fall landing on knee\n",
      "Found: user37_fall2.csv\n",
      "Processing project/datasets/adl/fall/user37/user37_fall2.csv | user37 | Activity 2 → Right fall\n",
      "Found: user37_fall3.csv\n",
      "Processing project/datasets/adl/fall/user37/user37_fall3.csv | user37 | Activity 3 → Left fall\n",
      "Found: user37_fall4.csv\n",
      "Processing project/datasets/adl/fall/user37/user37_fall4.csv | user37 | Activity 4 → Forward fall\n",
      "Found: user37_fall5.csv\n",
      "Processing project/datasets/adl/fall/user37/user37_fall5.csv | user37 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user37_fall6.csv\n",
      "Processing project/datasets/adl/fall/user37/user37_fall6.csv | user37 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user37_fall7.csv\n",
      "Processing project/datasets/adl/fall/user37/user37_fall7.csv | user37 | Activity 7 → Backward fall from seated position\n",
      "Found: user37_fall8.csv\n",
      "Processing project/datasets/adl/fall/user37/user37_fall8.csv | user37 | Activity 8 → Grabbing while falling\n",
      "Found: user38_fall1.csv\n",
      "Processing project/datasets/adl/fall/user38/user38_fall1.csv | user38 | Activity 1 → Forward fall landing on knee\n",
      "Found: user38_fall2.csv\n",
      "Processing project/datasets/adl/fall/user38/user38_fall2.csv | user38 | Activity 2 → Right fall\n",
      "Found: user38_fall3.csv\n",
      "Processing project/datasets/adl/fall/user38/user38_fall3.csv | user38 | Activity 3 → Left fall\n",
      "Found: user38_fall4.csv\n",
      "Processing project/datasets/adl/fall/user38/user38_fall4.csv | user38 | Activity 4 → Forward fall\n",
      "Found: user38_fall5.csv\n",
      "Processing project/datasets/adl/fall/user38/user38_fall5.csv | user38 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user38_fall6.csv\n",
      "Processing project/datasets/adl/fall/user38/user38_fall6.csv | user38 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user38_fall7.csv\n",
      "Processing project/datasets/adl/fall/user38/user38_fall7.csv | user38 | Activity 7 → Backward fall from seated position\n",
      "Found: user38_fall8.csv\n",
      "Processing project/datasets/adl/fall/user38/user38_fall8.csv | user38 | Activity 8 → Grabbing while falling\n",
      "Found: user39_fall1.csv\n",
      "Processing project/datasets/adl/fall/user39/user39_fall1.csv | user39 | Activity 1 → Forward fall landing on knee\n",
      "Found: user39_fall2.csv\n",
      "Processing project/datasets/adl/fall/user39/user39_fall2.csv | user39 | Activity 2 → Right fall\n",
      "Found: user39_fall3.csv\n",
      "Processing project/datasets/adl/fall/user39/user39_fall3.csv | user39 | Activity 3 → Left fall\n",
      "Found: user39_fall4.csv\n",
      "Processing project/datasets/adl/fall/user39/user39_fall4.csv | user39 | Activity 4 → Forward fall\n",
      "Found: user39_fall5.csv\n",
      "Processing project/datasets/adl/fall/user39/user39_fall5.csv | user39 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user39_fall6.csv\n",
      "Processing project/datasets/adl/fall/user39/user39_fall6.csv | user39 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user39_fall7.csv\n",
      "Processing project/datasets/adl/fall/user39/user39_fall7.csv | user39 | Activity 7 → Backward fall from seated position\n",
      "Found: user39_fall8.csv\n",
      "Processing project/datasets/adl/fall/user39/user39_fall8.csv | user39 | Activity 8 → Grabbing while falling\n",
      "Found: user4_fall1.csv\n",
      "Processing project/datasets/adl/fall/user4/user4_fall1.csv | user4 | Activity 1 → Forward fall landing on knee\n",
      "Found: user4_fall2.csv\n",
      "Processing project/datasets/adl/fall/user4/user4_fall2.csv | user4 | Activity 2 → Right fall\n",
      "Found: user4_fall3.csv\n",
      "Processing project/datasets/adl/fall/user4/user4_fall3.csv | user4 | Activity 3 → Left fall\n",
      "Found: user4_fall4.csv\n",
      "Processing project/datasets/adl/fall/user4/user4_fall4.csv | user4 | Activity 4 → Forward fall\n",
      "Found: user4_fall5.csv\n",
      "Processing project/datasets/adl/fall/user4/user4_fall5.csv | user4 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user4_fall6.csv\n",
      "Processing project/datasets/adl/fall/user4/user4_fall6.csv | user4 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user4_fall7.csv\n",
      "Processing project/datasets/adl/fall/user4/user4_fall7.csv | user4 | Activity 7 → Backward fall from seated position\n",
      "Found: user4_fall8.csv\n",
      "Processing project/datasets/adl/fall/user4/user4_fall8.csv | user4 | Activity 8 → Grabbing while falling\n",
      "Found: user40_fall1.csv\n",
      "Processing project/datasets/adl/fall/user40/user40_fall1.csv | user40 | Activity 1 → Forward fall landing on knee\n",
      "Found: user40_fall2.csv\n",
      "Processing project/datasets/adl/fall/user40/user40_fall2.csv | user40 | Activity 2 → Right fall\n",
      "Found: user40_fall3.csv\n",
      "Processing project/datasets/adl/fall/user40/user40_fall3.csv | user40 | Activity 3 → Left fall\n",
      "Found: user40_fall4.csv\n",
      "Processing project/datasets/adl/fall/user40/user40_fall4.csv | user40 | Activity 4 → Forward fall\n",
      "Found: user40_fall5.csv\n",
      "Processing project/datasets/adl/fall/user40/user40_fall5.csv | user40 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user40_fall6.csv\n",
      "Processing project/datasets/adl/fall/user40/user40_fall6.csv | user40 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user40_fall7.csv\n",
      "Processing project/datasets/adl/fall/user40/user40_fall7.csv | user40 | Activity 7 → Backward fall from seated position\n",
      "Found: user40_fall8.csv\n",
      "Processing project/datasets/adl/fall/user40/user40_fall8.csv | user40 | Activity 8 → Grabbing while falling\n",
      "Found: user41_fall1.csv\n",
      "Processing project/datasets/adl/fall/user41/user41_fall1.csv | user41 | Activity 1 → Forward fall landing on knee\n",
      "Found: user41_fall2.csv\n",
      "Processing project/datasets/adl/fall/user41/user41_fall2.csv | user41 | Activity 2 → Right fall\n",
      "Found: user41_fall3.csv\n",
      "Processing project/datasets/adl/fall/user41/user41_fall3.csv | user41 | Activity 3 → Left fall\n",
      "Found: user41_fall4.csv\n",
      "Processing project/datasets/adl/fall/user41/user41_fall4.csv | user41 | Activity 4 → Forward fall\n",
      "Found: user41_fall5.csv\n",
      "Processing project/datasets/adl/fall/user41/user41_fall5.csv | user41 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user41_fall6.csv\n",
      "Processing project/datasets/adl/fall/user41/user41_fall6.csv | user41 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user41_fall7.csv\n",
      "Processing project/datasets/adl/fall/user41/user41_fall7.csv | user41 | Activity 7 → Backward fall from seated position\n",
      "Found: user41_fall8.csv\n",
      "Processing project/datasets/adl/fall/user41/user41_fall8.csv | user41 | Activity 8 → Grabbing while falling\n",
      "Found: user5_fall1.csv\n",
      "Processing project/datasets/adl/fall/user5/user5_fall1.csv | user5 | Activity 1 → Forward fall landing on knee\n",
      "Found: user5_fall2.csv\n",
      "Processing project/datasets/adl/fall/user5/user5_fall2.csv | user5 | Activity 2 → Right fall\n",
      "Found: user5_fall3.csv\n",
      "Processing project/datasets/adl/fall/user5/user5_fall3.csv | user5 | Activity 3 → Left fall\n",
      "Found: user5_fall4.csv\n",
      "Processing project/datasets/adl/fall/user5/user5_fall4.csv | user5 | Activity 4 → Forward fall\n",
      "Found: user5_fall5.csv\n",
      "Processing project/datasets/adl/fall/user5/user5_fall5.csv | user5 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user5_fall6.csv\n",
      "Processing project/datasets/adl/fall/user5/user5_fall6.csv | user5 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user5_fall7.csv\n",
      "Processing project/datasets/adl/fall/user5/user5_fall7.csv | user5 | Activity 7 → Backward fall from seated position\n",
      "Found: user5_fall8.csv\n",
      "Processing project/datasets/adl/fall/user5/user5_fall8.csv | user5 | Activity 8 → Grabbing while falling\n",
      "Found: user6_fall1.csv\n",
      "Processing project/datasets/adl/fall/user6/user6_fall1.csv | user6 | Activity 1 → Forward fall landing on knee\n",
      "Found: user6_fall2.csv\n",
      "Processing project/datasets/adl/fall/user6/user6_fall2.csv | user6 | Activity 2 → Right fall\n",
      "Found: user6_fall3.csv\n",
      "Processing project/datasets/adl/fall/user6/user6_fall3.csv | user6 | Activity 3 → Left fall\n",
      "Found: user6_fall4.csv\n",
      "Processing project/datasets/adl/fall/user6/user6_fall4.csv | user6 | Activity 4 → Forward fall\n",
      "Found: user6_fall5.csv\n",
      "Processing project/datasets/adl/fall/user6/user6_fall5.csv | user6 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user6_fall6.csv\n",
      "Processing project/datasets/adl/fall/user6/user6_fall6.csv | user6 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user6_fall7.csv\n",
      "Processing project/datasets/adl/fall/user6/user6_fall7.csv | user6 | Activity 7 → Backward fall from seated position\n",
      "Found: user6_fall8.csv\n",
      "Processing project/datasets/adl/fall/user6/user6_fall8.csv | user6 | Activity 8 → Grabbing while falling\n",
      "Found: user7_fall1.csv\n",
      "Processing project/datasets/adl/fall/user7/user7_fall1.csv | user7 | Activity 1 → Forward fall landing on knee\n",
      "Found: user7_fall2.csv\n",
      "Processing project/datasets/adl/fall/user7/user7_fall2.csv | user7 | Activity 2 → Right fall\n",
      "Found: user7_fall3.csv\n",
      "Processing project/datasets/adl/fall/user7/user7_fall3.csv | user7 | Activity 3 → Left fall\n",
      "Found: user7_fall4.csv\n",
      "Processing project/datasets/adl/fall/user7/user7_fall4.csv | user7 | Activity 4 → Forward fall\n",
      "Found: user7_fall5.csv\n",
      "Processing project/datasets/adl/fall/user7/user7_fall5.csv | user7 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user7_fall6.csv\n",
      "Processing project/datasets/adl/fall/user7/user7_fall6.csv | user7 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user7_fall7.csv\n",
      "Processing project/datasets/adl/fall/user7/user7_fall7.csv | user7 | Activity 7 → Backward fall from seated position\n",
      "Found: user7_fall8.csv\n",
      "Processing project/datasets/adl/fall/user7/user7_fall8.csv | user7 | Activity 8 → Grabbing while falling\n",
      "Found: user8_fall1.csv\n",
      "Processing project/datasets/adl/fall/user8/user8_fall1.csv | user8 | Activity 1 → Forward fall landing on knee\n",
      "Found: user8_fall2.csv\n",
      "Processing project/datasets/adl/fall/user8/user8_fall2.csv | user8 | Activity 2 → Right fall\n",
      "Found: user8_fall3.csv\n",
      "Processing project/datasets/adl/fall/user8/user8_fall3.csv | user8 | Activity 3 → Left fall\n",
      "Found: user8_fall4.csv\n",
      "Processing project/datasets/adl/fall/user8/user8_fall4.csv | user8 | Activity 4 → Forward fall\n",
      "Found: user8_fall5.csv\n",
      "Processing project/datasets/adl/fall/user8/user8_fall5.csv | user8 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user8_fall6.csv\n",
      "Processing project/datasets/adl/fall/user8/user8_fall6.csv | user8 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user8_fall7.csv\n",
      "Processing project/datasets/adl/fall/user8/user8_fall7.csv | user8 | Activity 7 → Backward fall from seated position\n",
      "Found: user8_fall8.csv\n",
      "Processing project/datasets/adl/fall/user8/user8_fall8.csv | user8 | Activity 8 → Grabbing while falling\n",
      "Found: user9_fall1.csv\n",
      "Processing project/datasets/adl/fall/user9/user9_fall1.csv | user9 | Activity 1 → Forward fall landing on knee\n",
      "Found: user9_fall2.csv\n",
      "Processing project/datasets/adl/fall/user9/user9_fall2.csv | user9 | Activity 2 → Right fall\n",
      "Found: user9_fall3.csv\n",
      "Processing project/datasets/adl/fall/user9/user9_fall3.csv | user9 | Activity 3 → Left fall\n",
      "Found: user9_fall4.csv\n",
      "Processing project/datasets/adl/fall/user9/user9_fall4.csv | user9 | Activity 4 → Forward fall\n",
      "Found: user9_fall5.csv\n",
      "Processing project/datasets/adl/fall/user9/user9_fall5.csv | user9 | Activity 5 → Seated on bed and falling on ground\n",
      "Found: user9_fall6.csv\n",
      "Processing project/datasets/adl/fall/user9/user9_fall6.csv | user9 | Activity 6 → Forward fall body weight on hand\n",
      "Found: user9_fall7.csv\n",
      "Processing project/datasets/adl/fall/user9/user9_fall7.csv | user9 | Activity 7 → Backward fall from seated position\n",
      "Found: user9_fall8.csv\n",
      "Processing project/datasets/adl/fall/user9/user9_fall8.csv | user9 | Activity 8 → Grabbing while falling\n",
      "Found: test.csv\n",
      "Skipping test.csv — invalid format\n",
      "Found: test_processed.csv\n",
      "Skipping test_processed.csv — invalid format\n",
      "Found: train.csv\n",
      "Skipping train.csv — invalid format\n",
      "Found: train_processed.csv\n",
      "Skipping train_processed.csv — invalid format\n",
      "Found: volunteer_details.csv\n",
      "Skipping volunteer_details.csv — invalid format\n",
      "Final combined CSV uploaded to: s3://iti113-team12-bucket/project/datasets/adl/combined_ADLfalls_data.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define S3 info\n",
    "bucket = bucket_name\n",
    "prefix = \"project/datasets/adl/\"   # the root folder in S3\n",
    "output_key = \"project/datasets/adl/combined_ADLfalls_data.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Remove output if exists (optional)\n",
    "try:\n",
    "    s3.delete_object(Bucket=bucket, Key=output_key)\n",
    "    print(\"Previous combined file removed from S3.\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"Starting to combine CSV...\")\n",
    "\n",
    "# === List all CSV files under the prefix ===\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "if \"Contents\" not in response:\n",
    "    print(\"No files found in S3 path.\")\n",
    "else:\n",
    "    for obj in response[\"Contents\"]:\n",
    "        key = obj[\"Key\"]\n",
    "        if not key.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        filename = os.path.basename(key)\n",
    "        print(\"Found:\", filename)\n",
    "\n",
    "        # Match 'userX_adlY.csv'\n",
    "        adl_match = re.match(r\"(user\\d+)_adl(\\d+)\", filename)\n",
    "        # Match 'userX_fallY.csv'\n",
    "        fall_match = re.match(r\"(user\\d+)_fall(\\d+)\", filename)\n",
    "\n",
    "        if adl_match:\n",
    "            user_id = adl_match.group(1)\n",
    "            activity_num = int(adl_match.group(2))\n",
    "            activity_label = adl_labels.get(activity_num, \"Unknown ADL\")\n",
    "        elif fall_match:\n",
    "            user_id = fall_match.group(1)\n",
    "            activity_num = int(fall_match.group(2))\n",
    "            activity_label = fall_labels.get(activity_num, \"Unknown Fall\")\n",
    "        else:\n",
    "            print(f\"Skipping {filename} — invalid format\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {key} | {user_id} | Activity {activity_num} → {activity_label}\")\n",
    "\n",
    "        try:\n",
    "            # Read CSV directly from S3\n",
    "            obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "            df = pd.read_csv(obj[\"Body\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {key}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df = df.rename(columns={\"Unnamed: 5\": \"sensor_type\"})\n",
    "\n",
    "        first_col = df.columns[0]\n",
    "        df[\"timestamp\"] = df[first_col]\n",
    "        df = df[df[first_col] != first_col]\n",
    "\n",
    "        # --- Align sensor columns ---\n",
    "        hrt_mask = (df[\"z\"] == \"hrt\") & (df[\"sensor_type\"].isna() | df[\"sensor_type\"].eq(\"\"))\n",
    "        df.loc[hrt_mask, \"sensor_type\"] = \"hrt\"\n",
    "        df.loc[hrt_mask, [\"y\", \"z\"]] = pd.NA\n",
    "\n",
    "        df[\"activity_label\"] = activity_label\n",
    "        df[\"user_id\"] = user_id.strip(\"user\")\n",
    "\n",
    "        standard_columns = [\"timestamp\", \"x\", \"y\", \"z\", \"sensor_type\", \"activity_label\", \"user_id\"]\n",
    "        cols_to_keep = [c for c in standard_columns if c in df.columns]\n",
    "        df = df[cols_to_keep]\n",
    "\n",
    "        # Append into a local file first (SageMaker instance)\n",
    "        local_tmp = \"/tmp/combined_ADLfalls_data.csv\"\n",
    "        df.to_csv(local_tmp, mode=\"a\", header=not os.path.exists(local_tmp), index=False)\n",
    "\n",
    "    # Upload the combined file back to S3\n",
    "    s3.upload_file(local_tmp, bucket, output_key)\n",
    "    print(\"Final combined CSV uploaded to:\", f\"s3://{bucket}/{output_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95dc5836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:31:23.580939Z",
     "iopub.status.busy": "2025-08-19T08:31:23.580695Z",
     "iopub.status.idle": "2025-08-19T08:31:27.563794Z",
     "shell.execute_reply": "2025-08-19T08:31:27.563372Z",
     "shell.execute_reply.started": "2025-08-19T08:31:23.580923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded combined dataset:\n",
      "      timestamp         x         y         z sensor_type  activity_label  \\\n",
      "0  1.820000e+11  0.126893 -0.033519  0.067038         acc  Walking slowly   \n",
      "1  1.820000e+11 -0.268151 -0.167594  0.122104         acc  Walking slowly   \n",
      "2  1.830000e+11 -0.883460 -0.119710 -0.392649         acc  Walking slowly   \n",
      "3  1.830000e+11 -0.253785  0.605733 -0.526724         acc  Walking slowly   \n",
      "4  1.830000e+11 -0.189142  0.136469  0.165200         acc  Walking slowly   \n",
      "\n",
      "   user_id  \n",
      "0        1  \n",
      "1        1  \n",
      "2        1  \n",
      "3        1  \n",
      "4        1  \n",
      "Successfully loaded user dataset:\n",
      "   Subject Id  Gender  Height (cm)  Weight (KG)  Age  Heart Rate (base)  \\\n",
      "0           1    Male       167.64         65.0   25                114   \n",
      "1           2    Male       193.04         98.0   41                 82   \n",
      "2           3  Female       152.40         62.5   46                 79   \n",
      "3           4  Female       157.48         50.0   23                110   \n",
      "4           5  Female       170.18         62.0   20                 97   \n",
      "\n",
      "                  Health condition  \n",
      "0                Sinus Tachycardia  \n",
      "1  High Blood Pressure, Overweight  \n",
      "2        No existing Health Issues  \n",
      "3               Multiple allergies  \n",
      "4        No existing Health Issues  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    full_data_output_path = \"s3://iti113-team12-bucket/project/datasets/adl/combined_ADLfalls_data.csv\"\n",
    "    df = pd.read_csv(full_data_output_path)\n",
    "    print(\"Successfully loaded combined dataset:\")\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"\\nError loading combined csv file.\")\n",
    "\n",
    "try:\n",
    "    full_userData_path = \"s3://iti113-team12-bucket/project/datasets/adl/users/volunteer_details.csv\"\n",
    "    details = pd.read_csv(full_userData_path)\n",
    "    print(\"Successfully loaded user dataset:\")\n",
    "    print(details.head())\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"\\nError loading user csv file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0b632-8c9f-4a7d-b29c-c8ff7033976f",
   "metadata": {},
   "source": [
    "## Create scripts to be used in SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278363b1",
   "metadata": {},
   "source": [
    "#### 0. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f591000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:31:27.564670Z",
     "iopub.status.busy": "2025-08-19T08:31:27.564384Z",
     "iopub.status.idle": "2025-08-19T08:31:27.569337Z",
     "shell.execute_reply": "2025-08-19T08:31:27.568954Z",
     "shell.execute_reply.started": "2025-08-19T08:31:27.564654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting health_featureEngr.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile health_featureEngr.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input-path', type=str, required=True, help=\"Directory containing main dataset\")\n",
    "    parser.add_argument('--user-input-path', type=str, required=True, help=\"Directory containing volunteer dataset\")\n",
    "    parser.add_argument(\"--output-path\", type=str, required=True, help=\"Output directory for dataset with engineered features\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Use provided paths or fall back to SageMaker defaults\n",
    "    input_path = args.input_path\n",
    "    user_input_path = args.user_input_path\n",
    "    output_path = args.output_path\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Load data\n",
    "    input_file = os.path.join(input_path, 'combined_ADLfalls_data.csv')\n",
    "    supp_file = os.path.join(user_input_path, 'volunteer_details.csv')\n",
    "    print(f\"Reading input file from {input_file}...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Reading supplementary data file from {supp_file}...\")\n",
    "    details = pd.read_csv(supp_file)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "\n",
    "    # ======================================================\n",
    "    # Form discrete difference features from time-based data\n",
    "    # ======================================================\n",
    "    # Calculate diff\n",
    "    df[[\"diff_x\", \"diff_y\", \"diff_z\"]] = (\n",
    "        df.groupby([\"user_id\", \"activity_label\", \"sensor_type\"])[[\"x\", \"y\", \"z\"]]\n",
    "        .diff()\n",
    "    )\n",
    "\n",
    "    # Drop the first row of each group (where diffs are NaN)\n",
    "    df = df[df.groupby([\"user_id\", \"activity_label\", \"sensor_type\"]).cumcount() != 0].reset_index(drop=True)\n",
    "\n",
    "    # ====================================================\n",
    "    # Form vector sum features from 3-axis time-based data\n",
    "    # ====================================================\n",
    "    # Create a new column \"vec_sum\", initialize with NaN\n",
    "    df[\"vec_sum\"] = np.nan  \n",
    "\n",
    "    # Compute only for sensor types not equal to 'hrt'\n",
    "    mask = df[\"sensor_type\"] != \"hrt\"\n",
    "    df.loc[mask, \"vec_sum\"] = np.sqrt(\n",
    "        df.loc[mask, \"x\"]**2 +\n",
    "        df.loc[mask, \"y\"]**2 +\n",
    "        df.loc[mask, \"z\"]**2\n",
    "    )\n",
    "\n",
    "    # =================================\n",
    "    # Form mean, std of time-based data\n",
    "    # =================================\n",
    "    # Filter rows with expected sensor types only (optional for safety)\n",
    "    valid_sensors = ['gyro', 'acc', 'acg', 'mgm', 'hrt']\n",
    "    df = df[df['sensor_type'].isin(valid_sensors)]\n",
    "    # Mapping sensor types to your naming style\n",
    "    sensor_name_map = {\n",
    "        'gyro': 'rotation_rate',\n",
    "        'acg': 'user_acceleration',\n",
    "        'acc': 'acceleration',\n",
    "        'mgm': 'magnetometer',\n",
    "        'hrt': 'heart_rate'\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Group by user, activity, sensor\n",
    "    for (user_id, activity_label, sensor_type), group in df.groupby(['user_id', 'activity_label', 'sensor_type']):\n",
    "        sensor = sensor_name_map.get(sensor_type, sensor_type)\n",
    "\n",
    "        row = {\n",
    "            'user_id': user_id,\n",
    "            'activity_label': activity_label\n",
    "        }\n",
    "\n",
    "        # Mean and std of x,y,z,diff_x,diff_y,diff_z,vec_sum\n",
    "        for axis in ['x', 'y', 'z', 'diff_x', 'diff_y', 'diff_z', 'vec_sum']:\n",
    "            row[f'{sensor}_{axis}_mean'] = group[axis].astype(float).mean()\n",
    "            row[f'{sensor}_{axis}_std'] = group[axis].astype(float).std()\n",
    "            # row[f'{sensor}_{axis}'] = group[axis].astype(float).iloc[-1]  # last value # not keeping the original time-series data\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    # Convert to flat dataframe\n",
    "    df2 = pd.DataFrame(rows)\n",
    "\n",
    "    # Merge all sensors for same user_id/activity_label\n",
    "    df2 = (\n",
    "        df2\n",
    "        .groupby(['user_id', 'activity_label'], as_index=False)\n",
    "        .first()\n",
    "    )\n",
    "\n",
    "    # Drop heart rate y and z since these are empty\n",
    "\n",
    "    drop_cols = ['heart_rate_y','heart_rate_z','heart_rate_diff_y','heart_rate_diff_z','heart_rate_vec_sum']\n",
    "    columns_to_drop = [col for col in df2.columns if any(k in col.lower() for k in drop_cols)]\n",
    "\n",
    "    df2 = df2.drop(columns=columns_to_drop)\n",
    "\n",
    "    # =================================\n",
    "    # Add volunteer details to dataset\n",
    "    # =================================\n",
    "    # Convert both ID columns to int type before merging\n",
    "    df2['user_id'] = df2['user_id'].astype(int)\n",
    "\n",
    "\n",
    "    # Map Health condition to health_status\n",
    "    details['health_status'] = details['Health condition'].apply(\n",
    "        lambda x: 'Unhealthy' if any(cond in str(x).lower() for cond in ['obese', 'overweight', 'arthritis']) else 'Healthy'\n",
    "    )\n",
    "\n",
    "    # Create age_group based on threshold\n",
    "    details['age_group'] = np.where(details['Age'] <= 35, 'young', 'older')\n",
    "\n",
    "\n",
    "    # Merge the group_label into df based on user_id\n",
    "    df2 = df2.merge(details[['Subject Id', 'health_status', 'age_group']],\n",
    "                left_on='user_id', right_on='Subject Id', how='left')\n",
    "\n",
    "    # Drop the temporary column\n",
    "    df2 = df2.drop(columns=['Subject Id'])  # Drop Subject Id after merge\n",
    "\n",
    "    # -----------------------------------------\n",
    " \n",
    "    # Saving\n",
    "    print(\"Engineering features...\")\n",
    "\n",
    "    # Save data with engineered features\n",
    "    data_output = os.path.join(output_path, \"combined_ADLfalls_engineered.csv\")\n",
    "\n",
    "    print(f\"Saving train to {data_output}\")\n",
    "    df2.to_csv(data_output, index=False)\n",
    "\n",
    "    print(\"Feature engineering complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227f1c8-50fa-450f-84ef-a8de46986feb",
   "metadata": {},
   "source": [
    "#### 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c615a45-8a7e-4165-8eb7-ccaf4215699d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:31:27.570069Z",
     "iopub.status.busy": "2025-08-19T08:31:27.569917Z",
     "iopub.status.idle": "2025-08-19T08:31:27.575848Z",
     "shell.execute_reply": "2025-08-19T08:31:27.575480Z",
     "shell.execute_reply.started": "2025-08-19T08:31:27.570056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting health_datacleaning.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile health_datacleaning.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input-path', type=str, help=\"Directory containing main dataset\")\n",
    "    parser.add_argument(\"--output-train-path\", type=str, help=\"Output directory for train.csv\")\n",
    "    parser.add_argument(\"--output-test-path\", type=str, help=\"Output directory for test.csv\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Use provided paths or fall back to SageMaker defaults\n",
    "    input_path = args.input_path\n",
    "    output_train_path = args.output_train_path\n",
    "    output_test_path = args.output_test_path\n",
    "\n",
    "    os.makedirs(output_train_path, exist_ok=True)\n",
    "    os.makedirs(output_test_path, exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    input_file = os.path.join(input_path, 'combined_ADLfalls_engineered.csv')\n",
    "    print(f\"Reading input file from {input_file}...\")\n",
    "    df2 = pd.read_csv(input_file)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # ======================\n",
    "    # Standardize activities\n",
    "    # ======================\n",
    "    print(\"Standardizing activities...\")\n",
    "    # Convert to lowercase for easier matching\n",
    "    df2['activity_label'] = df2['activity_label'].astype(str).str.lower()\n",
    "\n",
    "    # Define mapping function\n",
    "    def categorize_activity(label):\n",
    "        if 'eat' in label or 'meal' in label or 'drink' in label:\n",
    "            return 'Eat'\n",
    "        elif 'errand' in label or 'shop' in label or 'store' in label or 'grocer' in label:\n",
    "            return 'Errands'\n",
    "        elif 'run' in label or 'jog' in label or 'walk' in label or 'climb' in label or 'exercise' in label or  'workout' in label or 'cycle' in label or  'jump' in label:\n",
    "            return 'Exercise'\n",
    "        elif 'brush' in label or 'toilet' in label or 'shower' in label or 'hygiene' in label or 'groom' in label or 'bath' in label:\n",
    "            return 'Hygiene'\n",
    "        # The following labels are combined under 'Exercise':\n",
    "        # elif 'run' in label or 'jog' in label:\n",
    "        #     return 'Run'\n",
    "        # elif 'walk' in label:\n",
    "        #     return 'Walk'\n",
    "        # elif 'climb' in label:\n",
    "        #     return 'Climbing stairs'\n",
    "        # elif 'sit' in label:\n",
    "        #     return 'Sitting down'\n",
    "        # elif 'exercise' in label or  'workout' in label or 'cycle' in label or  'jump' in label:\n",
    "        #     return 'Other exercise'\n",
    "        elif 'game' in label or 'gaming' in label or 'play' in label or 'art' in label or 'tv' in label or 'read' in label or 'video' in label:\n",
    "            return 'Hobby'\n",
    "        elif 'clean' in label or 'chores' in label or 'housework' in label or 'dishes' in label or 'cook' in label:\n",
    "            return 'Chores'\n",
    "        elif 'relax' in label or 'chill' in label or 'rest' in label:\n",
    "            return 'Relax'\n",
    "        elif 'sleep' in label or 'lying' in label or 'lie' in label:\n",
    "            return 'Sleep'\n",
    "        elif 'social' in label or 'talk' in label or 'phone' in label or 'message' in label or 'text' in label:\n",
    "            return 'Socialize'\n",
    "        elif 'travel' in label or 'bus' in label or 'drive' in label or 'commute' in label: # or 'fly' in label\n",
    "            return 'Travel'\n",
    "        elif 'work' in label or 'school' in label or 'lab' in label or 'homework' in label or 'report' in label or 'meeting' in label or 'class' in label or 'research' in label or 'study' in label:\n",
    "            return 'Work'\n",
    "        elif 'fall' in label:\n",
    "            return 'Fall'\n",
    "        else:\n",
    "            return None # rows not in the above categories will be dropped\n",
    "\n",
    "\n",
    "    # Apply the categorization\n",
    "    df2['activity_label'] = df2['activity_label'].apply(categorize_activity)\n",
    "\n",
    "    # Drop rows where category is None (would have been 'Other')\n",
    "    df2 = df2[df2['activity_label'].notna()]\n",
    "\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # ===========================================\n",
    "    # Drop correlated features & weak predictors\n",
    "    # ===========================================\n",
    "    features_to_drop = [\n",
    "        # Highly correlated (redundant) features\n",
    "        'acceleration_diff_x_std',\n",
    "        'rotation_rate_z_std',\n",
    "        'acceleration_x_std',\n",
    "        'user_acceleration_x_std',\n",
    "        'acceleration_y_std',\n",
    "        'acceleration_diff_z_std',\n",
    "        'acceleration_diff_y_std',\n",
    "        'acceleration_z_std',\n",
    "        'rotation_rate_x_std',\n",
    "        'rotation_rate_y_std',\n",
    "        'acceleration_vec_sum_mean',\n",
    "        'acceleration_vec_sum_std',\n",
    "\n",
    "        # Very weak ANOVA predictors\n",
    "        'rotation_rate_diff_y_mean',\n",
    "        'user_id',\n",
    "        'magnetometer_diff_x_mean',\n",
    "        'acceleration_diff_y_mean',\n",
    "        'rotation_rate_diff_x_mean',\n",
    "        'rotation_rate_diff_z_mean',\n",
    "        'heart_rate_diff_x_mean'\n",
    "    ]\n",
    "\n",
    "    df3 = df2.drop(columns=features_to_drop)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # =================\n",
    "    # Splitting data\n",
    "    # =================\n",
    "    print(\"Splitting into train/test...\")\n",
    "    # Stratified train-test split (stratify by health_status)\n",
    "    df3 = df3.dropna(subset=['health_status'])  # can't stratify on NaNs in target (if any)\n",
    "    train_df, test_df = train_test_split(\n",
    "        df3,\n",
    "        test_size=0.2,\n",
    "        stratify=df3['health_status'],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Save splits\n",
    "    train_output = os.path.join(output_train_path, \"train.csv\")\n",
    "    test_output = os.path.join(output_test_path, \"test.csv\")\n",
    "\n",
    "    print(f\"Saving train to {train_output}\")\n",
    "    train_df.to_csv(train_output, index=False)\n",
    "\n",
    "    print(f\"Saving test to {test_output}\")\n",
    "    test_df.to_csv(test_output, index=False)\n",
    "\n",
    "    print(\"Data cleaning complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e62d56b-3fc8-4151-99cd-da1d50b4d05b",
   "metadata": {},
   "source": [
    "#### 2. Preprocessor (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c99d8c87-5218-4c4c-a172-555bfa0168d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T09:43:27.310590Z",
     "iopub.status.busy": "2025-08-19T09:43:27.310129Z",
     "iopub.status.idle": "2025-08-19T09:43:27.316232Z",
     "shell.execute_reply": "2025-08-19T09:43:27.315855Z",
     "shell.execute_reply.started": "2025-08-19T09:43:27.310571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting health_preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile health_preprocessor.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# ---------------------\n",
    "# Include GroupImputer\n",
    "# ---------------------\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GroupImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_cols, target_cols, strategy='mean'): # mean for numerical data\n",
    "        self.group_cols = group_cols\n",
    "        self.target_cols = target_cols\n",
    "        self.strategy = strategy\n",
    "        self.impute_values_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"Fitting group imputer.\")\n",
    "\n",
    "        # If no target_cols passed, infer them\n",
    "        if self.target_cols is None:\n",
    "            self.target_cols = [\n",
    "                col for col in X.columns\n",
    "                if col not in self.group_cols and pd.api.types.is_numeric_dtype(X[col])\n",
    "            ]\n",
    "            \n",
    "        self.impute_values_ = (\n",
    "            X.groupby(self.group_cols)[self.target_cols]\n",
    "              .agg(self.strategy)\n",
    "              .reset_index()\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.target_cols:\n",
    "            merged = X_copy[self.group_cols].merge(\n",
    "                self.impute_values_[[*self.group_cols, col]],\n",
    "                on=self.group_cols,\n",
    "                how=\"left\"\n",
    "            )[col]\n",
    "            # First fill with group mean\n",
    "            X_copy[col] = X_copy[col].fillna(merged)\n",
    "            # Then fallback to global mean if still NaN\n",
    "            if X_copy[col].isna().any():\n",
    "                X_copy[col] = X_copy[col].fillna(X_copy[col].mean())\n",
    "        return X_copy\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return input_features\n",
    "        \n",
    "class DropActivityLabel(BaseEstimator, TransformerMixin):\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "        def transform(self, X):\n",
    "            return X.drop(columns=[\"activity_label\"])\n",
    "        def get_feature_names_out(self, input_features=None):\n",
    "            if input_features is None:\n",
    "                return None\n",
    "            return [f for f in input_features if f != \"activity_label\"]\n",
    "# -----------\n",
    "# Main script\n",
    "# ------------\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input-train-path', type=str, help=\"Previous output directory for train.csv\")\n",
    "    parser.add_argument('--output-train-path', type=str, help=\"Output directory for processed_train.csv\")\n",
    "    parser.add_argument('--input-test-path', type=str, help=\"Previous output directory for test.csv\")\n",
    "    parser.add_argument('--output-test-path', type=str, help=\"Output directory for processed_test.csv\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    input_train_path = os.path.join(args.input_train_path, \"train.csv\")\n",
    "    output_train_path = args.output_train_path\n",
    "    input_test_path = os.path.join(args.input_test_path, \"test.csv\")\n",
    "    output_test_path = args.output_test_path\n",
    "\n",
    "    # Make output directory\n",
    "    os.makedirs(output_train_path, exist_ok=True)\n",
    "    os.makedirs(output_test_path, exist_ok=True)\n",
    "\n",
    "    # Load train and test data\n",
    "  \n",
    "    if not os.path.exists(input_train_path):\n",
    "        raise FileNotFoundError(f\"Train data not found: {input_train_path}\")\n",
    "    print(f\"Reading input file from {input_train_path}...\")\n",
    "    train_df = pd.read_csv(input_train_path)\n",
    "\n",
    "    if not os.path.exists(input_test_path):\n",
    "        raise FileNotFoundError(f\"Test data not found: {input_test_path}\")\n",
    "    print(f\"Reading input file from {input_test_path}...\")\n",
    "    test_df = pd.read_csv(input_test_path)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Balancing imbalanced data\n",
    "    # ---------------------------\n",
    "    print('Balancing imbalanced data...')\n",
    "    X_train = train_df.drop(columns=['health_status'])\n",
    "    y_train = train_df['health_status']\n",
    "    y_train = y_train.map({'Healthy': 0, 'Unhealthy': 1})\n",
    "\n",
    "    # Combine training features and target for resampling\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    # Separate majority and minority classes in the training set only\n",
    "    train_majority = train_df[train_df.health_status == 0]\n",
    "    train_minority = train_df[train_df.health_status == 1]\n",
    "\n",
    "    len_maj = len(train_majority)\n",
    "    len_min = len(train_minority)\n",
    "    n_samples_each = round((len_maj+len_min)/2)\n",
    "    print(n_samples_each)\n",
    "\n",
    "    # Upsample minority class in the training set\n",
    "    train_minority_upsampled = resample(train_minority, \n",
    "                                        replace=True,    # sample with replacement\n",
    "                                        n_samples=n_samples_each,  # number of samples in upsampled minority class (reduced due to resource constraints)\n",
    "                                        random_state=42) # reproducible results\n",
    "\n",
    "    # Downsample majority class in the training set\n",
    "    train_majority_downsampled = resample(train_majority, \n",
    "                                        replace=False,    # sample without replacement\n",
    "                                        n_samples=n_samples_each,   # number of samples in downsampled majority class (reduced due to resource constraints)\n",
    "                                        random_state=42)  # reproducible results\n",
    "\n",
    "    # Combine upsampled minority class with downsampled majority class\n",
    "    train_combined = pd.concat([train_majority_downsampled, train_minority_upsampled])\n",
    "\n",
    "    # Shuffle and reset index\n",
    "    train_combined = train_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Re-separate the features and target in the combined training set\n",
    "    X_train = train_combined.drop(columns='health_status')\n",
    "    y_train = train_combined['health_status']\n",
    "\n",
    "    # Only separate the features and target in the combined test set (without resampling)\n",
    "    X_test = test_df.drop(columns='health_status')\n",
    "    y_test = test_df['health_status']\n",
    "    y_test = y_test.map({'Healthy': 0, 'Unhealthy': 1})\n",
    "    \n",
    "    # ---------------------\n",
    "    # Define pipeline columns\n",
    "    # ---------------------\n",
    "    nominal_cols_encode = ['activity_label', 'age_group']\n",
    "    numerical_cols_impute_scale = ['heart_rate_x_std', 'heart_rate_diff_x_std']\n",
    "    numerical_cols_scale = X_train.select_dtypes(include='number').drop(columns=numerical_cols_impute_scale).columns.tolist()\n",
    "\n",
    "    # ---------------------\n",
    "    # Build pipelines\n",
    "    # ---------------------\n",
    "        \n",
    "    nominal_onehot_pipe = Pipeline([\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    numerical_impute_scale_pipe = Pipeline([\n",
    "        (\"group_imputer\", GroupImputer(\n",
    "            group_cols=[\"activity_label\"],   # grouping columns\n",
    "            target_cols=None,  # columns to impute - impute all\n",
    "            strategy=\"mean\"   # strategy: mean for numeric\n",
    "        )),\n",
    "        (\"drop_activity_label\", DropActivityLabel()),\n",
    "        ('scaler', StandardScaler())\n",
    "        \n",
    "    ])\n",
    "\n",
    "    numerical_scale_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('nominal_encode', nominal_onehot_pipe, nominal_cols_encode),\n",
    "        ('num_impute_scale', numerical_impute_scale_pipe, numerical_cols_impute_scale + ['activity_label']),\n",
    "        ('num_scale', numerical_scale_pipe, numerical_cols_scale)\n",
    "    ], remainder='passthrough', verbose_feature_names_out=False)\n",
    "\n",
    "    # ---------------------\n",
    "    # Fit on train, transform train/test\n",
    "    # ---------------------\n",
    "    print(\"Train columns:\", X_train.columns.tolist())\n",
    "    print(\"Test columns:\", X_test.columns.tolist())\n",
    "    X_train_trf = preprocessor.fit_transform(X_train)\n",
    "    X_test_trf = preprocessor.transform(X_test)\n",
    "\n",
    "    # Convert back to DataFrame with feature names\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "    X_train_transformed = pd.DataFrame(X_train_trf, columns=feature_names)\n",
    "    X_test_transformed = pd.DataFrame(X_test_trf, columns=feature_names)\n",
    "\n",
    "    # Add back Y\n",
    "    train_transformed = pd.concat([X_train_transformed, y_train], axis=1)\n",
    "    test_transformed = pd.concat([X_test_transformed, y_test], axis=1)\n",
    "    \n",
    "    # ---------------------\n",
    "    # Save transformed datasets\n",
    "    # ---------------------\n",
    "    train_output_file = os.path.join(output_train_path, \"train_processed.csv\")\n",
    "    test_output_file = os.path.join(output_test_path, \"test_processed.csv\")\n",
    "\n",
    "    train_transformed.to_csv(train_output_file, index=False)\n",
    "    test_transformed.to_csv(test_output_file, index=False)\n",
    "\n",
    "    print(f\"Processed train saved to {train_output_file}\")\n",
    "    print(f\"Processed test saved to {test_output_file}\")\n",
    "\n",
    "    print(\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4babc3-1ed9-4644-9478-e7ccae9f99ee",
   "metadata": {},
   "source": [
    "#### 3. Training (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75bc2699-db86-4f29-8789-c9ec3632054b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:31:27.583459Z",
     "iopub.status.busy": "2025-08-19T08:31:27.583266Z",
     "iopub.status.idle": "2025-08-19T08:31:27.588049Z",
     "shell.execute_reply": "2025-08-19T08:31:27.587694Z",
     "shell.execute_reply.started": "2025-08-19T08:31:27.583446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting health_requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile health_requirements.txt\n",
    "boto3==1.28.57\n",
    "botocore==1.31.85\n",
    "\n",
    "mlflow\n",
    "sagemaker-mlflow\n",
    "scikit-learn\n",
    "pandas\n",
    "joblib\n",
    "numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d48e270-46cd-4f7e-bd40-43990e0a513b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:31:27.589591Z",
     "iopub.status.busy": "2025-08-19T08:31:27.589350Z",
     "iopub.status.idle": "2025-08-19T08:31:27.593091Z",
     "shell.execute_reply": "2025-08-19T08:31:27.592712Z",
     "shell.execute_reply.started": "2025-08-19T08:31:27.589577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting health_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile health_train.py\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# # Ensure MLflow is installed\n",
    "try:\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow\n",
    "except ImportError:\n",
    "    print(\"Installing MLflow...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",  \"boto3==1.37.1\", \"botocore==1.37.1\", \"s3transfer\", \"mlflow==2.22.0\", \"sagemaker-mlflow==0.1.0\"])\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow\n",
    "    \n",
    "# import mlflow\n",
    "# import sagemaker_mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--tracking-server-arn\", type=str, required=True)\n",
    "    parser.add_argument(\"--experiment-name\", type=str, default=\"Default\")\n",
    "    parser.add_argument(\"--model-output-path\", type=str, default=\"/opt/ml/model\", help=\"Output directory for model.joblib\")\n",
    "    parser.add_argument(\"-C\", \"--C\", type=float, default=0.5)\n",
    "    # parser.add_argument(\"--penalty\", type=str, default=\"l2\")\n",
    "    # parser.add_argument(\"--class-weight\", type=str, default=\"none\")  \n",
    "    # parser.add_argument(\"--solver\", type=str, default=\"lbfgs\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # # Convert string \"none\" to Python None\n",
    "    # if args.class_weight.lower() == \"none\":\n",
    "    #     class_weight = None\n",
    "    # else:\n",
    "    #     class_weight = args.class_weight\n",
    "\n",
    "\n",
    "    # Make output directory\n",
    "    model_output_path = args.model_output_path or \"/opt/ml/model\"\n",
    "    os.makedirs(model_output_path, exist_ok=True)\n",
    "    \n",
    "    # Load training data\n",
    "    input_train = glob.glob(\"/opt/ml/input/data/train/train_processed.csv\")[0] # first train_processed.csv\n",
    "    if not os.path.exists(input_train):\n",
    "        raise FileNotFoundError(f\"Processed train data not found: {input_train}\")\n",
    "    df = pd.read_csv(input_train)\n",
    "    X = df.drop(columns='health_status')\n",
    "    y = df['health_status']\n",
    "\n",
    "    # Set up MLflow\n",
    "    mlflow.set_tracking_uri(args.tracking_server_arn)\n",
    "    print(\"MLflow tracking URI set successfully.\")\n",
    "    mlflow.set_experiment(args.experiment_name)\n",
    "    print(f\"MLflow tracking URI set to: {mlflow.get_tracking_uri()}\")\n",
    "    print(f\"MLflow experiment set to: '{args.experiment_name}'\")\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"C\", args.C)\n",
    "        model = LogisticRegression(\n",
    "            C=args.C,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        f1_macro = f1_score(y, y_pred, average=\"macro\")\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"f1_macro\", f1_macro)\n",
    "    \n",
    "        mlflow.sklearn.log_model(sk_model=model, artifact_path=\"model\")\n",
    "    \n",
    "        joblib.dump(model, os.path.join(args.model_output_path, \"model.joblib\"))\n",
    "        with open(os.path.join(args.model_output_path, \"run_id.txt\"), \"w\") as f:\n",
    "            f.write(run.info.run_id)\n",
    "    \n",
    "        print(f\"Training complete. Accuracy: {acc:.4f}\")\n",
    "        print(f\"MLflow Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265de90-4f9b-4c63-bb0f-c0ee7229fe1b",
   "metadata": {},
   "source": [
    "#### 4. Evaluation (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01279a12-9585-47c7-ba68-c623b38adfb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T08:31:27.593838Z",
     "iopub.status.busy": "2025-08-19T08:31:27.593596Z",
     "iopub.status.idle": "2025-08-19T08:31:27.599234Z",
     "shell.execute_reply": "2025-08-19T08:31:27.598851Z",
     "shell.execute_reply.started": "2025-08-19T08:31:27.593825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting health_evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile health_evaluate.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import tarfile\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Parse Arguments ---\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model-path\", type=str, required=True, help=\"Previous output directory for model.joblib.\")\n",
    "    parser.add_argument(\"--input-test-path\", type=str, required=True, help=\"Previous output directory for test_processed.csv\")\n",
    "    parser.add_argument(\"--output-path\", type=str, required=True, help=\"Path to save the evaluation.json report.\")\n",
    "    parser.add_argument(\"--model-package-group-name\", type=str, required=True, help=\"Name of the SageMaker Model Package Group.\")\n",
    "    parser.add_argument(\"--region\", type=str, required=True, help=\"The AWS region for creating the boto3 client.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # --- Extract and Load Model ---\n",
    "    # SageMaker packages models in a .tar.gz file. We need to extract it first.\n",
    "    model_archive_path = os.path.join(args.model_path, 'model.tar.gz')\n",
    "    print(f\"Extracting model from archive: {model_archive_path}\")\n",
    "    with tarfile.open(model_archive_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=args.model_path)\n",
    "\n",
    "    # Load the model using joblib\n",
    "    model_file_path = os.path.join(args.model_path, \"model.joblib\")\n",
    "    if not os.path.exists(model_file_path):\n",
    "        raise FileNotFoundError(f\"Model file 'model.joblib' not found after extraction in: {args.model_path}\")\n",
    "    \n",
    "    print(f\"Loading model from: {model_file_path}\")\n",
    "    model = joblib.load(model_file_path)\n",
    "\n",
    "    # Load test data\n",
    "    input_test_path = args.input_test_path\n",
    "    input_test = os.path.join(input_test_path, \"test_processed.csv\")\n",
    "    if not os.path.exists(input_test):\n",
    "        raise FileNotFoundError(f\"Processed test data not found: {input_test}\")\n",
    "    df = pd.read_csv(input_test)\n",
    "    X_test = df.drop(columns='health_status')\n",
    "    y_test = df['health_status']\n",
    "\n",
    "    print(\"Running predictions on the test dataset.\")\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    f1_macro = f1_score(y_test, y_test_pred, average=\"macro\")\n",
    "    report = {\"accuracy\": accuracy,\n",
    "             \"f1\": f1_macro} # labelled as 'f1' instead of 'f1_macro' in case of change\n",
    "    print(f\"Calculated accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Calculated f1_macro: {f1_macro:.4f}\")\n",
    "\n",
    "    # --- Check for Existing Baseline Model in SageMaker Model Registry ---\n",
    "    print(f\"Checking for baseline model in region: {args.region}\")\n",
    "    sagemaker_client = boto3.client(\"sagemaker\", region_name=args.region)\n",
    "    try:\n",
    "        response = sagemaker_client.list_model_packages(\n",
    "            ModelPackageGroupName=args.model_package_group_name, \n",
    "            ModelApprovalStatus=\"Approved\", # filter for approved models only\n",
    "            SortBy=\"CreationTime\",\n",
    "            SortOrder=\"Descending\",\n",
    "            MaxResults=1,\n",
    "        )\n",
    "        # If the list is not empty, an approved model already exists\n",
    "        report[\"baseline_exists\"] = len(response[\"ModelPackageSummaryList\"]) > 0 # should include all models regardless of approval status\n",
    "        if report[\"baseline_exists\"]:\n",
    "            print(f\"An approved baseline model was found in '{args.model_package_group_name}'.\")\n",
    "        else:\n",
    "             print(f\"No approved baseline model was found in '{args.model_package_group_name}'.\")\n",
    "\n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        # If the ModelPackageGroup doesn't exist, there is no baseline\n",
    "        if \"ResourceNotFound\" in str(e):\n",
    "            report[\"baseline_exists\"] = False\n",
    "            print(f\"Model Package Group '{args.model_package_group_name}' not found. Assuming no baseline exists.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # --- Write Final Report ---\n",
    "    os.makedirs(args.output_path, exist_ok=True)\n",
    "    report_path = os.path.join(args.output_path, \"evaluation.json\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "        \n",
    "    print(f\"✅ Evaluation complete. Report written to: {report_path}\")\n",
    "    print(\"Evaluation Report:\")\n",
    "    print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b086e-e1af-47b4-bca7-eff8cfaca6ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T06:32:40.642926Z",
     "iopub.status.busy": "2025-07-24T06:32:40.642324Z",
     "iopub.status.idle": "2025-07-24T06:32:41.318403Z",
     "shell.execute_reply": "2025-07-24T06:32:41.317486Z",
     "shell.execute_reply.started": "2025-07-24T06:32:40.642900Z"
    }
   },
   "source": [
    "## Define the SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd78c79-c25a-4cee-bcef-7efc35dffce0",
   "metadata": {},
   "source": [
    "#### Pipeline settings & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d755f65-e06c-4ad1-9f9f-8f04ca4ff38e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T09:43:32.663213Z",
     "iopub.status.busy": "2025-08-19T09:43:32.662910Z",
     "iopub.status.idle": "2025-08-19T09:43:32.684185Z",
     "shell.execute_reply": "2025-08-19T09:43:32.683779Z",
     "shell.execute_reply.started": "2025-08-19T09:43:32.663194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current SageMaker Execution Role ARN is:\n",
      "arn:aws:iam::837028399719:role/iti113-team12-sagemaker-iti113-team12-domain-iti113-team12-Role\n"
     ]
    }
   ],
   "source": [
    "# Get the role from the current session\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "print(\"Your current SageMaker Execution Role ARN is:\")\n",
    "print(sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1927104-6c34-47b3-919d-3e3b82581ecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T09:43:32.799488Z",
     "iopub.status.busy": "2025-08-19T09:43:32.799136Z",
     "iopub.status.idle": "2025-08-19T09:43:32.802017Z",
     "shell.execute_reply": "2025-08-19T09:43:32.801617Z",
     "shell.execute_reply.started": "2025-08-19T09:43:32.799473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying data path:\n",
      "s3://iti113-team12-bucket/project/datasets/adl\n"
     ]
    }
   ],
   "source": [
    "# Same as defined above\n",
    "# data_path = f\"s3://{bucket_name}/{base_folder}/datasets/adl\"\n",
    "print(\"Verifying data path:\")\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4df01e03-eb61-46c1-a529-87c9297f85b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T09:43:32.948103Z",
     "iopub.status.busy": "2025-08-19T09:43:32.947639Z",
     "iopub.status.idle": "2025-08-19T09:43:32.985724Z",
     "shell.execute_reply": "2025-08-19T09:43:32.985311Z",
     "shell.execute_reply.started": "2025-08-19T09:43:32.948080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current SageMaker Execution Role ARN is: arn:aws:iam::837028399719:role/iti113-team12-sagemaker-iti113-team12-domain-iti113-team12-Role\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "import sagemaker\n",
    "\n",
    "# Get SageMaker role and session\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "print(f\"Your current SageMaker Execution Role ARN is: {sagemaker_role}\")\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Parameters\n",
    "model_package_group_name = \"healthStatusModels\"\n",
    "processing_instance_type = \"ml.m5.large\" #\"ml.t3.medium\"\n",
    "training_instance_type = \"ml.m5.large\"\n",
    "experiment_name_param = ParameterString(name=\"ExperimentName\", default_value=\"c-healthStatusClf\")\n",
    "accuracy_threshold_param = ParameterFloat(name=\"AccuracyThreshold\", default_value=0.5)\n",
    "f1_threshold_param = ParameterFloat(name=\"F1Threshold\", default_value = 0.5) # low for now\n",
    "\n",
    "\n",
    "# Hyperparameters for logistic regression\n",
    "C_param = ParameterFloat(name=\"C\", default_value=0.5)\n",
    "# penalty_param = ParameterString(name=\"penalty\", default_value=\"l2\")\n",
    "# class_weight_param = ParameterString(name=\"class_weight\", default_value=\"none\")\n",
    "# solver_param = ParameterString(name=\"solver\", default_value=\"lbfgs\")\n",
    "\n",
    "# Define input/output S3 URIs as pipeline parameters\n",
    "# data_path = f\"s3://{bucket_name}/{base_folder}/datasets/adl\"\n",
    "# data_path_adl = \"s3://iti113-team12-bucket/project/datasets/adl/adl/\"\n",
    "# data_path_fall = \"s3://iti113-team12-bucket/project/datasets/adl/fall/\"\n",
    "# data_path_adlfalls_users = \"s3://iti113-team12-bucket/project/datasets/adl/users/\"\n",
    "# dataset_s3_path = f\"{data_path}/users/volunteer_details.csv\"\n",
    "input_data_uri = ParameterString(\n",
    "    name=\"InputDataURI\", \n",
    "    default_value=f\"{data_path}/combined_ADLfalls_data.csv\",\n",
    ")\n",
    "user_data_uri = ParameterString(\n",
    "    name=\"UserDataURI\", \n",
    "    default_value=f\"{data_path}/users/volunteer_details.csv\",\n",
    ")\n",
    "engr_data_uri = ParameterString(\n",
    "    name=\"EngrDataURI\", \n",
    "    default_value=f\"{data_path}/combined_ADLfalls_engineered.csv\",\n",
    ")\n",
    "train_data_uri = ParameterString(\n",
    "    name=\"TrainDataURI\", \n",
    "    default_value=f\"{data_path}/preprocessed-data/train\" # S3 location\n",
    ")\n",
    "test_data_uri = ParameterString(\n",
    "    name=\"TestDataURI\", \n",
    "    default_value=f\"{data_path}/preprocessed-data/test\" \n",
    ")\n",
    "processed_train_data_uri = ParameterString(\n",
    "    name=\"ProcessedTrainDataURI\", \n",
    "    default_value=f\"{data_path}/preprocessed-data/train\"\n",
    ")\n",
    "processed_test_data_uri = ParameterString(\n",
    "    name=\"ProcessedTestDataURI\", \n",
    "    default_value=f\"{data_path}/preprocessed-data/test\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af1543",
   "metadata": {},
   "source": [
    "#### Define the feature engineering step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c071341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T09:43:33.228303Z",
     "iopub.status.busy": "2025-08-19T09:43:33.228018Z",
     "iopub.status.idle": "2025-08-19T09:43:33.249652Z",
     "shell.execute_reply": "2025-08-19T09:43:33.249248Z",
     "shell.execute_reply.started": "2025-08-19T09:43:33.228284Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "# Define the SKLearnProcessor\n",
    "featureEngr = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",  \n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"feature-engr\",\n",
    "    role=sagemaker_role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Define the feature engineering step\n",
    "featureEng_step = ProcessingStep(\n",
    "    name=f\"featureEngrStep-{int(time.time())}\",\n",
    "    processor=featureEngr,\n",
    "    code=\"health_featureEngr.py\", # Script was created in previous step\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name=\"input-path\",\n",
    "            source=input_data_uri,\n",
    "            destination=\"/opt/ml/processing/input\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            input_name=\"user-input-path\",\n",
    "            source=user_data_uri,\n",
    "            destination=\"/opt/ml/processing/input/user\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"output-path\",\n",
    "            source=\"/opt/ml/processing/output\", # local\n",
    "            destination=engr_data_uri # S3\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--input-path\", \"/opt/ml/processing/input\",\n",
    "        \"--user-input-path\", \"/opt/ml/processing/input/user\",\n",
    "        \"--output-path\", \"/opt/ml/processing/output\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9ebb7-a06e-4908-ad8d-82e8a7e27974",
   "metadata": {},
   "source": [
    "#### Define the data cleaning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d032ef26-3929-4d90-8e93-f7d2e3ea2253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T09:43:33.526559Z",
     "iopub.status.busy": "2025-08-19T09:43:33.526291Z",
     "iopub.status.idle": "2025-08-19T09:43:33.546212Z",
     "shell.execute_reply": "2025-08-19T09:43:33.545804Z",
     "shell.execute_reply.started": "2025-08-19T09:43:33.526542Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "# Define the SKLearnProcessor\n",
    "datacleaning = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",  \n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"cleaning-data\",\n",
    "    role=sagemaker_role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Define the data cleaning step\n",
    "datacleaning_step = ProcessingStep(\n",
    "    name=f\"CleaningStep-{int(time.time())}\",\n",
    "    processor=datacleaning,\n",
    "    code=\"health_datacleaning.py\", # Script was created in previous step\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name=\"input-path\",\n",
    "            source=featureEng_step.properties.ProcessingOutputConfig.Outputs[\"output-path\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/input\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"output-train-path\",\n",
    "            source=\"/opt/ml/processing/output/train\", # local\n",
    "            destination=train_data_uri # S3\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"output-test-path\",\n",
    "            source=\"/opt/ml/processing/output/test\", # local\n",
    "            destination=test_data_uri # S3\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--input-path\", \"/opt/ml/processing/input\",\n",
    "        \"--output-train-path\", \"/opt/ml/processing/output/train\",\n",
    "        \"--output-test-path\", \"/opt/ml/processing/output/test\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534bfd6-8d1c-49d4-9f4f-a1143378abbc",
   "metadata": {},
   "source": [
    "#### Define the preprocessing step (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42372c20-1182-4891-9971-22e656476ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T09:43:33.870386Z",
     "iopub.status.busy": "2025-08-19T09:43:33.869927Z",
     "iopub.status.idle": "2025-08-19T09:43:33.889433Z",
     "shell.execute_reply": "2025-08-19T09:43:33.889040Z",
     "shell.execute_reply.started": "2025-08-19T09:43:33.870368Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "# Define the SKLearnProcessor\n",
    "preprocessing = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",  \n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"preprocessing\",\n",
    "    role=sagemaker_role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Define the processing step\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=f\"PreprocessingStep-{int(time.time())}\",\n",
    "    processor=preprocessing,\n",
    "    code=\"health_preprocessor.py\", # Script was created in previous step\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name=\"input-train-path\",\n",
    "            source=datacleaning_step.properties.ProcessingOutputConfig.Outputs[\"output-train-path\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/input/train\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            input_name=\"input-test-path\",\n",
    "            source=datacleaning_step.properties.ProcessingOutputConfig.Outputs[\"output-test-path\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/input/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"output-train-path\",\n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            destination=processed_train_data_uri\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"output-test-path\",\n",
    "            source=\"/opt/ml/processing/output/test\",\n",
    "            destination=processed_test_data_uri\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--input-train-path\", \"/opt/ml/processing/input/train\",\n",
    "        \"--input-test-path\", \"/opt/ml/processing/input/test\",\n",
    "        \"--output-train-path\", \"/opt/ml/processing/output/train\",\n",
    "        \"--output-test-path\", \"/opt/ml/processing/output/test\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f0359-a0da-4a65-b6e8-0d8d79961990",
   "metadata": {},
   "source": [
    "#### Define the training step (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f53ff74-5f4e-43f7-8773-b04cf9f6ac90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T09:43:34.207497Z",
     "iopub.status.busy": "2025-08-19T09:43:34.207214Z",
     "iopub.status.idle": "2025-08-19T09:43:34.248184Z",
     "shell.execute_reply": "2025-08-19T09:43:34.247712Z",
     "shell.execute_reply.started": "2025-08-19T09:43:34.207480Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.workflow.steps import TrainingStep, TrainingInput\n",
    "\n",
    "# Training Step\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"health_train.py\", \n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=training_instance_type,\n",
    "    role=sagemaker_role,\n",
    "    hyperparameters={\n",
    "        \"tracking-server-arn\": mlflow_tracking_server_arn,\n",
    "        \"experiment-name\": experiment_name_param,\n",
    "        \"model-output-path\": \"/opt/ml/model\", # for saving joblib files locally\n",
    "        \"C\": C_param, # used parameter to vary it in pipeline\n",
    "        # \"penalty\": penalty_param,\n",
    "        # \"class-weight\": class_weight_param,\n",
    "        # \"solver\": solver_param\n",
    "    },\n",
    "    py_version=\"py3\",\n",
    "    requirements=\"requirements.txt\" \n",
    ")\n",
    "\n",
    "train_step = TrainingStep(\n",
    "    name=f\"TrainModel-{int(time.time())}\",\n",
    "    estimator=sklearn_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"output-train-path\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "            input_mode=\"File\",  # ensure it's mounted as a file\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "# parser.add_argument(\"--tracking-server-arn\", type=str, required=True)\n",
    "# parser.add_argument(\"--experiment-name\", type=str, default=\"Default\")\n",
    "# parser.add_argument('--input-train-path', type=str, help=\"Previous output directory for train_processed.csv\")\n",
    "# parser.add_argument(\"--model-output-path\", type=str, default=\"/opt/ml/model\", help=\"Output directory for model.joblib\")\n",
    "# parser.add_argument(\"-C\", \"--C\", type=float, default=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b888c7-a9a2-47ab-9565-0879e048639b",
   "metadata": {},
   "source": [
    "#### Define the Evalution step (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58178a7e-71d3-4c11-9e86-6c228a3ec87c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T09:43:34.698213Z",
     "iopub.status.busy": "2025-08-19T09:43:34.697927Z",
     "iopub.status.idle": "2025-08-19T09:43:34.735711Z",
     "shell.execute_reply": "2025-08-19T09:43:34.735302Z",
     "shell.execute_reply.started": "2025-08-19T09:43:34.698195Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.model_metrics import ModelMetrics, FileSource\n",
    "\n",
    "evaluation_processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"sklearn\", sagemaker_session.boto_region_name, \"1.2-1\"),\n",
    "    command=['python3'],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"evaluate-model\",\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name=f\"EvaluateModel-{int(time.time())}\",\n",
    "    processor=evaluation_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"output-test-path\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\")],\n",
    "    code=\"health_evaluate.py\",  # SageMaker will handle uploading and running this script\n",
    "    job_arguments=[  # Pass arguments here instead of in command\n",
    "        \"--model-path\", \"/opt/ml/processing/model\",\n",
    "        \"--input-test-path\", \"/opt/ml/processing/test\",\n",
    "        \"--output-path\", \"/opt/ml/processing/evaluation\",\n",
    "        \"--model-package-group-name\", model_package_group_name,\n",
    "        \"--region\", \"ap-southeast-1\",\n",
    "    ],\n",
    "    property_files=[evaluation_report],\n",
    "    depends_on=[train_step]\n",
    ")\n",
    "\n",
    "# parser.add_argument(\"--model-path\", type=str, required=True, help=\"Previous output directory for model.joblib.\")\n",
    "# parser.add_argument(\"--input-test-path\", type=str, required=True, help=\"Previous output directory for test_processed.csv\")\n",
    "# parser.add_argument(\"--output-path\", type=str, required=True, help=\"Path to save the evaluation.json report.\")\n",
    "# parser.add_argument(\"--model-package-group-name\", type=str, required=True, help=\"Name of the SageMaker Model Package Group.\")\n",
    "# parser.add_argument(\"--region\", type=str, required=True, help=\"The AWS region for creating the boto3 client.\")\n",
    "    \n",
    "\n",
    "model_metrics_report = ModelMetrics(\n",
    "    model_statistics=FileSource(\n",
    "        s3_uri=evaluation_step.properties.ProcessingOutputConfig.Outputs[\"evaluation\"].S3Output.S3Uri,\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78335c47-b9ac-405a-a219-bc827779bb07",
   "metadata": {},
   "source": [
    "#### Define the Model registration step (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a3e9786-f42b-4de2-b463-c825365feae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T09:43:35.371298Z",
     "iopub.status.busy": "2025-08-19T09:43:35.370998Z",
     "iopub.status.idle": "2025-08-19T09:43:35.377116Z",
     "shell.execute_reply": "2025-08-19T09:43:35.376697Z",
     "shell.execute_reply.started": "2025-08-19T09:43:35.371279Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.conditions import ConditionNot\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionEquals\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "# RegisterModel step (always defined, but executed conditionally)\n",
    "step_register_new = RegisterModel(\n",
    "    name=\"RegisterNewModel\",\n",
    "    estimator=sklearn_estimator,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    model_metrics=model_metrics_report,\n",
    "    approval_status=\"Approved\" # changed from \"PendingManualApproval\" since passed threshold\n",
    ")\n",
    "\n",
    "step_register_better_model = RegisterModel(\n",
    "    name=\"RegisterBetterModel\",\n",
    "    estimator=sklearn_estimator,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    model_metrics=model_metrics_report,\n",
    "    approval_status=\"Approved\" # changed from \"PendingManualApproval\" since passed threshold\n",
    ")\n",
    "\n",
    "\n",
    "# Conditions: check accuracy & f1 > threshold OR no model exists\n",
    "cond_accuracy = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"accuracy\"\n",
    "    ),\n",
    "    right=accuracy_threshold_param\n",
    ")\n",
    "\n",
    "cond_f1 = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"f1\"\n",
    "    ),\n",
    "    right=f1_threshold_param\n",
    ")\n",
    "\n",
    "cond_no_registered = ConditionEquals(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"baseline_exists\" # Check the key added to the report\n",
    "    ),\n",
    "    right=False # Condition is TRUE if baseline_exists is False\n",
    ")\n",
    "\n",
    "# Outer step: Checks for existence of registered model first\n",
    "step_cond_metrics = ConditionStep(\n",
    "    name=\"CheckAccuracyAndF1\",\n",
    "    conditions=[\n",
    "        cond_accuracy,\n",
    "        cond_f1,\n",
    "    ],\n",
    "    if_steps=[step_register_better_model], # Register model if both metrics pass\n",
    "    else_steps=[],\n",
    ")\n",
    "\n",
    "step_cond_no_registered = ConditionStep(\n",
    "    name=\"CheckIfNoModelExists\",\n",
    "    conditions=[cond_no_registered],\n",
    "    if_steps=[step_register_new], # Register model if no baseline exists\n",
    "    else_steps=[step_cond_metrics], # If model exists, check accuracy and f1\n",
    "    depends_on=[evaluation_step]\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a563330f-82dd-4496-ba31-ecee78495707",
   "metadata": {},
   "source": [
    "#### Define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b5f0579-bda6-493d-8e83-6bab8e162a6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T09:43:36.376802Z",
     "iopub.status.busy": "2025-08-19T09:43:36.376503Z",
     "iopub.status.idle": "2025-08-19T09:43:38.341869Z",
     "shell.execute_reply": "2025-08-19T09:43:38.341435Z",
     "shell.execute_reply.started": "2025-08-19T09:43:36.376784Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 'healthStatusPipeline' is defined and ready to be executed.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Define the Pipeline\n",
    "# This assembles our parameters and steps into a single workflow.\n",
    "pipeline_name = \"healthStatusPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[input_data_uri, \n",
    "                user_data_uri,\n",
    "                engr_data_uri,\n",
    "                train_data_uri,\n",
    "                test_data_uri,\n",
    "                processed_train_data_uri,\n",
    "                processed_test_data_uri,\n",
    "                experiment_name_param,\n",
    "                accuracy_threshold_param,\n",
    "                f1_threshold_param,\n",
    "                C_param],\n",
    "                # penalty_param,\n",
    "                # class_weight_param,\n",
    "                # solver_param],\n",
    "    steps=[featureEng_step,\n",
    "           datacleaning_step,\n",
    "           preprocessing_step,\n",
    "           train_step,\n",
    "           evaluation_step,\n",
    "           step_cond_no_registered] # Use the 'no registered model' check as the primary condition step\n",
    ")\n",
    "\n",
    "# Create or update the pipeline definition in your AWS account.\n",
    "pipeline.upsert(role_arn=sagemaker_role)\n",
    "\n",
    "print(f\"Pipeline '{pipeline_name}' is defined and ready to be executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ade4bb-f9d5-4042-b7fa-aace46705972",
   "metadata": {},
   "source": [
    "## Start Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f3237ec-2264-43db-977e-593be5de8971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T09:43:39.945394Z",
     "iopub.status.busy": "2025-08-19T09:43:39.945100Z",
     "iopub.status.idle": "2025-08-19T09:43:40.282617Z",
     "shell.execute_reply": "2025-08-19T09:43:40.282190Z",
     "shell.execute_reply.started": "2025-08-19T09:43:39.945375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting pipeline execution...\n",
      "--> Execution started! ARN: arn:aws:sagemaker:ap-southeast-1:837028399719:pipeline/healthStatusPipeline/execution/ap7hlhmh7pam\n",
      "{'PipelineArn': 'arn:aws:sagemaker:ap-southeast-1:837028399719:pipeline/healthStatusPipeline', 'PipelineExecutionArn': 'arn:aws:sagemaker:ap-southeast-1:837028399719:pipeline/healthStatusPipeline/execution/ap7hlhmh7pam', 'PipelineExecutionDisplayName': 'dataEngr-to-modelReg', 'PipelineExecutionStatus': 'Executing', 'CreationTime': datetime.datetime(2025, 8, 19, 9, 43, 40, 61000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2025, 8, 19, 9, 43, 40, 61000, tzinfo=tzlocal()), 'CreatedBy': {'IamIdentity': {'Arn': 'arn:aws:sts::837028399719:assumed-role/iti113-team12-sagemaker-iti113-team12-domain-iti113-team12-Role/SageMaker', 'PrincipalId': 'AROA4FYWHZJT3SRRDCPYT:SageMaker'}}, 'LastModifiedBy': {'IamIdentity': {'Arn': 'arn:aws:sts::837028399719:assumed-role/iti113-team12-sagemaker-iti113-team12-domain-iti113-team12-Role/SageMaker', 'PrincipalId': 'AROA4FYWHZJT3SRRDCPYT:SageMaker'}}, 'ResponseMetadata': {'RequestId': '85eff383-07d5-4ba2-927d-5ee08efd5df4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '85eff383-07d5-4ba2-927d-5ee08efd5df4', 'content-type': 'application/x-amz-json-1.1', 'content-length': '803', 'date': 'Tue, 19 Aug 2025 09:43:40 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# -------------------------\n",
    "# Running existing pipeline\n",
    "# -------------------------\n",
    "pipeline = Pipeline(name=\"healthStatusPipeline\")\n",
    "# bucket_name = 'iti113-team12-bucket'  # e.g., 'my-company-sagemaker-bucket'\n",
    "# base_folder = 'project'\n",
    "# data_path = f\"s3://{bucket_name}/{base_folder}/datasets/adl\"\n",
    "\n",
    "# Start the pipeline and pass the S3 path for v1.0 to our parameter\n",
    "print(\"\\nStarting pipeline execution...\")\n",
    "\n",
    "# Experiment with data\n",
    "execution = pipeline.start(\n",
    "    parameters={\n",
    "        \"InputDataURI\": f\"{data_path}/combined_ADLfalls_data.csv\",\n",
    "        \"UserDataURI\": f\"{data_path}/users/volunteer_details.csv\",\n",
    "        \"EngrDataURI\": f\"{data_path}/combined_ADLfalls_engineered.csv\",\n",
    "        \"TrainDataURI\": f\"{data_path}/preprocessed-data/train\",\n",
    "        \"TestDataURI\": f\"{data_path}/preprocessed-data/test\",\n",
    "        \"ProcessedTrainDataURI\": f\"{data_path}/preprocessed-data/train\",\n",
    "        \"ProcessedTestDataURI\": f\"{data_path}/preprocessed-data/test\",\n",
    "        \"C\": 0.5,\n",
    "        # \"penalty\": \"l1\", # l1 penalty is lasso, can put coefficients to 0\n",
    "        # \"class_weight\": \"balanced\", # target is imbalanced\n",
    "        # \"solver\": \"saga\" # required for l1 penalty + multinomial\n",
    "    },\n",
    "    execution_display_name=\"dataEngr-to-modelReg\"\n",
    ")\n",
    "print(f\"--> Execution started! ARN: {execution.arn}\")\n",
    "print(execution.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94519d76-fa52-476f-8a5a-19000e5a2080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T03:12:28.592955Z",
     "iopub.status.busy": "2025-07-30T03:12:28.592427Z",
     "iopub.status.idle": "2025-07-30T03:12:29.163014Z",
     "shell.execute_reply": "2025-07-30T03:12:29.162259Z",
     "shell.execute_reply.started": "2025-07-30T03:12:28.592929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "Pipeline Status: Succeeded\n",
      "Creation Time: 2025-07-30 02:30:38.031000+00:00\n",
      "Last Modified: 2025-07-30 02:49:11.783000+00:00\n",
      "Pipeline Status: Succeeded\n",
      "Creation Time: 2025-07-30 02:49:41.594000+00:00\n",
      "Last Modified: 2025-07-30 03:07:53.567000+00:00\n"
     ]
    }
   ],
   "source": [
    "sagemaker_client = sagemaker.Session().sagemaker_client\n",
    "\n",
    "# Get the details of the specific pipeline execution\n",
    "response = sagemaker_client.describe_pipeline_execution(\n",
    "    PipelineExecutionArn=execution_v1.arn\n",
    ")\n",
    "\n",
    "# Print the current status\n",
    "status = response['PipelineExecutionStatus']\n",
    "print(f\"Pipeline Status: {status}\")\n",
    "\n",
    "# You can also see the creation time and last modified time\n",
    "print(f\"Creation Time: {response['CreationTime']}\")\n",
    "if 'LastModifiedTime' in response:\n",
    "    print(f\"Last Modified: {response['LastModifiedTime']}\")\n",
    "\n",
    "# Get the details of the specific pipeline execution\n",
    "response = sagemaker_client.describe_pipeline_execution(\n",
    "    PipelineExecutionArn=execution_v2.arn\n",
    ")\n",
    "\n",
    "# Print the current status\n",
    "status = response['PipelineExecutionStatus']\n",
    "print(f\"Pipeline Status: {status}\")\n",
    "\n",
    "# You can also see the creation time and last modified time\n",
    "print(f\"Creation Time: {response['CreationTime']}\")\n",
    "if 'LastModifiedTime' in response:\n",
    "    print(f\"Last Modified: {response['LastModifiedTime']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec469f-36cd-418f-86e0-b5bf017ea16e",
   "metadata": {},
   "source": [
    "## Manual Deployment (unused)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d238e0-8a2a-4ca4-8e40-dd6b3c4acadd",
   "metadata": {},
   "source": [
    "#### 1. Identify best training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0f032ee-0a5f-4f54-b0a9-15ae5117853b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T03:12:36.540368Z",
     "iopub.status.busy": "2025-07-30T03:12:36.539877Z",
     "iopub.status.idle": "2025-07-30T03:12:37.177689Z",
     "shell.execute_reply": "2025-07-30T03:12:37.176996Z",
     "shell.execute_reply.started": "2025-07-30T03:12:36.540344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: <Experiment: artifact_location='s3://sagemaker-iti112-common/mlflow-1/106', creation_time=1753451874748, experiment_id='106', last_update_time=1753451874748, lifecycle_stage='active', name='8815v-PenguinPrediction', tags={}>\n",
      "Runs type: <class 'pandas.core.frame.DataFrame'>\n",
      "Best run: run_id                                            e844a036b919434eb7363bf3204d3b7e\n",
      "experiment_id                                                                  106\n",
      "status                                                                    FINISHED\n",
      "artifact_uri                     s3://sagemaker-iti112-common/mlflow-1/106/e844...\n",
      "start_time                                        2025-07-30 03:02:17.828000+00:00\n",
      "end_time                                          2025-07-30 03:02:21.510000+00:00\n",
      "metrics.accuracy                                                          0.992727\n",
      "metrics.f1_macro                                                          0.992483\n",
      "params.solver                                                                lbfgs\n",
      "params.class_weight                                                           None\n",
      "params.C                                                                       1.0\n",
      "params.penalty                                                                  l2\n",
      "tags.mlflow.user                                                              root\n",
      "tags.mlflow.log-model.history    [{\"run_id\": \"e844a036b919434eb7363bf3204d3b7e\"...\n",
      "tags.mlflow.runName                                                 clean-grub-770\n",
      "tags.mlflow.source.name                                                   train.py\n",
      "tags.mlflow.source.type                                                      LOCAL\n",
      "Name: 0, dtype: object\n",
      "Registering model from URI: runs:/e844a036b919434eb7363bf3204d3b7e/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'penguin-model-8815v' already exists. Creating a new version of this model...\n",
      "2025/07/30 03:12:37 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: penguin-model-8815v, version 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'penguin-model-8815v' registered with version: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '7' of model 'penguin-model-8815v'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_tracking_server_arn)\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(\"healthstatus_classifier\")\n",
    "runs = mlflow.search_runs(experiment.experiment_id)\n",
    "\n",
    "print(\"Experiment:\", experiment)\n",
    "\n",
    "# Find the run with the best metric (e.g., highest f1)\n",
    "runs = mlflow.search_runs(experiment.experiment_id)\n",
    "print(\"Runs type:\", type(runs))\n",
    "# print(\"Runs columns:\", runs.columns)\n",
    "# print(\"First few rows:\\n\", runs.head())\n",
    "\n",
    "# Check if 'metrics.f1_macro' exists:\n",
    "if 'metrics.f1_macro' not in runs.columns:\n",
    "    raise ValueError(\"No 'metrics.f1_macro' column found. Available metrics:\", runs.columns)\n",
    "\n",
    "best_run = runs.loc[runs['metrics.f1_macro'].idxmax()]\n",
    "print(\"Best run:\", best_run)\n",
    "\n",
    "# Construct the model URI from the run\n",
    "best_run_id = best_run.run_id\n",
    "\n",
    "model_uri = f\"runs:/{best_run_id}/model\"\n",
    "print(f\"Registering model from URI: {model_uri}\")\n",
    "\n",
    "# Register the model to the MLflow Model Registry\n",
    "model_name = \"healthstatus_classifier_model\"\n",
    "registered_model = mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=model_name\n",
    ")\n",
    "\n",
    "print(f\"Model '{model_name}' registered with version: {registered_model.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e26db-6f45-425d-9ff5-c98415381589",
   "metadata": {},
   "source": [
    "#### 2. Retrieve model artifact from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6161397-309f-4588-9fa3-d3bc0b055eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T03:12:43.153147Z",
     "iopub.status.busy": "2025-07-30T03:12:43.152759Z",
     "iopub.status.idle": "2025-07-30T03:12:43.213601Z",
     "shell.execute_reply": "2025-07-30T03:12:43.213047Z",
     "shell.execute_reply.started": "2025-07-30T03:12:43.153106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model S3 Artifact URI: s3://sagemaker-iti112-common/mlflow-1/106/e844a036b919434eb7363bf3204d3b7e/artifacts/model\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "model_version = client.get_model_version(model_name, registered_model.version)\n",
    "\n",
    "model_artifact_s3 = model_version.source\n",
    "print(\"Model S3 Artifact URI:\", model_artifact_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c048c-43bc-49f3-8058-d1678ca40c3e",
   "metadata": {},
   "source": [
    "#### 3. Download, repackage model, upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "389e80bf-7efb-4440-b46a-78dbbf4bfa84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T03:12:44.172147Z",
     "iopub.status.busy": "2025-07-30T03:12:44.171256Z",
     "iopub.status.idle": "2025-07-30T03:12:44.822104Z",
     "shell.execute_reply": "2025-07-30T03:12:44.821259Z",
     "shell.execute_reply.started": "2025-07-30T03:12:44.172114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe82ea6ec5c44d92b5ed5921df3c7366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Compressed model uploaded to: s3://sagemaker-iti112-common/8708815v@myaccount.nyp.edu.sg/models/penguin-model-8815v-v7/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import boto3\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_folder = \"/tmp/model\" # temp local dir\n",
    "code_folder = os.path.join(model_folder, \"code\") \n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "# Download mMLflow artifact to local\n",
    "mlflow.artifacts.download_artifacts(\n",
    "    artifact_uri=model_artifact_s3,\n",
    "    dst_path=model_folder\n",
    ")\n",
    "\n",
    "# Find the actual model file inside model_folder (model.pkl)\n",
    "model_file_path = None\n",
    "for root, dirs, files in os.walk(model_folder):\n",
    "    for f in files:\n",
    "        if f.endswith(\".pkl\"): \n",
    "            model_file_path = os.path.join(root, f)\n",
    "            break\n",
    "\n",
    "if not model_file_path:\n",
    "    raise FileNotFoundError(\"No model.pkl found in MLflow artifacts\")\n",
    "\n",
    "# Copy it to a clean directory so it's at root level\n",
    "root_dir = \"/tmp/model_root\"\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "shutil.copy(model_file_path, os.path.join(root_dir, \"model.pkl\"))\n",
    "\n",
    "# Compress model to model.tar.gz with model.pkl at root\n",
    "model_tar_path = \"/tmp/model.tar.gz\"\n",
    "with tarfile.open(model_tar_path, \"w:gz\") as tar:\n",
    "    tar.add(root_dir, arcname=\".\")\n",
    "\n",
    "# Upload model.tar.gz to S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "# parsed = boto3.session.Session().resource(\"s3\").Bucket(sagemaker_session.default_bucket())\n",
    "s3_key = f\"{base_folder}/models/{model_name}-v{registered_model.version}/model.tar.gz\"\n",
    "s3.upload_file(model_tar_path, bucket_name, s3_key)\n",
    "\n",
    "model_s3_uri = f\"s3://{bucket_name}/{s3_key}\"\n",
    "print(\"✅ Compressed model uploaded to:\", model_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c4a88-b41f-4f8b-b78a-ffc2e456de3b",
   "metadata": {},
   "source": [
    "#### 4. Write inference.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9d16592-d900-4e1e-96a5-11cfb5139403",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T03:12:45.848626Z",
     "iopub.status.busy": "2025-07-30T03:12:45.847907Z",
     "iopub.status.idle": "2025-07-30T03:12:45.853586Z",
     "shell.execute_reply": "2025-07-30T03:12:45.852749Z",
     "shell.execute_reply.started": "2025-07-30T03:12:45.848593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "# inference.py\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    return joblib.load(os.path.join(model_dir, \"model.pkl\"))\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    if content_type == \"application/json\":\n",
    "        return pd.DataFrame.from_dict(eval(request_body))  # simple eval for test input\n",
    "    raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    return model.predict(input_data)\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    return str(prediction.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453fb121-867f-4104-b838-a397d23385c8",
   "metadata": {},
   "source": [
    "#### 5. Check model file structure\n",
    "Make sure model.tar.gz contains model.pkl at the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eabc0946-3f96-4675-bfc6-e88cfc7d391b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T03:12:47.194925Z",
     "iopub.status.busy": "2025-07-30T03:12:47.194187Z",
     "iopub.status.idle": "2025-07-30T03:12:47.279238Z",
     "shell.execute_reply": "2025-07-30T03:12:47.278453Z",
     "shell.execute_reply.started": "2025-07-30T03:12:47.194894Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3_uri = model_s3_uri #\"s3://your-bucket/path/to/model.tar.gz\"\n",
    "\n",
    "bucket = s3_uri.replace(\"s3://\", \"\").split(\"/\")[0]\n",
    "key = \"/\".join(s3_uri.replace(\"s3://\", \"\").split(\"/\")[1:])\n",
    "\n",
    "local_path = \"model.tar.gz\"\n",
    "s3.download_file(bucket, key, local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf24e9c3-d5d0-4abe-bb5c-b59a64d7ff80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T03:12:47.637833Z",
     "iopub.status.busy": "2025-07-30T03:12:47.637216Z",
     "iopub.status.idle": "2025-07-30T03:12:47.643331Z",
     "shell.execute_reply": "2025-07-30T03:12:47.642668Z",
     "shell.execute_reply.started": "2025-07-30T03:12:47.637806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?rwxr-xr-x sagemaker-user/users          0 2025-07-30 03:12:44 ./ \n",
      "?rw-r--r-- sagemaker-user/users       1084 2025-07-30 03:12:44 ./model.pkl \n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# Make sure model.tar.gz contains model.pkl at the root\n",
    "# OR contains MLflow's default format with `MLmodel` + `model.pkl`\n",
    "\n",
    "with tarfile.open(local_path, \"r:gz\") as tar:\n",
    "    tar.list()   # shows file structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd7f49-2327-4228-8858-70f11829e815",
   "metadata": {},
   "source": [
    "#### 6. Create SageMaker Model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8c6a9-39b7-4889-a28d-18628e2edb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "# Create the model object\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_s3_uri,\n",
    "    role=sagemaker_role,\n",
    "    entry_point=\"inference.py\", \n",
    "    framework_version=\"1.2-1\",\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562fa24-5176-4c18-b4aa-fe8d9c4333b4",
   "metadata": {},
   "source": [
    "#### 7. Check and deploy endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "630216b4-91ab-4249-87de-372b549582a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T03:20:51.726928Z",
     "iopub.status.busy": "2025-07-30T03:20:51.726561Z",
     "iopub.status.idle": "2025-07-30T03:26:56.455944Z",
     "shell.execute_reply": "2025-07-30T03:26:56.455070Z",
     "shell.execute_reply.started": "2025-07-30T03:20:51.726903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-scikit-learn-2025-07-30-03-20-52-727\n",
      "INFO:sagemaker:Creating endpoint-config with name 8815v-penguin-clf\n",
      "INFO:sagemaker:Creating endpoint with name 8815v-penguin-clf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!Model deployed at endpoint: 8815v-penguin-clf\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Define the endpoint name\n",
    "endpoint_name = \"8815v-penguin-clf\" \n",
    "# deleted previous endpoint & endpoint config instead of using {registered_model.version}\"\n",
    "\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "def endpoint_exists(endpoint_name):\n",
    "    try:\n",
    "        sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if \"Could not find endpoint\" in str(e):\n",
    "            return False\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "if endpoint_exists(endpoint_name):\n",
    "    predictor = sklearn_model.deploy(\n",
    "        instance_type=\"ml.t2.medium\",\n",
    "        initial_instance_count=1,\n",
    "        endpoint_name=endpoint_name,\n",
    "        update_endpoint=True # Update endpoint if it already exists\n",
    "    )\n",
    "else:\n",
    "    # Create endpoint if it doesn't exist\n",
    "    predictor = sklearn_model.deploy(\n",
    "        instance_type=\"ml.t2.medium\",\n",
    "        initial_instance_count=1,\n",
    "        endpoint_name=endpoint_name\n",
    "    )\n",
    "\n",
    "print(f\"Model deployed at endpoint: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ea9aa-f390-4528-9520-1cb7a0ffc3ec",
   "metadata": {},
   "source": [
    "## Inference and Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db9128-861b-4d2a-af7f-88dd18866ca3",
   "metadata": {},
   "source": [
    "#### Test endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c4506ff-8130-42ad-a961-ca6dbc64554f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T04:06:29.747757Z",
     "iopub.status.busy": "2025-07-30T04:06:29.747162Z",
     "iopub.status.idle": "2025-07-30T04:06:29.942240Z",
     "shell.execute_reply": "2025-07-30T04:06:29.941340Z",
     "shell.execute_reply.started": "2025-07-30T04:06:29.747727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  species  culmen_length_mm  culmen_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0  Adelie         -2.005109        -0.534410          -1.645021    -1.636340   \n",
      "1  Adelie         -0.816790         1.761560          -0.708965    -0.388429   \n",
      "2  Adelie         -1.529781        -0.085198          -0.852974    -1.074780   \n",
      "3  Gentoo          1.084521        -0.634235           1.667176     1.421042   \n",
      "4  Gentoo          1.066240        -0.534410           0.875129     1.483437   \n",
      "\n",
      "   sex_MALE  island_Dream  island_Torgersen  \n",
      "0       0.0           1.0               0.0  \n",
      "1       0.0           0.0               0.0  \n",
      "2       0.0           0.0               1.0  \n",
      "3       1.0           0.0               0.0  \n",
      "4       1.0           0.0               0.0  \n",
      "Sending payload:\n",
      " {'culmen_length_mm': [-2.0051090117600747, -0.8167896138012701, -1.5297812525765526, 1.0845214229328168, 1.0662395860411429], 'culmen_depth_mm': [-0.5344098100591036, 1.7615595346970407, -0.0851984165198584, -0.6342345641789365, -0.5344098100591036], 'flipper_length_mm': [-1.645020777953519, -0.7089651837644805, -0.8529737367166403, 1.667175939946156, 0.8751288987092771], 'body_mass_g': [-1.636340446583799, -0.3884294418561582, -1.0747804944563606, 1.4210415149989208, 1.4834370652353028], 'sex_MALE': [0.0, 0.0, 0.0, 1.0, 1.0], 'island_Dream': [1.0, 0.0, 0.0, 0.0, 0.0], 'island_Torgersen': [0.0, 0.0, 1.0, 0.0, 0.0]}\n",
      "Prediction response: ['Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Gentoo']\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import StringDeserializer\n",
    "\n",
    "# Create the predictor with JSON handling\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=JSONSerializer(), # convert to JSON format\n",
    "    deserializer=StringDeserializer()\n",
    ")\n",
    "\n",
    "# Get X_test\n",
    "processed_test_uri = f\"{data_path}/v1.0/processed/test/test_processed.csv\"\n",
    "\n",
    "X_test = pd.read_csv(processed_test_uri)\n",
    "\n",
    "test_sample = X_test.iloc[:5]\n",
    "print(test_sample)\n",
    "# Convert to a dict (records format: {col_name: [v1, v2, ...]})\n",
    "test_sample = test_sample.drop(columns='species')\n",
    "payload = test_sample.to_dict(orient=\"list\")\n",
    "\n",
    "print(\"Sending payload:\\n\", payload)\n",
    "\n",
    "response = predictor.predict(payload)\n",
    "print(\"Prediction response:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c7f24e-127c-4623-8134-372c97a72f82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### View endpoint logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c463b635-01a6-46ca-b1be-2a599d424e6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T03:35:05.705392Z",
     "iopub.status.busy": "2025-07-30T03:35:05.705106Z",
     "iopub.status.idle": "2025-07-30T03:35:06.054067Z",
     "shell.execute_reply": "2025-07-30T03:35:06.053316Z",
     "shell.execute_reply.started": "2025-07-30T03:35:05.705370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for logs in: /aws/sagemaker/Endpoints/8815v-penguin-clf\n",
      "\n",
      "--- Logs from stream: AllTraffic/i-0ade6cfefcc711c88 ---\n",
      "169.254.178.2 - - [30/Jul/2025:03:34:51 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:34:46 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:34:41 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:34:36 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:34:31 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:34:26 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:34:21 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:34:16 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:34:11 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:34:06 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:34:01 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:33:57 +0000] \"POST /invocations HTTP/1.1\" 200 50 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:33:56 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:33:51 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:33:46 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:33:41 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:33:36 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:33:31 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:33:26 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:33:21 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:33:16 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:33:11 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:33:06 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:33:01 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:32:56 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:32:51 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:32:46 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:32:41 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:32:36 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:32:31 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:32:26 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:32:21 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:32:16 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:32:11 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:32:06 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:32:01 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:31:56 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:31:51 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:31:46 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:31:41 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:31:36 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:31:31 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:31:26 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:31:21 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:31:16 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:31:11 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:31:06 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:31:01 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:30:56 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "169.254.178.2 - - [30/Jul/2025:03:30:52 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"AHC/2.0\"\n",
      "------------------------------------------------------ \n",
      "\n",
      "--- Logs from stream: AllTraffic/i-0ca5d1d544e24b61f ---\n",
      "------------------------------------------------------ \n",
      "\n",
      "--- Logs from stream: AllTraffic/i-0a04cddafa3d05c7a ---\n",
      "------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Enter the name of your SageMaker endpoint\n",
    "endpoint_name = \"8815v-penguin-clf\"\n",
    "\n",
    "# The log group is created based on the endpoint name\n",
    "log_group_name = f\"/aws/sagemaker/Endpoints/{endpoint_name}\"\n",
    "\n",
    "# Create a CloudWatch Logs client\n",
    "logs_client = boto3.client(\"logs\")\n",
    "\n",
    "print(f\"Searching for logs in: {log_group_name}\\n\")\n",
    "\n",
    "try:\n",
    "    # Find all log streams in the log group, ordered by the most recent\n",
    "    response = logs_client.describe_log_streams(\n",
    "        logGroupName=log_group_name,\n",
    "        orderBy='LastEventTime',\n",
    "        descending=True\n",
    "    )\n",
    "\n",
    "    log_streams = response.get(\"logStreams\", [])\n",
    "\n",
    "    if not log_streams:\n",
    "        print(\"No log streams found. The endpoint might not have processed any requests yet.\")\n",
    "    \n",
    "    # Loop through each stream and print its recent log events\n",
    "    for stream in log_streams:\n",
    "        stream_name = stream['logStreamName']\n",
    "        print(f\"--- Logs from stream: {stream_name} ---\")\n",
    "\n",
    "        # Get log events from the stream\n",
    "        log_events = logs_client.get_log_events(\n",
    "            logGroupName=log_group_name,\n",
    "            logStreamName=stream_name,\n",
    "            startFromHead=False,  # False gets recent logs first\n",
    "            limit=50  # Get up to 50 recent log events\n",
    "        )\n",
    "        \n",
    "        # Print events in chronological order\n",
    "        for event in reversed(log_events.get(\"events\", [])):\n",
    "            print(event['message'].strip())\n",
    "        \n",
    "        print(\"-\" * (len(stream_name) + 24), \"\\n\")\n",
    "\n",
    "except logs_client.exceptions.ResourceNotFoundException:\n",
    "    print(f\"Error: Log group '{log_group_name}' was not found.\")\n",
    "    print(\"Please check the endpoint name and ensure it has been invoked.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49938d-658b-4a23-9a72-a6ac4a1a56f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f28b28b-b413-49ff-a3f7-ad113ec2af8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T03:20:33.728701Z",
     "iopub.status.busy": "2025-07-30T03:20:33.728043Z",
     "iopub.status.idle": "2025-07-30T03:20:33.944262Z",
     "shell.execute_reply": "2025-07-30T03:20:33.943628Z",
     "shell.execute_reply.started": "2025-07-30T03:20:33.728673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting SageMaker endpoint: 8815v-penguin-clf...\n",
      "Deleted old endpoint config: 8815v-penguin-clf\n"
     ]
    }
   ],
   "source": [
    "# Clean Up Resources\n",
    "print(f\"Deleting SageMaker endpoint: {endpoint_name}...\")\n",
    "predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully.\")\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    print(f\"Deleted old endpoint config: {endpoint_name}\")\n",
    "except ClientError as e:\n",
    "    if \"Could not find\" not in str(e):\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e5e93-7cf4-4519-b836-658fce0e8739",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Automated deployment with second pipeline\n",
    "(Couldn't make the Eventbridge work despite watching for 'model created with approved status' on top of 'model state change to approved status' -  pipeline created but no executions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7229dd4a-a694-4c84-a993-b00b9bfda54a",
   "metadata": {},
   "source": [
    "#### Deployment script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b69eb8ac-a7be-4d2a-ae6b-dfc307875a34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T08:20:54.214977Z",
     "iopub.status.busy": "2025-07-28T08:20:54.214602Z",
     "iopub.status.idle": "2025-07-28T08:20:54.220425Z",
     "shell.execute_reply": "2025-07-28T08:20:54.219713Z",
     "shell.execute_reply.started": "2025-07-28T08:20:54.214951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deploy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy.py\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# --- Install required packages ---\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"boto3==1.28.57\", \"botocore==1.31.57\", \"numpy==1.24.1\", \"sagemaker\" ])\n",
    "\n",
    "# Ensure sagemaker SDK is installed before importing\n",
    "try:\n",
    "    import sagemaker\n",
    "except ImportError:\n",
    "    print(\"sagemaker SDK not found. Installing now...\")\n",
    "    install(\"sagemaker\")\n",
    "    import sagemaker\n",
    "\n",
    "import argparse\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.model import ModelPackage\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Accept the registered model's ARN instead of the S3 data path\n",
    "    parser.add_argument(\"--model-package-arn\", type=str, required=True)\n",
    "    parser.add_argument(\"--role\", type=str, required=True)\n",
    "    parser.add_argument(\"--endpoint-name\", type=str, required=True)\n",
    "    parser.add_argument(\"--region\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    boto_session = boto3.Session(region_name=args.region)\n",
    "    sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "\n",
    "    # Create a SageMaker Model object directly from the Model Package ARN\n",
    "    model = ModelPackage(\n",
    "        model_package_arn=args.model_package_arn,\n",
    "        role=args.role,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "\n",
    "    # Deploy the model to an endpoint\n",
    "    print(f\"Deploying registered model from ARN to endpoint: {args.endpoint_name}\")\n",
    "    model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.t2.medium\",\n",
    "        endpoint_name=args.endpoint_name,\n",
    "        # Update endpoint if it already exists\n",
    "        update_endpoint=True\n",
    "    )\n",
    "    print(\"Deployment complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfef03-d828-4de3-b732-e0acb636dc11",
   "metadata": {},
   "source": [
    "#### Define deployment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f5db2d2b-63e6-4ef2-a40c-fc2d47d01a20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T08:20:55.108237Z",
     "iopub.status.busy": "2025-07-28T08:20:55.107954Z",
     "iopub.status.idle": "2025-07-28T08:20:58.142948Z",
     "shell.execute_reply": "2025-07-28T08:20:58.142270Z",
     "shell.execute_reply.started": "2025-07-28T08:20:55.108217Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment pipeline ARN: arn:aws:sagemaker:ap-southeast-1:287730026636:pipeline/8815V-Deploy\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "import sagemaker\n",
    "\n",
    "# Define Parameters for the deployment pipeline\n",
    "# This will be provided by the EventBridge trigger\n",
    "model_package_arn_param = ParameterString(name=\"ModelPackageArn\", default_value=\"\")\n",
    "role_param = ParameterString(name=\"ExecutionRole\", default_value=sagemaker_role)\n",
    "endpoint_name_param = ParameterString(name=\"EndpointName\", default_value=\"8815v-penguin-clf-endpoint\")\n",
    "\n",
    "# Create a ScriptProcessor for deployment\n",
    "# Using a more recent scikit-learn version is generally a good idea\n",
    "deploy_processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"sklearn\", sagemaker_session.boto_region_name, version=\"1.2-1\"),\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    role=role_param,\n",
    "    base_job_name=\"deploy-registered-model\"\n",
    ")\n",
    "\n",
    "# Define the deployment step that takes the model ARN as an argument\n",
    "step_deploy = ProcessingStep(\n",
    "    name=\"DeployRegisteredModel\",\n",
    "    processor=deploy_processor,\n",
    "    code=\"deploy.py\",\n",
    "    job_arguments=[\n",
    "        \"--model-package-arn\", model_package_arn_param,\n",
    "        \"--role\", role_param,\n",
    "        \"--endpoint-name\", endpoint_name_param,\n",
    "        \"--region\", \"ap-southeast-1\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define the independent deployment pipeline\n",
    "deploy_pipeline = Pipeline(\n",
    "    name=\"8815V-Deploy\",\n",
    "    parameters=[model_package_arn_param, role_param, endpoint_name_param],\n",
    "    steps=[step_deploy]\n",
    ")\n",
    "\n",
    "# Create or update the pipeline definition\n",
    "# Capture the response which contains the ARN\n",
    "response = deploy_pipeline.upsert(role_arn=sagemaker_role)\n",
    "\n",
    "# Extract the ARN from the response dictionary\n",
    "pipeline_arn = response['PipelineArn']\n",
    "\n",
    "print(f\"Deployment pipeline ARN: {pipeline_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c1460-a499-40bd-8af9-5c85f057bbbc",
   "metadata": {},
   "source": [
    "#### Eventbridge to watch for event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d2008ef-6458-43ec-a161-00b8afaac22e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T08:20:58.144402Z",
     "iopub.status.busy": "2025-07-28T08:20:58.144051Z",
     "iopub.status.idle": "2025-07-28T08:20:58.229554Z",
     "shell.execute_reply": "2025-07-28T08:20:58.228832Z",
     "shell.execute_reply.started": "2025-07-28T08:20:58.144381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating or updating EventBridge rule: 8708815v-myaccount.nyp.edu.sg-TriggerModelDeploymentOnApproval\n",
      "EventBridge rule created successfully!\n",
      "Now, when a model is approved in the Model Registry, the deployment pipeline will trigger automatically.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize the EventBridge client\n",
    "events_client = boto3.client(\"events\")\n",
    "\n",
    "# Define the event pattern to listen for\n",
    "# This pattern triggers when a model package in your group has its status changed to \"Approved\"\n",
    "event_pattern = {\n",
    "    \"source\": [\"aws.sagemaker\"],\n",
    "    \"detail-type\": [\n",
    "        \"SageMaker Model Package State Change\",\n",
    "        \"SageMaker Model Package Created\"],\n",
    "    \"detail\": {\n",
    "        \"ModelPackageGroupName\": [model_package_group_name], \n",
    "        \"ModelApprovalStatus\": [\"Approved\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the target for the rule (our deployment pipeline)\n",
    "# We need to map the event's detail to the pipeline's parameters\n",
    "target = {\n",
    "    \"Id\": \"DeployPenguinPipelineTarget\",\n",
    "    \"Arn\": pipeline_arn, # The ARN of the pipeline we just created\n",
    "    \"RoleArn\": sagemaker_role, # The execution role for the pipeline\n",
    "    \"SageMakerPipelineParameters\": {\n",
    "        \"PipelineParameterList\": [\n",
    "            {\n",
    "                # Map the ARN from the event to the pipeline's \"ModelPackageArn\" parameter\n",
    "                \"Name\": \"ModelPackageArn\",\n",
    "                \"Value\": \"$.detail.ModelPackageArn\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create or update the EventBridge rule\n",
    "try:\n",
    "    username_lower = \"8708815v@myaccount.nyp.edu.sg\".lower().replace(\"@\", \"-\")\n",
    "    rule_name = f\"{username_lower}-TriggerModelDeploymentOnApproval\"\n",
    "    print(f\"Creating or updating EventBridge rule: {rule_name}\")\n",
    "    response = events_client.put_rule(\n",
    "        Name=rule_name,\n",
    "        EventPattern=json.dumps(event_pattern),\n",
    "        State=\"ENABLED\",\n",
    "        Description=\"Triggers the SageMaker pipeline to deploy a model upon approval.\"\n",
    "    )\n",
    "    \n",
    "    # Add the pipeline as a target for the rule\n",
    "    events_client.put_targets(Rule=rule_name, Targets=[target])\n",
    "    print(\"EventBridge rule created successfully!\")\n",
    "    print(\"Now, when a model is approved in the Model Registry, the deployment pipeline will trigger automatically.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating rule: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df1104-4ac3-47df-a5ef-3631cc729280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
